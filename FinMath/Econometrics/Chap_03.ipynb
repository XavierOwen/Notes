{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Regression Analysis\n",
    "##  Motivation for Multiple Regression\n",
    "### The Model with Two Independent Variables\n",
    "\n",
    "we skip some similar definition and approaches$\\DeclareMathOperator*{\\argmin}{argmin}\n",
    "\\newcommand{\\using}[1]{\\stackrel{\\mathrm{#1}}{=}}\n",
    "\\newcommand{\\ffrac}{\\displaystyle \\frac}\n",
    "\\newcommand{\\space}{\\text{ }}\n",
    "\\newcommand{\\bspace}{\\;\\;\\;\\;}\n",
    "\\newcommand{\\QQQ}{\\boxed{?\\:}}\n",
    "\\newcommand{\\CB}[1]{\\left\\{ #1 \\right\\}}\n",
    "\\newcommand{\\SB}[1]{\\left[ #1 \\right]}\n",
    "\\newcommand{\\P}[1]{\\left( #1 \\right)}\n",
    "\\newcommand{\\dd}{\\mathrm{d}}\n",
    "\\newcommand{\\Tran}[1]{{#1}^{\\mathrm{T}}}\n",
    "\\newcommand{\\d}[1]{\\displaystyle{#1}}\n",
    "\\newcommand{\\EE}[2][\\,\\!]{\\mathbb{E}_{#1}\\left[#2\\right]}\n",
    "\\newcommand{\\Var}[2][\\,\\!]{\\mathrm{Var}_{#1}\\left[#2\\right]}\n",
    "\\newcommand{\\Cov}[2][\\,\\!]{\\mathrm{Cov}_{#1}\\left(#2\\right)}\n",
    "\\newcommand{\\Corr}[2][\\,\\!]{\\mathrm{Corr}_{#1}\\left(#2\\right)}\n",
    "\\newcommand{\\I}[1]{\\mathrm{I}\\left( #1 \\right)}\n",
    "\\newcommand{\\N}[1]{\\mathrm{N} \\left( #1 \\right)}\n",
    "\\newcommand{\\ow}{\\text{otherwise}}$. One thing to mention is that the key assumption about how $u$ related to $x_1$ and $x_2$ is $\\EE{u \\mid x_1 , x_2} = 0$.\n",
    "\n",
    "### The Model with $k$ Independent Variables\n",
    "\n",
    "Not too many things different from before, only one thing: $\\EE{u \\mid x_1 , x_2, \\dots, x_k}= 0$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mechanics and Interpretation of Ordinary Least Squares\n",
    "### Obtaining the OLS estimates\n",
    "\n",
    "OLS ***first order conditions***:\n",
    "\n",
    "$$\\left\\{\\begin{align}\n",
    "\\sum_{i=1}^{n} \\P{y_i - \\hat\\beta_0 - \\hat\\beta_1x_{i1} - \\cdots - \\hat\\beta_k x_{ik}} &= 0\\\\\n",
    "\\sum_{i=1}^{n} x_{i1}\\P{y_i - \\hat\\beta_0 - \\hat\\beta_1x_{i1} - \\cdots - \\hat\\beta_k x_{ik}} &= 0\\\\\n",
    "\\sum_{i=1}^{n} x_{i2}\\P{y_i - \\hat\\beta_0 - \\hat\\beta_1x_{i1} - \\cdots - \\hat\\beta_k x_{ik}} &= 0\\\\\n",
    "&\\vdots \\\\\n",
    "\\sum_{i=1}^{n} x_{ik}\\P{y_i - \\hat\\beta_0 - \\hat\\beta_1x_{i1} - \\cdots - \\hat\\beta_k x_{ik}} &= 0\\\\\n",
    "\\end{align}\\right.$$\n",
    "\n",
    "Also these can be obtained by **moment methods**矩方法 cause these are equivalent to $\\EE{u} = 0$ and $\\EE{x_j u} = 0$ where $j = 1, 2, \\dots, k$.\n",
    "\n",
    "$Remark$\n",
    "\n",
    "Further assumption about whether there's an unique solution about this equation set will be shown later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the OLS Regression Equation\n",
    "\n",
    "OLS ***regression line***: $\\hat y = \\hat \\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 x_2 + \\cdots \\hat\\beta_k x_k$\n",
    "\n",
    "### On the Meaning of \"Holding Other Factors Fixed\"  in Multiple regression\n",
    "\n",
    "The power of multiple regression analysis is that it allows us to do in nonexperimental environments what natural scientists are able to do in a controlled laboratory setting: keep other factors fixed.\n",
    "\n",
    "### Changing More than One Independent Variable Simultaneously\n",
    "### OLS Fitted Values and Residuals\n",
    "\n",
    "***Fitted value***: $\\hat y_i = \\hat \\beta_0 + \\hat\\beta_1 x_{i1} + \\cdots + \\hat\\beta_k x_{ik}$\n",
    "\n",
    "***Residual***: $\\hat u_i = y_i - \\hat y_i$\n",
    "\n",
    "Here're some properties inherited from the **SLR** method.\n",
    "\n",
    "- The sample average of the residuals is zero: $\\sum\\limits_{i=1}^{n} \\hat u_i = 0$ ($\\EE{\\hat u} = 0$) and so $\\bar y = \\bar{\\hat y}$\n",
    "-  The sample covariance between each independent variable and the OLS residuals is zero: $\\sum\\limits_{i=1}^{n} x_{ij} \\hat u_i = 0$ and so:  the sample covariance between the OLS fitted values and the OLS residuals is zero: $\\sum\\limits_{i=1}^{n}\\hat y_i \\hat u_i = 0$\n",
    "- $\\bar y = \\hat \\beta_0 + \\hat\\beta_1 \\bar x_1 + \\cdots + \\hat \\beta_k \\bar x_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A \"Partialling Out\" Interpretation of Multiple Regression\n",
    "\n",
    "We now focus on the \"partialling Out\" Interpretation.\n",
    "\n",
    "Consider the case with $k=2$ independent variables, and the regression result is $\n",
    "\\hat y = \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 x_2$. Here's another expression for $\\hat\\beta_1$.\n",
    "\n",
    "$$\\hat\\beta_1 = \\ffrac{\\sum\\limits_{i=1}^{n} \\hat r_{i1}y_i} {\\sum\\limits_{i=1}^{n} \\hat r_{i1}^2}$$\n",
    "\n",
    "Here $\\hat r_{i1}$ is the residual from a simple regression of $x_1$ on $x_2$ (which has NOTHING to do with $y$). After that we do another regression of $y$ on $\\hat r_i$, so that to obtian $\\hat\\beta_1$.\n",
    "\n",
    "This can be interpreted as we first partial out the correlated part of $x_1$ and $x_2$ where only $\\hat r_i$ left. So that $\\hat \\beta_1$ now can measure the sample relation ship between $y$ and $x_1$ after $x_2$ has been partialled out.\n",
    "\n",
    "$$\\hat\\beta_1 = \\ffrac{\\sum \\P{\\hat r_{i1} - \\bar{\\hat r}_{i1}}\\P{y_i - \\bar y}} {\\sum \\P{\\hat r_{i1} - \\bar{\\hat r}_{i1}}^2 }$$\n",
    "\n",
    "then by the fact that $\\sum \\hat r_{i1} = 0$ (it's a residual anyway), we can see its final appearance. Also in the general model with $k$ explanatory variables, $\\hat\\beta_1$ can be written as the same.\n",
    "\n",
    "$Proof$\n",
    "\n",
    ">To derive the equation, we first follow the strategy and write: $x_{i1} = \\hat x_{i1} + \\hat r_{i1}$ from the regression of $x_1$ on $x_2, x_3, \\dots,x_k$ for all $i = 1,2,\\dots,n$. Then we plug it back into **OLS first order conditions** and obtain:\n",
    ">\n",
    ">$$\\sum\\nolimits_{i=1}^{n} \\P{\\hat x_{i1} + \\hat r_{i1}} \\P{y_i - \\hat\\beta_0 - \\hat\\beta_1 x_{i1} - \\cdots - \\hat\\beta_k x_{ik}} = 0$$\n",
    ">\n",
    ">At this time, $\\hat x_{i1}$ is the linear combination of the other explainatory variables $x_{12}, x_{i3}, \\dots, x_{ik}$ and thus $\\sum \\hat x_{i1}\\hat u_i = 0$. Therefore,\n",
    "\n",
    ">$$\\sum_{i=1}^{n} \\hat r_{i1} \\P{y_i - \\hat\\beta_0 - \\hat\\beta_1 x_{i1} - \\cdots - \\hat\\beta_k x_{ik}} = 0$$\n",
    ">\n",
    ">Then, on account of the fact that $\\hat r_{i1}$ are the residuals from regressing $x_1$ on $x_2, x_3, \\dots, x_k$ we have $\\sum_{i=1}^{n} x_{ij} \\hat r_{i1} = 0$ for all $j = 2,3,\\dots,k$. Therefore, the preceding equation is simplified to:\n",
    ">\n",
    ">$$\\sum_{i=1}^{n} \\hat r_{i1} \\P{y_i - \\hat\\beta_1 x_{i1}} = 0 \\\\\n",
    "\\Longrightarrow\n",
    "\\hat\\beta_1 = \\ffrac{\\d{\\sum_{i=1}^{n} \\hat r_{i1}y_i}} {\\d{\\sum_{i=1}^{n} \\hat r_{i1} x_{i1}}}$$\n",
    ">\n",
    ">Finally, use the fact that $\\sum_{i=1}^{n} \\hat x_{i1} \\hat r_{i1} = 0$, we have \n",
    ">\n",
    ">$$\\hat\\beta_1 = \\ffrac{\\d{\\sum_{i=1}^{n} \\hat r_{i1}y_i}} {\\d{\\sum_{i=1}^{n} \\hat r_{i1} \\P{x_{i1} - \\hat x_{i1} }}} = \\ffrac{\\d{\\sum_{i=1}^{n} \\hat r_{i1}y_i}} {\\d{\\sum_{i=1}^{n} \\hat r_{i1}^2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Simple and Multiple Regression Estimates\n",
    "\n",
    "If the model has two variables but we itentionally omit one, say $x_2$, then denote the simple regression result as $\\tilde y = \\tilde \\beta_0 + \\tilde \\beta_1 x_1$ while the one with full variable has the form: $\\hat y = \\hat \\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 x_2$. And the relation between $\\tilde \\beta_0$ and $\\tilde \\beta_1$ can be expressed as: $\\boxed{\\tilde\\beta_1 = \\hat\\beta_1 + \\hat\\beta_2 \\cdot \\tilde\\delta_1}$, where $\\tilde\\delta_1$ is the slope coefficient from the simple regression of $x_{i2}$ on $x_{i1}$, $i = 1,2,\\dots, n$.\n",
    "\n",
    "Interpretation: $\\tilde\\beta_1$ is somewhat the sum of the partial effects of $x_1$ on $\\hat y$ and the partial effects of $x_2$ on $\\hat y$ times the slope in the sample regression of $x_2$ on $x_1$. ***3A.4***\n",
    "\n",
    "And only in two cases will they equal: \n",
    "\n",
    "1. $\\hat\\beta_2 = 0$, that the partial effect of $x_2$ on $\\hat y$ is $0$ in the sample$\\\\[0.5em]$\n",
    "2. $\\tilde\\delta_1 = 0$, that $x_1$ and $x_2$ are uncorrelated in the sample\n",
    "\n",
    "And the generalized one:\n",
    "\n",
    "1. the OLS coefficients on $x_2$ through $x_k$ are all $0$\n",
    "2. $x_1$ is uncorrelated with *each* of $x_2, x_3,\\dots,x_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goodness-of-Fit\n",
    "\n",
    "Then we define the $\\text{SST} = \\sum \\P{y_i - \\bar y}^2$, $\\text{SSE} = \\sum \\P{\\hat y_i - \\bar y}^2$, and $\\text{SSR} = \\sum \\hat u_i^2$ and $R^2 = \\ffrac{\\text{SSE}} {\\text{SST}}$. And using the same argument in the SLR, we have $\\text{SSR} = \\text{SST} - \\text{SSE}$.\n",
    "\n",
    "And there's also an alternative expression of $R^2$, seemingly using an asymptotic way:\n",
    "\n",
    "$$\\begin{align}\n",
    "R^2 &= \\rho^2\\P{y_i, \\hat y_i} \\\\\n",
    "&= \\ffrac{\\Cov{y_i, \\hat y_i}^2} {\\Var{y_i}\\Var{\\hat y_i}} \\\\\n",
    "&= \\ffrac{\\EE{\\P{y_i - \\EE{y_i}}\\P{\\hat y_i - \\EE{\\hat y_i}}}^2} {\\EE{y_i - \\EE{y_i}}^2\\EE{\\hat y_i - \\EE{\\hat y_i}}^2} \\\\\n",
    "&\\approx \\ffrac{\\P{\\sum\\limits_{i=1}^{n} \\P{y_i - \\bar y}\\P{\\hat y - \\bar{\\hat y}}}^2} {\\P{\\sum\\limits_{i=1}^{n} \\P{y_i - \\bar{y}}^2}\\P{\\sum\\limits_{i=1}^{n} \\P{\\hat y - \\bar{\\hat y}}^2}}\n",
    "\\end{align}$$\n",
    "\n",
    "### Regression through the Origin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Expected Value of the OLS Estimators\n",
    "\n",
    "$Assumption$ $\\text{MLR}.1$ to $\\text{MLR}.4$\n",
    "\n",
    "- Linear in parameters: In the population, the relationship between $y$ and the explanatory variables is linear: $y = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_k x_k + u$. This model is called the ***population model*** or ***true model***\n",
    "- Random Sampling: The data is a random sample drawn from the population: $\\CB{\\P{x_{i1},x_{i2},\\dots,x_{ik}}:i=1,\\dots,n}$\n",
    "    - and we write: $y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_kx_{ik} + u_i\\\\[0.5em]$\n",
    "- No perfect collinearity: In the sample (and therefore in the population), none of the independent variables is constant and there are no *exact linear* relationships among the independent variables\n",
    "    - Later we will see that its variance will soar up if almost linear\n",
    "    - And if yes, we say ***perfect collinearity*** occurs and can't be estimated using OLS.\n",
    "- Zero conditional mean: The value of the explanatory variables must contain no information about the mean of the unobserved factors: $\\EE{u \\mid x_{1} , x_{2}, \\dots, x_{k}} = 0$\n",
    "\n",
    "$Theorem.1$\n",
    "\n",
    "By assumptions $\\text{MLR}.1$ to $\\text{MLR}.4$, we claim that $\\EE{\\hat\\beta_j} = \\beta_j$\n",
    "\n",
    "$Proof$\n",
    "\n",
    "> Using matrices is a better way, but here we just focus on one slope parameter.\n",
    ">\n",
    ">First under $\\text{MLR}.3$ we have $\\hat\\beta_1 = \\ffrac{\\sum\\limits_{i=1}^{n} \\hat r_{i1} y_i} {\\sum\\limits_{i = 1}^{n} \\hat{r}^2_{i1}}$.\n",
    ">\n",
    ">Then under $\\text{MLR}.1$, we have $y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_k x_{ik} + u_i$; we can substitute this $y_i$ back and obtain $\\hat\\beta_1 = \\ffrac{\\sum\\limits_{i=1}^{n} \\hat r_{i1} \\P{\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_k x_{ik} + u_i}} {\\sum\\limits_{i = 1}^{n} \\hat{r}^2_{i1}}$.\n",
    ">\n",
    ">We now deal with the terms separately.\n",
    ">\n",
    ">$\\sum \\hat r_{i1} = 0$, since it's a residual; $\\sum x_{ij}\\hat r_{i1} = 0$, since that's the Covariance of residual and explanatory variable; they hold true for all $j = 2,3,\\dots,k$. And $\\sum x_{i1}\\hat r_{i1} = \\sum \\hat r_{i1}^2$ since $x_{i1} = \\text{linear function}\\P{x_{i2}, x_{i3}, \\dots, x_{ik}} + \\hat r_{i1}$.\n",
    "\n",
    ">Finally, $\\hat \\beta_1 = \\beta_1 + \\ffrac{\\sum\\limits_{i=1}^{n} \\hat r_{i1} u_i} {\\sum\\limits_{i = 1}^{n} \\hat{r}^2_{i1}}$\n",
    ">\n",
    ">Next step, under assumption $\\text{MLR}.2$ and $\\text{MLR}.4$ we consider the expecation of $\\hat\\beta_1$ conditioned on $\\mathbf{X} = \\P{X_1, X_2, \\dots, X_k}$:\n",
    ">\n",
    ">$$\\begin{align}\n",
    "\\EE{\\hat\\beta_1 \\mid \\mathbf{X}} &= \\beta_1 + \\ffrac{\\sum\\limits_{i=1}^{n} \\hat r_{i1} \\EE{u_i\\mid \\mathbf{X}}} {\\sum\\limits_{i = 1}^{n} \\hat{r}^2_{i1}} \\\\\n",
    "&= \\beta_1 + \\ffrac{\\sum\\limits_{i=1}^{n} \\hat r_{i1} \\cdot 0} {\\sum\\limits_{i = 1}^{n} \\hat{r}^2_{i1}} \\\\\n",
    "&= \\beta_1 = \\EE{\\hat\\beta_1}\n",
    "\\end{align}$$\n",
    "\n",
    "### Including Irrelevant Variables in a Regression Model\n",
    "\n",
    "More variables are included in the model while they have no partial effects on $y$ in the population. A simple example, say the model is $y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + u$ and $x_3$ is useless here. Then in terms of conditional expectations, $\\EE{y \\mid x_1,x_2,x_3} = \\EE{y \\mid x_1,x_2} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2$\n",
    "\n",
    "Then, $\\EE{\\hat\\beta_3} = 0$. Though $\\hat\\beta_3$ might not be exactly $0$, it's expectation will. And our conclusion is including one or more *irrelevant variables* in a multiple regression model, or overspecifying the model, does not affect the unbiasedness of the OLS estimators. However, variances will suffer the harm.\n",
    "\n",
    "### Omitted Variable Bias: The Simple Case\n",
    "\n",
    "See this from a simple case. We suppose the true model is $y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + u$ while we assume it to be in another form $y = \\alpha_0 + \\alpha_1 x_1 + w$. What's the bias from this?\n",
    "\n",
    "First we assume that $x_2 = \\delta_0 + \\delta_1 x_1 + v$, thus the model changes to $y = \\P{\\beta_0 + \\beta_2 \\delta_0} + \\P{\\beta_1 + \\beta_2 \\delta_1}x_1 + \\P{\\beta_2 v + u}$. Here the estimated intercept is $\\P{\\beta_0 + \\beta_2 \\delta_0}$, altered, from $\\beta_0$; the estimated slope on $x_1$ will be $\\P{\\beta_1 + \\beta_2 \\delta_1}$, also altered, from $\\beta_1$; and the error term, which was changed to $\\P{\\beta_2 v + u}$ from a simple $u$. **All estimated coefficients will be biased now**.\n",
    "\n",
    "If we do the sample regression of $y$ only on $x_1$, we will have $\\tilde y = \\tilde\\beta_0 + \\tilde\\beta_1 x_1$. An interesting algebraic relationship is $\\tilde\\beta_1 = \\hat\\beta_1 + \\hat\\beta_2 \\tilde\\delta_1$. Thus, $\\EE{\\tilde\\beta_1} = \\beta_1 + \\beta_2 \\tilde\\delta_1$ and $\\text{Bias}\\P{\\tilde\\beta_1} = \\EE{\\tilde\\beta_1} - \\beta_1 = \\beta_2 \\tilde\\delta_1$, called the ***omitted variable***.\n",
    "\n",
    "1. $\\beta_2 = 0$: when it just really not a variable in the **true model**.\n",
    "2. $\\tilde\\delta_1 = 0$: since $\\tilde\\delta_1$ is the sample covariance between $x_1$ and $x_2$ over the sample variance of $x_1$, it's $0$ $iff$ $x_1$ and $x_2$ are *uncorrelated* in the sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Omitted Variable Bias: More general Cases\n",
    "\n",
    "Suppose the population model: $y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + u$ satisfies the Assumptions $\\text{MLR}.1$ to $\\text{MLR}.4$. However if we omit the variable $x_3$, the estimated model is $\\tilde y = \\tilde\\beta_0 + \\tilde\\beta_1 x_1 + \\tilde\\beta_2 x_2$.\n",
    "\n",
    "To see how $\\EE{\\tilde\\beta_1}$ and $\\EE{\\tilde\\beta_2}$ are biased, we write them out. To obtain a value for this, we first need to assume that $x_1$ and $x_2$ are uncorrelated, then\n",
    "\n",
    "$$\\begin{align}\n",
    "\\EE{\\tilde\\beta_1} &= \\EE{\\hat\\beta_1 + \\hat\\beta_3 \\tilde\\delta_1} & \\EE{\\tilde\\beta_2} &= \\EE{\\hat\\beta_2 + \\hat\\beta_3 \\tilde\\delta_2}\\\\\n",
    "&= \\beta_1 + \\beta_3 \\cdot \\ffrac{\\d{\\sum_{i=1}^{n} \\P{x_{i1} - \\bar{x}_1}x_{i3}}} {\\d{\\sum_{i=1}^{n} \\P{x_{i1} - \\bar{x}_1}^2}} &&= \\beta_2 + \\beta_3 \\cdot \\ffrac{\\d{\\sum_{i=1}^{n} \\P{x_{i2} - \\bar{x}_2}x_{i3}}} {\\d{\\sum_{i=1}^{n} \\P{x_{i2} - \\bar{x}_2}^2}}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Variance of the OLS Estimators\n",
    "\n",
    "$Assumption$ $\\text{MLR}.5$\n",
    "- Homoscedasticity: The value of the explanatory variables must contain no information about the variance of the unobserved factors: $\\Var{u \\mid x_{1},x_{2},\\dots, x_{k}} = \\sigma^2$\n",
    "\n",
    "$Remark$\n",
    "\n",
    "$\\text{MLR}.1$ to $\\text{MLR}.5$ are collectively known as the ***Gauss-Markov assumptions*** (for cross-sectional regression).\n",
    "\n",
    "$Theorem.2$\n",
    "\n",
    "By assumptions $\\text{MLR}.1$ to $\\text{MLR}.5$, we claim that $\\Var{\\hat\\beta_j} = \\ffrac{\\sigma^2} {\\text{SST}_j \\P{1-R_j^2}}$ for $j = 1,2,\\dots,k$. Here $\\text{SST}_j$ is the **Total sample variation** in explanatory variable $x_j$: $\\sum_{i=1}^{n}\\P{x_{ij} - \\bar x_j}^2$ and $R_j^2 = \\rho^2\\P{x_j,\\hat x_j} = \\ffrac{\\P{\\sum\\limits_{i=1}^{n} \\P{x_{ij} - \\bar x_j}\\P{\\hat x_{ij} - \\bar{\\hat x_j}}}^2} {\\P{\\sum\\limits_{i=1}^{n} \\P{x_{ij} - \\bar{x_j}}^2}\\P{\\sum\\limits_{i=1}^{n} \\P{\\hat x_{ij} - \\bar{\\hat x_j}}^2}}$. This $R_j^2$ is from regressing $x_j$ on all other independent variables (and including an intercept).\n",
    "\n",
    "### The Components of The OLS Variances: Multicollinearity\n",
    "\n",
    "- The Error Variance: $\\sigma^2$.\n",
    "    - Bigger error variance, bigger sampling variance, less imprecise the estimation\n",
    "- The total Sample Variation in $x_j$: $\\text{SST}$ \n",
    "    - More sample, higher $\\text{SST}$, more accurate\n",
    "    - No sample variance is so rare and not allowed by $\\text{MLR}.4$\n",
    "    - ***micronumerosity***: Small sample size can lead to large sampling variances $\\text{SST}_j$\n",
    "- the Linear relationships among the Independent Variables: $R_j^2$\n",
    "    - If two are correlated, then $R_j \\to 1$ which greatly magnify the variance\n",
    "    - ***multicollinearity***: high (but not perfect) correlation between two or more independent variables\n",
    "\n",
    "Here we call $1/\\P{1-R_j^2}$ the ***Variance Inflation Factor***. And the conlusion is: dropping some variables will reduce the multicollinearity while lead to omitted variable bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variances in Misspecified Models\n",
    "\n",
    "- True Model: $y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + u$\n",
    "- Estimated Model: $\\hat y = \\hat \\beta_0 + \\hat \\beta_1 x_1 + \\hat \\beta_2 x_2$\n",
    "- Estimated Model with $\\beta_2$ omitted: $\\tilde y = \\tilde \\beta_0 + \\tilde \\beta_1 x_1$\n",
    "\n",
    "Their variances are: $\\Var{\\hat \\beta_1} = \\ffrac{\\sigma^2} {\\text{SST}_1 \\P{1-R_1^2}}$ and $\\Var{\\tilde \\beta_1} = \\ffrac{\\sigma^2} {\\text{SST}_1}$. And we can divine this into two cases:\n",
    "\n",
    "1. for $\\beta_2  =   0$, $\\EE{\\hat \\beta_1} = \\beta_1$ and $\\EE{\\tilde\\beta_1} = \\beta_1$, besides, $\\Var{\\tilde\\beta_1} < \\Var{\\hat\\beta_1}$\n",
    "2. for $\\beta_2 \\neq 0$, $\\EE{\\hat \\beta_1} = \\beta_1$ and $\\EE{\\tilde\\beta_1} \\neq \\beta_1$, but still $\\Var{\\tilde\\beta_1} < \\Var{\\hat\\beta_1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating $\\sigma^2$: Standard Errors of the OLS Estimators\n",
    "\n",
    "Analogy to the simple regresion, $\\hat\\sigma^2 = \\ffrac{1} {n-k-1} \\sum \\hat u_i^2 = \\ffrac{\\text{SSR}} {n-k-1}$, unbiased. Here's the theorem\n",
    "\n",
    "$Remark$\n",
    "\n",
    "$n-k-1$ is the ***degrees of freedom***: $\\text{number of observations} - \\text{number of estimated parameters}$\n",
    "\n",
    "$Theorem.3$\n",
    "\n",
    "Under $\\text{MLR}.1$ through $\\text{MLR}.5$, $\\EE{\\hat\\sigma^2} = \\sigma^2$\n",
    "\n",
    "Here, $\\hat\\sigma^2$ is called the ***standard error of the regression (SER)***.\n",
    "\n",
    "Then we can use this to estimate the sampling variation. The ***standard deviation*** of $\\hat\\beta_j$: \n",
    "\n",
    "$$\\text{sd}\\P{\\hat\\beta_j} = \\sqrt{\\Var{\\hat\\beta_j}} = \\sqrt{\\ffrac{1} {\\text{SST}_j \\P{1-R_j^2}}}\\sigma$$\n",
    "\n",
    "and then the estimated one, ***standard error*** of $\\hat\\beta_j$, by replacing the $\\sigma$ in the last expression with $\\hat\\sigma$:\n",
    "\n",
    "$$\\text{se}\\P{\\hat\\beta_j} = \\sqrt{\\widehat{\\Var{\\hat\\beta_j}}} = \\sqrt{\\ffrac{1} {\\text{SST}_j \\P{1-R_j^2}}}\\hat\\sigma$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficiency of OLS: The Gauss-Markov Theorem\n",
    "\n",
    "1. Under Assumption $\\text{MLR}.1$ to $\\text{MLR}.4$, OLS is unbiased\n",
    "2. And then $\\text{MLR}.5$, it becomes the one with the smallest variance\n",
    "\n",
    "Thus we call this estimation the ***best linear unbiased estimators (BLUE)*** of the regression coefficients.\n",
    "\n",
    "$Remark$\n",
    "\n",
    "Linear here means that the estimator can be expressed as a weighted sum of dependent variables:\n",
    "\n",
    "$$\\tilde\\beta_j = \\sum_{i=1}^{n} w_{ij}y_i$$\n",
    "\n",
    "And best means the least variance among all others.\n",
    "\n",
    "$Theorem.4$ GAUSS-MARKOV THEOREM\n",
    "\n",
    "Under Assumption $\\text{MLR}.1$ through $\\text{MLR}.5$, $\\hat\\beta_1, \\hat\\beta_2,\\dots,\\hat\\beta_k$, the OLS estimators, are the ***best linear unbiased estimators (BLUEs)*** of $\\beta_1, \\beta_2,\\dots,\\beta_k$, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "267px",
    "width": "286px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
