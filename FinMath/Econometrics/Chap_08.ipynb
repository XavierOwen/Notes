{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heteroskedasticity\n",
    "## Consequences of Heteroskedasticity for OLS\n",
    "\n",
    "$\\DeclareMathOperator*{\\argmin}{argmin}\n",
    "\\DeclareMathOperator*{\\argmax}{argmax}\n",
    "\\DeclareMathOperator*{\\plim}{plim}\n",
    "\\newcommand{\\using}[1]{\\stackrel{\\mathrm{#1}}{=}}\n",
    "\\newcommand{\\ffrac}{\\displaystyle \\frac}\n",
    "\\newcommand{\\asim}{\\overset{\\text{a}}{\\sim}}\n",
    "\\newcommand{\\space}{\\text{ }}\n",
    "\\newcommand{\\bspace}{\\;\\;\\;\\;}\n",
    "\\newcommand{\\QQQ}{\\boxed{?\\:}}\n",
    "\\newcommand{\\void}{\\left.\\right.}\n",
    "\\newcommand{\\Tran}[1]{{#1}^{\\mathrm{T}}}\n",
    "\\newcommand{\\d}[1]{\\displaystyle{#1}}\n",
    "\\newcommand{\\CB}[1]{\\left\\{ #1 \\right\\}}\n",
    "\\newcommand{\\SB}[1]{\\left[ #1 \\right]}\n",
    "\\newcommand{\\P}[1]{\\left( #1 \\right)}\n",
    "\\newcommand{\\abs}[1]{\\left| #1 \\right|}\n",
    "\\newcommand{\\norm}[1]{\\left\\| #1 \\right\\|}\n",
    "\\newcommand{\\dd}{\\mathrm{d}}\n",
    "\\newcommand{\\Exp}{\\mathrm{E}}\n",
    "\\newcommand{\\RR}{\\mathbb{R}}\n",
    "\\newcommand{\\EE}{\\mathbb{E}}\n",
    "\\newcommand{\\II}{\\mathbb{I}}\n",
    "\\newcommand{\\NN}{\\mathbb{N}}\n",
    "\\newcommand{\\ZZ}{\\mathbb{Z}}\n",
    "\\newcommand{\\QQ}{\\mathbb{Q}}\n",
    "\\newcommand{\\PP}{\\mathbb{P}}\n",
    "\\newcommand{\\AcA}{\\mathcal{A}}\n",
    "\\newcommand{\\FcF}{\\mathcal{F}}\n",
    "\\newcommand{\\AsA}{\\mathscr{A}}\n",
    "\\newcommand{\\FsF}{\\mathscr{F}}\n",
    "\\newcommand{\\Var}[2][\\,\\!]{\\mathrm{Var}_{#1}\\left[#2\\right]}\n",
    "\\newcommand{\\Avar}[2][\\,\\!]{\\mathrm{Avar}_{#1}\\left[#2\\right]}\n",
    "\\newcommand{\\Cov}[2][\\,\\!]{\\mathrm{Cov}_{#1}\\left(#2\\right)}\n",
    "\\newcommand{\\Corr}[2][\\,\\!]{\\mathrm{Corr}_{#1}\\left(#2\\right)}\n",
    "\\newcommand{\\I}[1]{\\mathrm{I}\\left( #1 \\right)}\n",
    "\\newcommand{\\N}[1]{\\mathcal{N} \\left( #1 \\right)}\n",
    "\\newcommand{\\ow}{\\text{otherwise}}\n",
    "\\newcommand{\\FSD}{\\text{FSD}}Review$\n",
    "\n",
    ">Homoskedasticity assumption $\\text{MLR}.5$, that $\\Var{u\\mid x_1,x_2,\\dots, x_k} = \\sigma^2$, plays no role in showing whether OLS was unbiased or consistent. Only something like omitting an important variable would have this effect.\n",
    ">\n",
    ">Also, $R^2$ and $\\bar R^2$ are unaffected by the presence of heteroskedasticity. They are the estimators of population $R^2 = 1 - \\sigma_u^2/\\sigma_y^2$ where the two variances are *un*conditional while heteroskedasticity is under conditioning on $\\mathbf{x}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under $\\text{MLR}.1$ to $\\text{MLR}.4$, $\\text{SSR}/n$ consistently estimates $\\sigma_u^2$ and $\\text{SST}/n$ consistently estimates $\\sigma_y^2$. Therefore, $R^2$ and $\\bar R^2$ are both consistant estimators of the population $R^2$, whether or not the homoskedasticity assumption holds.\n",
    "\n",
    "But to do the inference by $t$ statistic and $F$ statistic, we still need that assumption. Besides, if $\\Var{u\\mid\\mathbf{x}}$ is no longer constant, OLS is no longer BLUE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heteroskedasticity-Robust Inference after OLS Estimation\n",
    "\n",
    "Consider the simple linear regression, the estimator of slope parameter is\n",
    "\n",
    "$$\\hat\\beta_1 = \\beta_1 + \\ffrac{\\d{\\sum_{i=1}^{n} \\P{x_i - \\bar x}} u_i}{\\d{\\sum_{i=1}^{n} \\P{x_i - \\bar x}^2}}$$\n",
    "\n",
    "$$\\Var{u_i\\mid x_i} = \\sigma_i^2\\;{\\Longrightarrow}\\;\\Var{\\hat\\beta_1} = \\ffrac{\\sum \\P{x_i - \\bar x}^2\\sigma_i^2}{\\text{SST}_x^2}$$\n",
    "\n",
    "Now we give a valid estimator for general MLR\n",
    "\n",
    "$$\\widehat{\\Var{\\hat \\beta_j}} = \\ffrac{\\d{\\sum_{i=1}^{n}\\hat r_{ij}^2 \\hat u_i^2}}{\\text{SSR}_j^2}$$\n",
    "\n",
    "where $\\hat r_{ij}$ denotes the $i$th residual from regressing $x_j$ on all other independent variables, and $\\text{SSR}_j$ is the sum of squared residuals from this regression. More than that, we have the ***heteroskedasticity-robust standard error*** for $\\hat\\beta_j$:\n",
    "\n",
    "$$\\sqrt{\\widehat{\\Var{\\hat \\beta_j}}}= \\ffrac{\\d{\\sqrt{\\sum_{i=1}^{n}\\hat r_{ij}^2 \\hat u_i^2}}}{\\text{SSR}_j^2}$$\n",
    "\n",
    "Here the $\\text{SSR}_j^2$ can be replaced by $\\text{SST}_j^2\\P{1-R_j^2}$, where $\\text{SST}_j^2$ is the total sum of squares of $x_j$, and $R_j^2$ is the usual $R^2$ from regressing $x_j$ on all other explanatory variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the ***heteroskedasticity-robust $t$ statistic***.\n",
    "\n",
    "$$t = \\ffrac{\\text{estimate} - \\text{hypothesized value}}{\\textbf{heteroskedasticity-robust} \\text{ standard error}}$$\n",
    "\n",
    "Using these formulas, the usual $t$ test is valid asymptotically; but the usual $F$-statistic does not work under heteroskedasticity.\n",
    "\n",
    "### Computing Heteroskedasticity-Robust LM Tests\n",
    "\n",
    "Skipped, however. OK, the usual LM statistic:\n",
    "\n",
    "1. Estimate the restricted model to obtain the residual $\\tilde u$\n",
    "2. regress $\\tilde u$ on all of the independent variables\n",
    "3. $\\text{LM} = n \\cdot R_{\\tilde u}^2$, where $R_{\\tilde u}^2$ is the $R^2$ from this regression\n",
    "\n",
    "Then the HR LM statistic in the general case\n",
    "\n",
    "1. The same: estimate the restricted model to obtain the residuals $\\tilde u$\n",
    "2. Regress each of the independent variables which are excluded under the null hypothesis, on all of the included variables; say there're $q$ excluded variables (restricted model for them: $x_j = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_{k-q} x_{k-q} + u$, $j = k-q+1,\\dots,k$)\n",
    "3. Obtain $q$ sets of residuals $\\P{\\tilde r_1,\\tilde r_2,\\dots, \\tilde r_q}$, then find the element-wise production of $\\tilde r_j$ and $\\tilde u$\n",
    "4. Regress $y\\equiv 1$ on $\\beta = 0, \\tilde r_1 \\tilde u, \\tilde r_2 \\tilde u,\\dots,\\tilde r_q \\tilde u$\n",
    "5. The **Heteroskedasticity-Robust LM Statistic** is now $n-\\text{SSR}_1$ (I bet this is a minus sign, it could be some other signs though...), where $\\text{SSR}_1$ is just the usual sum of squared residuals from the regression in the final step.\n",
    "\n",
    "By the way, under $H_0$, $\\text{LM}$ is distributed approximately as $\\chi_q^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing for Heteroskedasticity\n",
    "\n",
    "Model: $y = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_k x_k + u$; assumption: $\\text{MLR}.1$ through $\\text{MLR}.4$, so that the OLS estimators are still unbiased and consistent.\n",
    "\n",
    "To test the heteroskedasticity, we have $H_0: \\Var{u\\mid x_1,x_2,\\dots,x_k} = \\sigma^2$. Since $u$ has a zero conditional expectation, this is equivalent to\n",
    "\n",
    "$$H_0: \\Exp\\SB{u^2\\mid x_1,\\dots,x_k} = \\Exp\\SB{u^2} = \\sigma^2$$\n",
    "\n",
    "So we are actually testing weather $u^2$ is related (in expected value) to one or more of the explanatory variables. Then we assume the linear function\n",
    "\n",
    "$$u^2 = \\delta_0 + \\delta_1 x_1 + \\delta_2 x_2 + \\cdots + \\delta_k x_k + v$$\n",
    "\n",
    "where $v$ is an error term with mean zero given the $x_j$. Then we rewrite the null hypothesis of homoskedasticity as $H_0: \\delta_1 = \\delta_2 = \\cdots = \\delta_k = 0$.\n",
    "\n",
    "Then we can use $F$ statistic or $\\text{LM}$ to test this. And to do so, we first need to estimate the left side, the residual and since it's unable to obtain, we will use its estimation $\\hat u_i$ so actually the equation to be process is actually\n",
    "\n",
    "$$\\hat u^2 = \\delta_0 + \\delta_1 x_1 + \\delta_2 x_2 + \\cdots + \\delta_k x_k + \\text{error}$$\n",
    "\n",
    "Then apply the $F$ test or $\\text{LM}$ test. And to distinguish two different $R$-square, we denote the $F$ statistic in this regression\n",
    "\n",
    "$$F = \\ffrac{\\ffrac{R_{\\hat u^2}^2}{k}}{\\ffrac{1-R_{\\hat u^2}^2}{n-k-1}}$$\n",
    "\n",
    "And the $\\text{LM}$ statistic is $\\text{LM} = n\\cdot R_{\\hat u^2}^2$. Under $H_0$, it's distributed asymptotically as $\\chi_k^2$. The $\\text{LM}$ version of the test is typically called the ***Breusch-Pagan test for heteroskedasticity (BP test)***.\n",
    "\n",
    "$Remark$\n",
    "\n",
    ">Larger $R_{\\hat u^2}^2$ could be the evidence against the null hypothesis.\n",
    "\n",
    "**Steps for BP test**:\n",
    "\n",
    "1. Estimate the model $y = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_k x_k + u$ by OLS, as usual. And for each observation, find the residual $\\hat u^2$\n",
    "2. Regression the equation $\\hat u^2 = \\delta_0 + \\delta_1 x_1 + \\delta_2 x_2 + \\cdots + \\delta_k x_k + \\text{error}$, with the $R$-squared $R_{\\hat u^2}^2$.\n",
    "3.  Form either the $F$ statistic or the $\\text{LM}$ statistic and compute the $p$-value. Use $F_{k,n-k-1}$ distribution for $F$ statistic and $\\chi_k^2$ distribution for the otherone. If the $p$-value is below the chosen significance level, meaning that it's sufficiently small, we reject the null hypothesis and admit the heteroskedasticity.\n",
    "\n",
    "$Remark$\n",
    "\n",
    "> If we suspect that heteroskedasticity depends only upon certain independent variables, we can simple modify the BP test that we regress $\\hat u^2$ only on the chosen variables and then carry out the appropriate $F$ or $\\text{LM}$ test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The White Test for Heteroskedasticity\n",
    "\n",
    "The ***White test for heteroskedasticity*** is the $\\text{LM}$ statistic for testing that all of the $\\delta_j$ where $j=1,2,\\dots$ (no $0$ here), defined by\n",
    "\n",
    "$$\\begin{align}\n",
    "\\hat u^2 &= \\P{y - \\hat\\beta_0 - \\hat\\beta_1 x_1 - \\cdots - \\hat\\beta_k x_k}^2 \\\\\n",
    "&\\equiv \\delta_0 + \\sum_{i=1}^k \\delta_i x_i + \\sum_{i=1}^k \\delta_{k+i} x_i^2  + \\sum_{j<k} \\delta_{i\\P{j,k}} x_jx_k + \\text{error}\n",
    "\\end{align}$$\n",
    "\n",
    "$F$ statistics can also do, however one drawback is that this method is of too many parameters so that with less $df$. So here's a better one.\n",
    "\n",
    "$$\\hat u^2 = \\delta_0 + \\delta_1 \\hat y + \\delta_2 \\hat y^2 + \\text{error}$$\n",
    "\n",
    "$H_0: \\delta_1 = \\delta_2 = 0$; $\\text{LM} = n \\cdot R_{\\hat u^2}^2\\sim \\chi_2^2$\n",
    "\n",
    "**Case**:\n",
    "\n",
    "1. Estimate the model $y = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_k x_k + u$ by OLS, as usual. And for each observation, find the residual $\\hat u$ and fitted value $\\hat y$; then use the two to compute $\\hat u^2$ and $\\hat y^2$\n",
    "2. Regress $\\hat u^2 = \\delta_0 + \\delta_1 \\hat y + \\delta_2 \\hat y^2 + \\text{error}$. Keep the $R$-squared from this regression, $R_{\\hat u^2}^2$\n",
    "3. Form either the $F$ or $\\text{LM}$ statistic and compute the $p$-value. Use $F_{2,n-3}$ or $\\chi_2^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted Least Squares Estimation\n",
    "\n",
    "If we have correctly specified the form of the variance, then weighted least squares (WLS) is more efficient than OLS, and WLS leads to new $t$ and $F$ statistics that have $t$ and $F$ distributions.\n",
    "\n",
    "### The Heteroskedasticity is Known up to a Multiplicative Constant\n",
    "\n",
    "Model: $y = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_k x_k + u$, and let $\\mathbf{x}$ denote all the explanatory variables. We assume that\n",
    "\n",
    "$$\\Var{u\\mid\\mathbf{x}} = \\sigma^2 h\\P{\\mathbf x}$$\n",
    "\n",
    "where $h\\P{\\mathbf x}$ is some function of the explanatory variables that determines the heteroskedasticity. It's *known* and *positive*. Only the population parameter $\\sigma^2$ is required to be estimated. Then we take the original equation\n",
    "\n",
    "$$y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_k x_{ik} + u_i$$\n",
    "\n",
    "We'll transform it into an equation that has homoskedastic errors under Gauss-Markov Assumptions.\n",
    "\n",
    "$$\\Exp\\SB{u_i\\mid\\mathbf x_i} = 0\\Rightarrow\\Exp\\SB{\\ffrac{u_i}{h_i}\\mid\\mathbf x_i} = 0$$\n",
    "\n",
    "$$\\Var{u_i\\mid \\mathbf x_i} = \\Exp\\SB{u_i^2\\mid \\mathbf x_i} = \\sigma^2 h_i$$\n",
    "\n",
    "$$\\Exp\\SB{\\P{\\ffrac{u_i}{\\sqrt{h_i}}}^2\\mid\\mathbf x_i} = \\ffrac{\\Exp\\SB{u_i^2\\mid\\mathbf x_i}}{h_i} = \\ffrac{\\sigma^2 h_i}{h_i} = \\sigma^2$$\n",
    "\n",
    "Then inspired by this, we divide $\\sqrt{h_i}$ on both sides of the equation.\n",
    "\n",
    "$$\\ffrac{y_i}{\\sqrt{h_i}} = \\ffrac{\\beta_0}{\\sqrt{h_i}} + \\beta_1\\P{\\ffrac{x_{i1}}{\\sqrt{h_i}}} + \\beta_2\\P{\\ffrac{x_{i2}}{\\sqrt{h_i}}} + \\cdots + \\beta_k\\P{\\ffrac{x_{ik}}{\\sqrt{h_i}}}+ \\ffrac{u_i}{\\sqrt{h_i}}$$\n",
    "\n",
    "and we'll rewrite this as\n",
    "\n",
    "$$y_i^* = \\beta_0x_{i0}^* + \\beta_1x_{i1}^* + \\cdots + \\beta_kx_{ik}^* + u_{i}^*,\\bspace x_{i0}^* = \\ffrac{1}{\\sqrt{h_i}}$$\n",
    "\n",
    "then we can use the Gauss-Markov assumptions. More than that, $\\text{MLR}.6$ can be kept holding true after that transform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimators $\\beta_j^*$ here are examples of ***generalized least squares (GLS) estimators***. And WLS is one special case that is used to account for heteroskedasticity in the errors. Mathematically, the WLS estimators are the values of the $b_j$ that make \n",
    "\n",
    "$$\\sum_{i=1}^{n}\\ffrac{1}{h_i} \\P{y_i - b_0 - b_1 x_{i1} - \\cdots - b_kx_{ik}}^2$$\n",
    "\n",
    "as small as possible.\n",
    "\n",
    "***\n",
    "One special case that when the observations are reported as averages of certain level, the variables should be weighted by the size of the unit. A simple example:\n",
    "\n",
    "$$\\overline{\\text{contrib}}_i = \\beta_0 + \\beta_1 \\overline{\\text{earns}}_i + \\beta_2 \\overline{\\text{age}}_i + \\beta_3 \\text{mrate}_i + \\overline u_i$$\n",
    "\n",
    "It can be considered an example for a average from a firm $i$'s total $m_i$ workers, where $\\overline u_i = m_i^{-1} \\sum_{e=1}^{m_i} u_{i,e}$.\n",
    "\n",
    "If, from the respect of individuals, the data are homoskedastic, $\\Var{u_{i,e}} = \\sigma^2$, then from the respect of firms, we have $\\Var{\\bar u_i} = \\sigma^2/m_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Heteroskedasticity Function Must Be Estimated: Feasible GLS\n",
    "\n",
    "For the unknown heteroscedasticity function, we assume a general one\n",
    "\n",
    "$$\\Var{u\\mid \\mathbf x} = \\sigma^2 \\exp\\CB{\\delta_0 + \\delta_1 x_1 + \\cdots + \\delta_k x_k} = \\sigma^2 h\\P{\\mathbf x}$$\n",
    "\n",
    "To estimate $\\delta_j$, we write\n",
    "\n",
    "$$u^2 = \\sigma^2 \\exp\\CB{\\delta_0 + \\delta_1 x_1 + \\cdots + \\delta_k x_k}v$$\n",
    "\n",
    "Further, if we assume that $\\Exp\\SB{v \\mid \\mathbf x} = 1$, we have\n",
    "\n",
    "$$\\log\\P{u^2} = \\alpha_0 + \\delta_1 x_1 + \\cdots + \\delta_k x_k + e$$\n",
    "\n",
    "where $e$ has a zero mean expectation and is independent from $\\mathbf x$. This is the one that satisfies all Gauss-Markov assumptions so that we can obtain $\\hat \\delta_j$. Denote the fitted value in this regression as $\\hat g_i$ then we have the weight:\n",
    "\n",
    "$$\\ffrac{1}{h_i} = \\ffrac{1}{\\hat h_i} = e^{-\\hat g_i}$$\n",
    "\n",
    "**Step**:\n",
    "\n",
    "1. Regress $y$ on $x_1,x_2,\\dots,x_k$ and obtain the residual $\\hat u$\n",
    "2. Obtain $\\log\\P{\\hat u^2}$\n",
    "3. Regress $\\log\\P{u^2} = \\alpha_0 + \\delta_1 x_1 + \\cdots + \\delta_k x_k + e$, then let $\\hat g_i$ denote the fitted value\n",
    "4. $\\hat h = \\exp\\CB{\\hat g} = \\exp\\CB{\\hat \\alpha_0 + \\hat\\delta_1 x_1 + \\cdots + \\hat\\delta_k x_k}$\n",
    "5. Use weight $1/\\hat h$ and then apply WLS on $y =\\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_k x_k + u$\n",
    "***\n",
    "<center>Feasible GLS is consistent and asymptotically more efficient than OLS.</center>\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if the Assumed Heteroskedasticity Function is Wrong?\n",
    "\n",
    "- WLS is still consistent under $\\text{MLR}.1$ through $\\text{MLR}.4$\n",
    "- robust standard errors should be computed\n",
    "- WLS is consistent under $\\text{MLR}.4$ but not necessarily under $\\text{MLR}.4'$\n",
    "\n",
    "### Prediction and Prediction Intervals with Heteroskedasticity\n",
    "\n",
    "## The Linear Probability Model Revisited\n",
    "\n",
    "$$\\Var{y\\mid \\mathbf x} = p\\P{\\mathbf x} \\P{1-p\\P{\\mathbf x}}\\Rightarrow \\hat h_i = \\hat y_i \\P{1-\\hat y_i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "129px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
