{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at data - Distribution\n",
    "$Def$  \n",
    "**Statistics**: The science of learning from Data.\n",
    "\n",
    "**Cases**: the objects described by a set of data.\n",
    "> **e.g.**  \n",
    ">Customers, companies, experimental subjects, or other objects.\n",
    "\n",
    "**Variable**: a special characteristic of a case. \n",
    "\n",
    "**Label**: a special variable used in some data sets to distinguish between cases.\n",
    "  \n",
    "$\\odot$\n",
    "Different cases can have different values of a variable.$\\square$\n",
    "\n",
    ">**e.g.**  \n",
    ">\n",
    "| Number | Name |   Album   |  Genre  |\n",
    "|:------:|:----:|:---------:|:-------:|\n",
    "|    1   |  ABC | Jackson 5 |   Pop   |\n",
    "|    2   |  XYZ | Jackson 6 | Country |\n",
    "|    3   |  123 | Jackson 5 |   Pop   |\n",
    ">\n",
    ">1. Lable: {1,2,3,...}\n",
    ">2. Variables: \"Number\", \"Name\", \"Album\", \"Genre\"\n",
    ">3. Cases, totally 3 cases, each is a row\n",
    "\n",
    "## Variables\n",
    "We construct a set of data by first deciding which *cases* or units we want to study. For each case, we record information about characteristics that we call *variables*.\n",
    "\n",
    "Two kinds:\n",
    "1. Categorical Variable: Places individual into one of several groups or categories\n",
    "2. Quantitative Variable: Takes numerical values for which arithmetic operations make sense\n",
    "\n",
    "$\\dagger$  \n",
    "NOT ALL numbers are Quant.V., **operations required**.\n",
    "\n",
    "## Displaying\n",
    "The nature of the variable decides the graphical tools.\n",
    "\n",
    "1. Categorical Variable\n",
    "  - Bar graphs \n",
    "  - Pie charts\n",
    "2. Quantitative Variable\n",
    "  - Histograms \n",
    "  - Stemplots\n",
    "  - Time plots\n",
    "  \n",
    ">|  ID | Name | Grade | totalPoints |\n",
    "|:---:|:----:|:-----:|:-----------:|\n",
    "| 101 |  ABC |   A   |     956     |\n",
    "| 102 |  XYZ |   F   |     125     |\n",
    "| 103 |  123 |   C   |     693     |\n",
    ">\n",
    ">Here ID, Name, Grade are categorical variables, and only totalPoints is the quantitative variable.\n",
    "\n",
    "**Distribution**: which tells us the values that a variable takes and how often it takes each value.\n",
    "\n",
    "### Categorical Variable\n",
    "|       Pie Chart       |    Bar Graph    |\n",
    "|:---------------------:|:---------------:|\n",
    "|           ![Bar Graphs](./Raw/Bar Graphs.png)           |        ![Pie Charts](./Raw/Pie Charts.png)       |\n",
    "|PCs show the distribution of a categorical variable as a “pie” whose slices are sized by the counts or percents for the categories.|BGs represent categories as bars whose heights show the category counts or percents.|\n",
    "| porpotion of the whole | roughly amounts |\n",
    "\n",
    "### Stemplots\n",
    "Separate each observation into a stem and a leaf that are then plotted to display the distribution while maintaining the original values of the variable.\n",
    "\n",
    "Figure like this: 16, 43, 38, 48, 42, 23, 36, 35, 37, 34, 25, 28, 26, 43, 51, 33, 40, 35, 41, 42, can be drawn in this way:  \n",
    "![](./Raw/Stemplots.jpg)\n",
    "1. Write the *stems*. \n",
    "2. Go through the data and write each *leaf* on the proper stem. \n",
    "3. Rearrange the leaves\n",
    "\n",
    "### Histograms\n",
    "Show the distribution of a quantitative variable by bars. The height of a bar represents the number of individuals whose values fall within the corresponding class.\n",
    "\n",
    "$\\odot$\n",
    "For large datasets and/or quantitative variables that take many values.$\\square$\n",
    "\n",
    ">**e.g.**  \n",
    ">\n",
    "|       Figures       |    Plot    |\n",
    "|:-------------------:|:----------:|\n",
    "|![](./Raw/Data_Histogram.png)|![](./Raw/Histograms.png)|\n",
    "> Here $-<$ means that it is greater or equal to the left side but must be less than the right side, or briefly, from the left side up to the right side.\n",
    "\n",
    "### Time plots\n",
    "The behavior over time.\n",
    "\n",
    "![](./Raw/Time plots.png)\n",
    "\n",
    "### Examine Distributions\n",
    "**Outlier**: an individual that falls outside the overall pattern.\n",
    "\n",
    "And about the symmetricity:\n",
    "\n",
    "|       **Symmetric**       |      **Left-skewed**     |      **Right-skewed**     |\n",
    "|:-------------------------:|:------------------------:|:-------------------------:|\n",
    "|  ![](./Raw/Symmetric.png) |![](./Raw/Left-skewed.png)|![](./Raw/Right-skewed.png)|\n",
    "|Bell Curve: the right and left sides of the graph are approximately mirror images of each other.|A ***right-leaning*** curve: the right side of the graph (containing the half of the observations with larger values) is much longer than the left side.|A ***left-leaning*** curve: the left side of the graph is much longer than the right side.|\n",
    "\n",
    "## Describing Distributions with Numbers\n",
    "### Measure fot the center\n",
    "**Mean**: Average  \n",
    "$$\\bar{x} = \\frac{1} {n} \\sum x_i = \\frac{\\textrm{sum of observations}} {n}$$\n",
    "\n",
    "**Median**: The midpoint of a distribution, the number such that half of the observations are smaller and the other half are larger.\n",
    "\n",
    "$\\odot$\n",
    "The mean cannot resist the influence of extreme observations (outliers), it is not a resistant measure of center, while median is unaffeted by that.$\\square$\n",
    " \n",
    "1. If n is *odd*, the median M is the center observation in the ordered list.\n",
    "2. If n is *even*, the median M is the average of the two center observations in the ordered list.\n",
    "\n",
    "$Compare$\n",
    "\n",
    "|       **Symmetric**       |      **Left-skewed**     |      **Right-skewed**     |\n",
    "|:-------------------------:|:------------------------:|:-------------------------:|\n",
    "|  ![](./Raw/comparingMeanAndMedian_Symmetric.png) |![](./Raw/comparingMeanAndMedian_Left-skewed.png)|![](./Raw/comparingMeanAndMedian_Right-skewed.png)|\n",
    "|$\\mathrm{Mean} = \\mathrm{Median}$|$\\mathrm{Mean} < \\mathrm{Median}$|$\\mathrm{Mean} > \\mathrm{Median}$|\n",
    "\n",
    "### Measure for the spread, the quartiles\n",
    "1. First quartile $Q_1$: the median of the observations located to the left of the median in the ordered list.\n",
    "2. Third quartile $Q_3$: the median of the observations located to the right of the median in the ordered list.\n",
    "3. **Interquartile range** (IQR): $IQR = Q_3 - Q_1$\n",
    "\n",
    "### Five-Number Summary\n",
    "a distribution consists of the smallest observation, the first quartile, the median, the third quartile, and the largest observation, written in order from smallest to largest.\n",
    "\n",
    "1. $\\min$\n",
    "2. $Q_1$\n",
    "3. $M=Q_2$\n",
    "4. $Q_3$\n",
    "5. $\\max$\n",
    "\n",
    ">**e.g.**  \n",
    ">![](./Raw/Boxplots.png)\n",
    "\n",
    "### 1.5 × IQR Rule\n",
    "$Conclusion$\n",
    "We call an observation an outlier if it falls more than 1.5 × IQR above the third quartile or below the first quartile.\n",
    "> **e.g.**  \n",
    "> For the New York travel time data:\n",
    ">\n",
    "|  Stems | 0 |      1      |    2    |  3  |   4   | 5 |   6   | 7 | 8 |\n",
    "|:------:|:-:|:-----------:|:-------:|:---:|:-----:|:-:|:-----:|:-:|:-:|\n",
    "| Leaves | 5 | 0,0,5,5,5,5 | 0,0,0,5 | 0,0 | 0,0,5 |   | 0,0,5 |   | 5 |\n",
    ">\n",
    ">$$\n",
    "\\begin{align*}\n",
    "Q_1 &= 15 \\\\\n",
    "Q_3 &= 42.5\\\\\n",
    "IQR &= 27.5\\\\\n",
    "1.5 \\times IQR &= 41.25 \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    ">\n",
    ">So the data that is not an outlier must lie in range: \n",
    ">\n",
    "$$\\left [Q_1 – 1.5 \\times IQR, Q_3 + 1.5 \\times IQR \\right ] = [-26.25,83.75]$$\n",
    "\n",
    "### Measure for the spread, the Standard Deviation $S_x$, or $\\sigma$\n",
    "**standard deviation**: measures the average distance of the observations from their mean.\n",
    "1. $S_x$ measures spread (deviation) about the mean and should be used only when the mean is the measure of center.\n",
    "2. $S_x = 0$ only when all observations have the same value and there is no spread. Otherwise, $s > 0$.\n",
    "3. $S_x$ is influenced by outliers. \n",
    "4. $S_x$ has the same units of measurement as the original observations. \n",
    "\n",
    "  \n",
    "$\\odot$\n",
    "A  measure of spread looks at how far each observation is from the mean.$\\square$\n",
    "\n",
    "$\\dagger$  \n",
    "A deviation is just one observation minus the mean, no squre, nor absolute.\n",
    "\n",
    "$$\\boxed{ \\mathrm{Variance} = S_{x}^{2} = \\frac {1} {\\boxed{\\mathbf{n-1}}} \\sum_{i=1}^{n} \\left( x_i - \\bar{x} \\right) ^{2} }$$\n",
    "\n",
    "And *Standard deviation* is just the square root of the *Variace*.\n",
    "  \n",
    "$\\odot$\n",
    "Sum of *Deviation* equals to $0$.$\\square$\n",
    "\n",
    "$Compare$  \n",
    "When shall we use \n",
    "1. Mean and standard deviation\n",
    "2. Median and interquartile range  \n",
    "\n",
    "$Answer$  \n",
    "1. The median and IQR are usually better than the mean and standard deviation for describing a **skewed distribution** or **a distribution with outliers**.\n",
    "2. Use mean and standard deviation ONLY for reasonably symmetric distributions that ***DO NOT*** have outliers.\n",
    "\n",
    "### Changing the Unit of Measurement\n",
    "Like we can do a linear transformation: $X_{\\textrm{new}} = a + b \\cdot X$.\n",
    "1. Multiplying each observation by a positive number $b$ multiplies both measures of center (mean, median) and spread ($IQR$, $s$) by $b$.\n",
    "2. Adding the same number $a$ (positive or negative) to each observation adds $a$ to measures of center and to quartiles, but it does not change measures of spread ($IQR$, $s$).\n",
    "\n",
    "$\\dagger$  \n",
    "And variance will be multiplied by $b^2$.\n",
    "\n",
    "## Density Curves and Normal Distributions\n",
    "### Density curve\n",
    "1. Always on or above the horizontal axis\n",
    "2. With an area of exactly 1 underneath it\n",
    "\n",
    "A density curve describes the overall pattern of a distribution. The area under the curve and above any range of values on the horizontal axis is the proportion of all observations that fall in that range.\n",
    "\n",
    "And for thir mean and median:\n",
    "1. The median of a density curve is the *equal-areas point*―the point that divides the area under the curve in half.\n",
    "2. The mean of a density curve is the *balance point*, that is, the point at which the curve would balance if made of solid material.\n",
    "3. The median and the mean are the same for a symmetric density curve. They both lie at the center of the curve. The mean of a skewed curve is pulled away from the median in the direction of the long tail.\n",
    "\n",
    ">![](./Raw/Mean and Median_Density Curve.jpg)\n",
    "\n",
    "And now for the mean and standard deviation of the actual distribution represented by the density curve are denoted by $\\mu$ and $\\sigma$.\n",
    "\n",
    "### Normal Distribution\n",
    "Giving the mean $\\mu$ and the standard deviation $\\sigma$, we can have a normal curve that is symmetric, single-peaked, and bell-shaped.\n",
    "\n",
    "1. The mean of a Normal distribution is the center of the symmetric Normal curve. \n",
    "2. The standard deviation is the distance from the center to the change-of-curvature points on either side.\n",
    "3. We abbreviate the Normal distribution with mean $\\mu$ and standard deviation $\\sigma$ as $N(\\mu,\\sigma)$.\n",
    "\n",
    "$Conclusion$  \n",
    "1. Approximately 68% of the observations fall within $\\sigma$ of $\\mu$.\n",
    "2. Approximately 95% of the observations fall within $2\\sigma$ of $\\mu$.\n",
    "3. Approximately 99.7% of the observations fall within $3\\sigma$ of $\\mu$.\n",
    "\n",
    "### Standardizing Observations\n",
    "$Conclusion$  \n",
    "If a variable $X$ has a distribution with mean $\\mu$ and standard deviation $\\sigma$, then the standardized value of $X$, or its **z-score**, is $Z = (X-\\mu)/\\sigma$, which follows the standard Normal distribution, $N(0,1)$.\n",
    "\n",
    "### Normal Quantile Plots\n",
    "One way to assess if a distribution is indeed approximately Normal.\n",
    "\n",
    "The z-scores of the original data are used for the x-axis against which the data that are plotted on the y-axis of the Normal quantile plot. \n",
    "\n",
    "1. If the distribution is indeed Normal, the plot will show a straight line, indicating a good match between the data and a Normal distribution. \n",
    "2. Systematic deviations from a straight line indicate a non-Normal distribution. Outliers appear as points that are far away from the overall pattern of the plot.\n",
    "\n",
    ">**e.g.**  \n",
    ">\n",
    ">| Good fit to a straight line | Curved pattern |\n",
    ">|:---------------------------:|:--------------:|\n",
    ">| ![](./Raw/Goodfit_NQPlot.jpg) | ![](./Raw/Curved_NQPlot.jpg)  |\n",
    ">| The distribution is close to Normal.|The data are **right** skewed.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at Data - Relationship\n",
    "## Relationships\n",
    "$Def$  \n",
    "Two variables measured on the same cases are **associated** if knowing the value of one of the variables tells you something that you would not otherwise know about the value of the other variable.\n",
    "\n",
    "1. Response variable: measures an outcome of a study. \n",
    "2. Explanatory variable: explains or causes changes in the response variable. \n",
    "\n",
    "## Scatterplots散点图\n",
    "The most useful graph for displaying the relationship between two quantitative variables.\n",
    "\n",
    "It shows the relationship between two quantitative variables measured on the same individuals. The values of one variable appear on the horizontal axis, and the values of the other variable appear on the vertical axis. **Each individual corresponds to one point on the graph**.\n",
    "  \n",
    "$\\odot$\n",
    "1. Plot the explanatory variable on the X axis, and the response variable on the Y axis.\n",
    "2. Label and Scale the axes.\n",
    "3. Plot each point individually.$\\square$\n",
    "\n",
    "### Interpreting\n",
    "Focus on the *direction*, *form*, and *strength* of the relationship, and point out the *outliers* if exist.\n",
    "\n",
    "|Positive relationship| Negative relationship|\n",
    "|:------------------:|:---------------------:|\n",
    "|above-average values of one tend to accompany above-average values<br/> of the other, and below-average values also tend to occur together|above-average values of one tend to accompany below-average values of the other|\n",
    "|![](./Raw/Scatterplot_PLR.jpg)|![](./Raw/Scatterplot_NLR.jpg)|\n",
    "|Slope going up|Slope going down|\n",
    "\n",
    ">**e.g.**  \n",
    ">![](./Raw/Scatterplot_eg.png)\n",
    ">1. There is a moderately **strong** (*relationship strength*), **positive** (*relationship direction*), **linear** (*relationship form*) relationship between body weight and backpack weight.\n",
    ">2. It appears that lighter hikers are carrying lighter backpacks.\n",
    "\n",
    "### Adding categorical variables\n",
    "Using a different dot to represent another category. And make a legend of them.\n",
    "\n",
    "### Nonlinear relationship\n",
    "![](./Raw/Scatterplot_nonlinear.jpg)\n",
    "\n",
    "## Correlation\n",
    "$r$, correlation: measures the strength of the **linear relationship** between two **quantitative** variables.\n",
    "\n",
    "$$r = \\frac{1} {n-1} \\sum\\left( \\frac{x_i - \\bar{x}} {s_x} \\cdot \\frac{y_i - \\bar{y}} {s_y} \\right)$$\n",
    "\n",
    "$Property$  \n",
    "1. $r$ is always a number between $–1$ and $1$.\n",
    "2. $r > 0$ indicates a positive association.\n",
    "3. $r < 0$ indicates a negative association.\n",
    "4. Values of $r$ near $0$ indicate a very weak linear relationship.\n",
    "5. The strength of the linear relationship increases as $r$ moves away from $0$ toward $–1$ or $1$.\n",
    "6. The extreme values $r = –1$ and $r = 1$ occur only in the case of a perfect linear relationship.\n",
    "7. Correlation makes no distinction between explanatory and response variables.\n",
    "8. $r$ has **no units** and does not change when we change the units of measurement of $x$, $y$, or both.\n",
    "\n",
    "$\\odot$\n",
    "1. Required both variables be quantitative.\n",
    "2. Can't describe curved relationships between variables.\n",
    "3. Not resistant, and can be strongly affected by a few outlying observations.\n",
    "4. Not a complete summary of two-variable data.$\\square$\n",
    "\n",
    ">**e.g.**  \n",
    ">![](./Raw/Correlation.jpg)\n",
    "\n",
    "## Least-Squares Regression\n",
    "### Regression Lines\n",
    "$Def$  \n",
    "A straight line that describes how a response variable $y$ changes as an explanatory variable $x$ changes.\n",
    "\n",
    "**Regression equation**: $\\hat{y} = b_0 + b_1 x$\n",
    "\n",
    "Here $x$ is the value of explanatory variable, $\\hat{y}$ is the *predicted value* of response variabe for a *given* $x$, $b_1$ is the *slope*, $b_0$ is the *intercept*, value of $\\hat{y}$ when $x=0$.\n",
    "\n",
    "$\\dagger$  \n",
    "\n",
    "Interpolation is OK, be careful when using prediction. **Extrapolation** is the use of a regression line for prediction far outside the range of values of the explanator variable $x$.\n",
    "\n",
    "### Least-Squares Regression Line\n",
    "$Def$  \n",
    "the line that minimizes the sum of the squares of the vertical distances of the data points from the line. And the equation is $\\hat{y} = b_0 + b_1 x$ with *slope* and *intercept*:\n",
    "\n",
    "$$b_1 = r \\frac{s_y} {s_x}, b_0 = \\bar{y} - b_1 \\bar{x}$$\n",
    "\n",
    "Here $s_x$ is the standard deviation.\n",
    "\n",
    "$\\odot$\n",
    "1. A change of one standard deviation in $x$ corresponds to a change of $r \\times $ standard deviations in $y$.\n",
    "2. The LSRL always passes through $(\\bar{x}, \\bar{y})$.\n",
    "3. The distinction between explanatory and response variables is essential. And if we reverse the roles of the two variables, we get a different LSRL.$\\square$\n",
    "\n",
    "### Correlation and Regression\n",
    "The two variables play different roles in regression. And the **square of correlation**, $r^2$, is the fraction of the variation in the values of $y$ that is explained by the least-squared regression of $y$ on $x$, also called the **coefficient of determination**.\n",
    "\n",
    "## Cautions About Correlation and Regression\n",
    "### Residuals\n",
    "$Def$  \n",
    "Observed $y$ minus predicted $y$, $i.e.$, $y - \\hat{y}$.\n",
    "\n",
    "**Residual Plots**  \n",
    "1. Ideally there should be a “random” scatter around zero.\n",
    "2. Residual patterns suggest deviations from a linear relationship.\n",
    "\n",
    ">**e.g.**\n",
    ">\n",
    "|           Regression          |         Residual Plot         |\n",
    "|:-----------------------------:|:-----------------------------:|\n",
    "| ![](./Raw/Residual_PlotA.jpg) | ![](./Raw/Residual_PlotB.jpg) |\n",
    "\n",
    "### Outliers and Influential Points\n",
    "$Def$  \n",
    "The observation that lies outside the overall pattern of the other observations.\n",
    "\n",
    "**Influential**: an observation is influential if removing it would markedly change the result of the calculation.  \n",
    "$\\odot$\n",
    "And if it's an outlier in the $x$ direction, it is often influential. However if it's in the $y$ direction, it is often with large residual.$\\square$\n",
    "\n",
    "### Cautions\n",
    "1. Both Correlation and Regression describe linear relationships.\n",
    "2. Both are affected by outliers.\n",
    "3. Beware of extrapolation, in predicting y when x is outside the range of observed x’s.\n",
    "4. Beware of **lurking variables**: These have an important effect on the relationship among the variables in a study, but are not included in the study.\n",
    "5. Correlation does not imply causation!\n",
    "\n",
    "## Data Analysis for Two-Way Tables\n",
    "A Two-way table describes two categorical variables, organizing counts according to a **row variable** and a **column variable**. Each combination of values for these two variables is called a **cell**.\n",
    "\n",
    ">![](./Raw/two-wayTable.png)\n",
    "\n",
    "What we do to the two-way table?\n",
    "1. Transform them in to **contingency table**, that is presented each figure with their corresponding frequency.\n",
    "2. The inside entries are the **JOINT probability distribution**.\n",
    "3. The entries of the *last row* and the *last column* are called the **MARGINAL probability distribution**: the distribution of values of that variable among all individuals *within same category* described by the table.\n",
    "\n",
    ">![](./Raw/contingencyTable.png)\n",
    "\n",
    "### Maginal distribution and conditional distribution\n",
    "|           Maginal          |         Conditional       |\n",
    "|:-----------------------------:|:-----------------------------:|\n",
    "| ![](./Raw/marginalDistribution.png) | ![](./Raw/conditionalDistribution.png) |\n",
    "|Given the joint distribution, consider the distribution inside each category|Given the joint distribution, consider the distribution under certain condition|\n",
    "\n",
    "### Simpson’s Paradox\n",
    "$Def$  \n",
    "An association or comparison that holds for all of several groups can reverse direction when the data are combined to form a single group. This reversal is called Simpson’s paradox.\n",
    "\n",
    "The lurking variable creates subgroups, and failure to take these subgroups into consideration can lead to misleading conclusions regarding the association between the two variables.\n",
    "\n",
    ">**e.g.**  \n",
    ">![](./Raw/simpsonParadoxA.png)\n",
    "And after we considering the lurking variable,\n",
    ">![](./Raw/simpsonParadoxB.png)\n",
    "> Our finding is that before the variable \"School\", men are accepted more than women in percentage, however the conclusion is inversed if \"School\" is considered.\n",
    "\n",
    "## The Question of Causation因果关系\n",
    "$\\odot$\n",
    "Association, however strong, does NOT imply causation.$\\square$\n",
    "\n",
    "In the following subsection, dashed lines show an association, solid arrows show a cause-and-effect link. $x$ is explanatory, $y$ is response, and $z$ is a lurking variable.\n",
    "\n",
    "|           Common Response          |         Confounding       | Causation |\n",
    "|:-----------------------------:|:-----------------------------:|:-----------:|\n",
    "| ![](./Raw/commonResponse.jpg) | ![](./Raw/confounding.jpg) | ![](./Raw/causation.jpg) |\n",
    "|The observed relationship between the variables can be explained by a lurking variable. Both $x$ and $y$ may change in response to changes in $z$.|Two variables' effects on a response variable cannot be distinguished from each other. The confounded variables may be either explanatory variables or lurking variables.|A properly conducted experiment may establish causation.|\n",
    "|Most students who have high SAT scores ($x$) in high school have high GPAs ($y$) in their first year of college. And “ability and knowledge” is the lurking variable.|Religious people live longer than nonreligious people, but they also take better care of themselves and are less likely to smoke or be overweight.|A properly conducted experiment may establish causation.|\n",
    "\n",
    "Criteria for Causation\n",
    "1. strong association\n",
    "2. consistent association\n",
    "  1. The connection happens in repeated trials. \n",
    "  2. The connection happens under varying conditions.\n",
    "3. higher doses are associated with stronger responses.\n",
    "4. alleged cause do precede the effect.\n",
    "5. The alleged cause is plausible.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Producing Data\n",
    "## Sources of Data\n",
    "1. *Anecdotal data* represent individual cases that often come to our attention because they are striking in some way.\n",
    "2. *Available data* are data that were produced in the past for some other purpose but that may help answer a present question inexpensively.\n",
    "3. a *sample of individuals* is selected from a larger *population of individuals*.  \n",
    "\n",
    "$Compare$\n",
    "\n",
    "An observational study observes individuals and measures variables of interest but does not attempt to influence the \n",
    "responses. The purpose is to describe some group or situation.\n",
    "\n",
    "An experiment deliberately imposes some treatment on individuals to measure their responses. The purpose is to study \n",
    "whether the treatment causes a change in the response.\n",
    "\n",
    "Experiments don’t just observe individuals or ask them questions. They actively impose some treatment in order to measure the response.\n",
    "***\n",
    "Well-designed experiments take steps to avoid **confounding**.\n",
    "\n",
    "Confounding occurs when two variables are associated in such a way that their effects on a response variable cannot be distinguished from each other.\n",
    "\n",
    "A *lurking variabl*e is a variable that is not among the explanatory or response variables in a study but that may influence the response variable.\n",
    "## Design of Experiments\n",
    "## Sampling Design\n",
    "**population**: entire group of individuals about which we want information.\n",
    "\n",
    "**sample**:  part of the population from which we actually collect information.\n",
    "\n",
    "We use information from a *sample* to draw conclusions about the entire *population*. We collect data from a \n",
    "**representative Sample** so that we can make an inference about thw whold population.\n",
    "\n",
    "The design of a sample is **biased** if it *systematically* favors certain outcomes.\n",
    "\n",
    "**convenience sample**: Choosing individuals simply because they are easy to reach.\n",
    "\n",
    "**voluntary response sample**: people who choose themselves by responding to a general appeal. $\\odot$ This often show great bias because people with stronger opinions are more likely to respond.\n",
    "\n",
    "**Random sampling**: the use of chance to select a sample, is the *central principle* of statistical sampling.\n",
    "\n",
    "**simple random sample** (SRS) of size $n$: $n$ individuals from the population chosen in such a way that every set of $n$ individuals has an equal chance to be the sample actually selected.\n",
    "\n",
    "We can get random numbers generated by a computer or calculator to choose samples. Or use *table of random digits*.\n",
    "\n",
    "**possible sample**: a sample chosen by chance.\n",
    "\n",
    "To select a **stratified random sample**, first *classify* the population into groups of similar individuals, called **strata**. Then choose a *separate SRS in each stratum* and combine these SRSs to form the full sample.\n",
    "\n",
    "Some source of error:\n",
    "1. *Undercoverage*: when some groups in the population are left out of the process of choosing the sample.\n",
    "2. *Nonresponse*: when an individual chosen for the sample can’t be contacted or refuses to participate.\n",
    "3. *Response bias*: a systematic pattern of incorrect responses in a sample survey\n",
    "4. *wording of questions*: **most important influence** on the answers\n",
    "\n",
    "## Toward Statistical Inference\n",
    "A **parameter** is a number that describes some characteristic of the population. In statistical practice, the value of a parameter is not known because we cannot examine the entire population.\n",
    "\n",
    "A **statistic** is a number that describes some characteristic of a sample. The value of a statistic can be computed directly from the sample data. We often use a statistic to estimate an unknown parameter.\n",
    "\n",
    "**s***tatistics* come from **s***amples* and **p***arameters* come from **p***opulations*.\n",
    "\n",
    "Knowing the **sampling distribution** would be better to perform statistical inference.\n",
    "\n",
    "The **population distribution** of a variable is the distribution of values of the variable among all individuals in the population.\n",
    "\n",
    "The **sampling distribution** of a statistic is the distribution of values taken by the statistic in all possible samples of the same size from the same population.\n",
    "\n",
    "**Bias** concerns the center of the sampling distribution. A statistic used to estimate a parameter is **unbiased** if the mean of its sampling distribution is equal to the true value of the parameter being estimated.\n",
    "\n",
    "The **variability of a statistic** is described by the spread of its sampling distribution. This spread is determined by the sampling design and the sample size n. Statistics from larger probability samples have smaller spreads.\n",
    "\n",
    "![](./Raw/bias_variaility.png)\n",
    "\n",
    "To reduce bias, use random sampling. And To reduce variability of a statistic from an SRS, use a larger sample.\n",
    "\n",
    "**inference**: Draw conclusions about a population on the basis of sample data\n",
    "\n",
    "Why *Random Sampling*?\n",
    "1. To *eliminate bias* in selecting samples from the list of available individuals.\n",
    "2. The laws of probability allow trustworthy inference about the population.\n",
    "  - Results from random samples come with a margin of error that sets bounds on the size of the likely error.\n",
    "  - Larger random samples give better information about the population than smaller samples.\n",
    "\n",
    "## Ethics\n",
    "When collecting data from people.\n",
    "\n",
    "**Basic Data Ethics**\n",
    "1. The organization that carries out the study must have an **institutional review board** that reviews all planned studies in advance in order to protect the subjects from possible harm.\n",
    "  - Reviews the plan of study\n",
    "  - Can require changes\n",
    "  - Reviews the consent form\n",
    "  - Monitors progress at least once a year\n",
    "2. All individuals who are subjects in a study must give their **informed consent** before data are collected, the nature of this research and any risk of harm it might bring.\n",
    "3. All individual data must be kept **confidential**. Only statistical summaries for groups of subjects may be made public.\n",
    "\n",
    "Who can't give informed consent?\n",
    "- Prison inmates\n",
    "- Very young children\n",
    "- People with mental disorders\n",
    "\n",
    "### Confidentiality\n",
    "1. All individual data must be kept confidential. **Only** *statistical summaries* may be made public. \n",
    "2. Not the same with **anoymity**\n",
    "3. Separate the identity of the subjects from the rest of the data immediately!\n",
    "\n",
    "### Clinical Trials\n",
    "- Randomized comparative experiments are the only way to see the true effects of new treatments.\n",
    "- Most benefits of clinical trials go to future patients. We must balance future benefits against present risks.\n",
    "- The interests of the subject must always prevail over the interests of science and society.\n",
    "\n",
    "### Behavioral and Social Science Experiments\n",
    "1. These experiments rely on hiding the true purpose of the study.\n",
    "2. Subjects would change their behavior if told in advance what investigators were looking for.\n",
    "3. “Ethical Principles”: consent, unless a study merely observes behavior in a public space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Probability: The Study of Randomness\n",
    "## Randomness\n",
    "We call a *phenomenon* **random** if individual outcomes are uncertain but there is nonetheless a regular distribution of outcomes in a large number of repetitions.\n",
    "\n",
    "The **probability** of any outcome of a chance process is the proportion of times the outcome would occur in a very long series of repetitions.\n",
    "\n",
    "**independent trail**: the outcome of a new trail is not influenced by the result of the previous trail\n",
    "\n",
    "## Probability Models\n",
    "The **sample space** $S$ of a chance process is the set of **all possible outcomes**.\n",
    "\n",
    "An **event** is an outcome or a set of outcomes of a random phenomenon. That is, an event is a **subset** of the *sample space*.\n",
    "\n",
    "**Probability** is a function that assigns a *measure (number)* between $\\left[0, 1\\right]$ to an *event*: $P(\\text{event}) = p$, where  $0 \\leq p \\leq 1$.\n",
    "\n",
    "$Rule$\n",
    "1. Any probability is a number between $0$ and $1$.\n",
    "2. All possible outcomes together must have probability $1$.\n",
    "3. If two events have no outcomes in common, the probability that one or the other occurs is the *sum* of their individual probabilities.\n",
    "4. The probability that an event does not occur is $1$ minus the probability that the event does occur.\n",
    "\n",
    "or in math languages\n",
    "1. for event $A$, $0 \\leq P(A) \\leq 1$\n",
    "2. for sample space $S$, $P(S) = 1$\n",
    "3. for disjoint event $A$, and $B$, $P(A \\text{ OR } B) = P(A) + P(B)$\n",
    "4. the complement of any event $A$, is the event that $A$ doesn't occur, denoted as $A^c$. $P(A^c) = 1 - P(A)$\n",
    "\n",
    "Other rules\n",
    "\n",
    "Two events $A$ and $B$ are **independent** if knowing that one occurs does not change the probability that the other occurs. We have $P(A \\text{ AND } B) = P(A) \\times P(B)$.\n",
    "\n",
    "## Random Variables\n",
    "A **random variable** is a **function** that assigns numerical values (state space) to each of the outcomes in the *sample space*.\n",
    "\n",
    "The probability distribution of a random variable gives its possible values and their probabilities.\n",
    "\n",
    "And the complete information (solution) of a random variable is contained in its DISTRIBUTION FUNCTION.\n",
    "\n",
    "### Discrete Random Variable\n",
    "For discrete random variable $X$, it takes a fixed set of possible values with gaps between. \n",
    "\n",
    "| Value       | $x_1$ | $x_2$ | $x_3$ | $\\dots$ |\n",
    "|:-------------|:-----:|:-----:|:-----:|:-------:|\n",
    "| Probability | $p_1$ | $p_2$ | $p_3$ | $\\dots$ |\n",
    "\n",
    "1. Every probability $p_i$ is a number between $0$ and $1$.\n",
    "2. The sum of the probabilities is $1$.\n",
    "\n",
    ">**e.g.** Tossing a Die, $X$ is the $r.v.$ of number. \n",
    ">\n",
    ">*state space*: $\\{1, 2, 3, 4, 5, 6\\}$, *distribution func of* $X$: $P(X = i) = \\displaystyle \\frac{1} {i}, i = 1 , 2, \\dots, 6$\n",
    "\n",
    "### Continuous Random Variable\n",
    "For continuous Random Variable $Y$, it takes on all values in an interval of numbers. \n",
    "\n",
    "It has *infinitely many possible values* and *only* **intervals of values** have positive probability.\n",
    "\n",
    "A **continuous probability model** assigns probabilities as areas *under a **density curve***. The area under the curve and above any range of values is the probability of an outcome in that range.\n",
    "\n",
    "### Normal Probability Models\n",
    "After standardize, $\\displaystyle z = \\frac{x - \\mu} {\\sigma}$, we have $Z \\sim \\mathrm{N}(\\mu = 0, \\sigma = 1)$.\n",
    "\n",
    "## Means and Variances of Random Variables\n",
    "For discrete $r.v.$, the mean is $\\mu_x = \\sum x_i P_i$, a weighted average.\n",
    "\n",
    "$\\odot$ The expected value does not need to be a possible value of $X$.$\\square$\n",
    "\n",
    "and the variance is $\\sigma_x^2 = Var(X) = \\sum (x_i - \\mu_X)^2p_i$\n",
    "\n",
    "### The Law of Large Numbers\n",
    "As the number of observations drawn (sample size, $n$) increases, the sample mean $\\bar{x}$ of the observed values gets closer and closer to the mean $\\mu$ of the population.\n",
    "\n",
    "### Some rules\n",
    "For $r.v.$ $X$ and $Y$\n",
    "1. $\\mu_{a + bX} = a + b\\cdot \\mu_X$\n",
    "2. $\\mu_{X+Y} = \\mu_X + \\mu_Y$\n",
    "3. $\\sigma_{a + bX}^2 = b^2 \\cdot \\sigma_x^2$\n",
    "4. if independent, $\\sigma_{X + Y}^2 = \\sigma_x^2 + \\sigma_Y^2$\n",
    "5. if with correlation $\\rho$, $\\sigma_{X + Y}^2 = \\sigma_x^2 + \\sigma_Y^2 + 2 \\rho \\sigma_X \\sigma_Y, \\sigma_{X - Y}^2 = \\sigma_x^2 + \\sigma_Y^2 - 2 \\rho \\sigma_X \\sigma_Y$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
