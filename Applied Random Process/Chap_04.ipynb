{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Chains\n",
    "## Intro\n",
    "$$\\DeclareMathOperator*{\\argmin}{argmin}\n",
    "\\DeclareMathOperator*{\\argmax}{argmax}\n",
    "\\newcommand{\\using}[1]{\\stackrel{\\mathrm{#1}}{=}}\n",
    "\\newcommand{\\ffrac}{\\displaystyle \\frac}\n",
    "\\newcommand{\\space}{\\text{ }}\n",
    "\\newcommand{\\bspace}{\\;\\;\\;\\;}\n",
    "\\newcommand{\\QQQ}{\\boxed{?\\:}}\n",
    "\\newcommand{\\void}{\\left.\\right.}\n",
    "\\newcommand{\\CB}[1]{\\left\\{ #1 \\right\\}}\n",
    "\\newcommand{\\SB}[1]{\\left[ #1 \\right]}\n",
    "\\newcommand{\\P}[1]{\\left( #1 \\right)}\n",
    "\\newcommand{\\dd}{\\mathrm{d}}\n",
    "\\newcommand{\\Tran}[1]{{#1}^{\\mathrm{T}}}\n",
    "\\newcommand{\\d}[1]{\\displaystyle{#1}}\n",
    "\\newcommand{\\EE}[2][\\,\\!]{\\mathbb{E}_{#1}\\left[#2\\right]}\n",
    "\\newcommand{\\Var}[2][\\,\\!]{\\mathrm{Var}_{#1}\\left[#2\\right]}\n",
    "\\newcommand{\\Cov}[2][\\,\\!]{\\mathrm{Cov}_{#1}\\left(#2\\right)}\n",
    "\\newcommand{\\Corr}[2][\\,\\!]{\\mathrm{Corr}_{#1}\\left(#2\\right)}\n",
    "\\newcommand{\\I}[1]{\\mathrm{I}\\left( #1 \\right)}\n",
    "\\newcommand{\\N}[1]{\\mathrm{N} \\left( #1 \\right)}\n",
    "\\newcommand{\\ow}{\\text{otherwise}}$$Sometimes to assume that the successive values $X_i$ are all independent is just unjustified. Thus we define the ***stochastic process*** $\\CB{X_n, n=0,1,2,\\dots}$. If $X_n = i$, we say that the process is in state $i$ at time $n$.\n",
    "\n",
    "We suppose that whenever the process is in state $i$, there is a fixed probability $P_{i j}$ that it will next be in state $j$. That is, we suppose that: \n",
    "\n",
    "$$P\\CB{X_{n+1} = j \\mid X_n = i, X_{n-1} = i_{n-1},\\dots, X_0 = i_0} = P_{ij}$$\n",
    "\n",
    "for all states $i_0,i_1,\\dots,i_{n-1},i,j$ and all $n \\geq 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Markov Property马氏性: $P\\CB{X_{n+1} = j\\mid X_n =i,X_{n-1} = i_{n-1},\\cdots, X_0 = i_0} = P\\CB{X_{n+1}= j \\mid X_n = i}$ for all states $i_0,i_1,\\dots,i_{n-1},i,j$ and all $n \\geq 0$.\n",
    "\n",
    "$Remark$\n",
    "\n",
    ">$$P\\CB{X_{n+1} = j\\mid X_n =i,X_{n-1} = i_{n-1},\\cdots, X_0 = i_0} = P\\CB{X_{n+1}= j \\mid X_n = i}\\\\\n",
    "\\iff P\\CB{X_{n+1} = j ,X_{n-1} = i_{n-1},\\cdots, X_0 = i_0\\mid X_n =i} \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\\\\n",
    "\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;= P\\CB{X_{n+1}= j \\mid X_n = i} \\cdot P\\CB{X_{n-1} = i_{n-1},\\cdots, X_0 = i_0\\mid X_n =i}$$\n",
    ">\n",
    ">对未来判断只依赖今天$X_n$，与历史无关\n",
    "\n",
    "- Time-homogenous时齐性: $P\\CB{X_{n+1}=j\\mid X_n = i} = P\\CB{X_1 = j \\mid X_0 = i}$ for all $n$, meaning that the transition is independent of $n$.\n",
    "\n",
    "***\n",
    "\n",
    "Then we focus on the one-step transition matrix. Let $P_{ij}$ be the probability that the process will, when in state $i$, next make a transition to state $j$. Then, $P_{ij}\\geq 0$ for $i,j \\geq 0$ and \n",
    "\n",
    "$$\\sum_{j=0}^{\\infty} P_{ij} = 1, i = 0,1,2,\\dots\\\\\n",
    "\\mathbf{P} = \\begin{Vmatrix}\n",
    "P_{00} & P_{01} & P_{02} & \\cdots \\\\\n",
    "P_{10} & P_{11} & P_{12} & \\cdots \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\\\\n",
    "P_{i0} & P_{i1} & P_{i2} & \\cdots \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\\\\n",
    "\\end{Vmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.4**\n",
    "\n",
    "Suppose that whether or not it rains today depends on previous weather conditions through the last two days. Specifically, suppose that if it has rained for the past two days, then it will rain tomorrow with probability $0.7$; if it rained today but not yesterday, then it will rain tomorrow with probability $0.5$; if it rained yesterday but not today, then it will rain tomorrow with\n",
    "probability $0.4$; if it has not rained in the past two days, then it will rain tomorrow with probablity $0.2$.\n",
    "\n",
    ">Since it's about the last TWO days, we can define $4$ states:\n",
    "\n",
    ">1. if it rained both today and yesterday\n",
    ">2. if it rained today but not yesterday\n",
    ">3. if it rained yesterday but not today\n",
    ">4. if it did not rain either yesterday or today\n",
    ">\n",
    ">$\\bspace\\begin{Vmatrix}\n",
    "0.7 & 0 & 0.3 & 0 \\\\\n",
    "0.5 & 0 & 0.5 & 0 \\\\\n",
    "0 & 0.4 & 0 & 0.6 \\\\\n",
    "0 & 0.2 & 0 & 0.8\n",
    "\\end{Vmatrix}$\n",
    ">\n",
    ">The $0$s represent those impossible transition.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.** Random Walk Model\n",
    "\n",
    "A Markov chain whose state space is given by the integers $i = 0,\\pm 1, \\pm 2, \\dots$ is said to be a ***random walk*** if, for some number $0 < p < 1$, we have\n",
    "\n",
    "$$P_{i,i+1} = p = 1 - P_{i,i-1}$$\n",
    "\n",
    "We can think this as a model for an individual walking on a straight line who at each point of time either takes one step to the right with probability $p$ or one step to the left with probability $1-p$\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.** Gambling Model, more about the Random walk.\n",
    "\n",
    "Start from random walk model. The man now stand on a cliff where only points from $0$ to $N$ are available otherwise he will fall. Suppose he starts from point $0$. Then the model will be \n",
    "\n",
    "$\\bspace P_{i,i+1} = p = 1 - P_{i,i-1},i = 1,2,\\dots,N-1$ and $P_{00} = P_{NN} = 1$\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapman-Kolmogorov Equations\n",
    "\n",
    "Now we define the $n$-step transition probabilities $P_{ij}^{\\:\\!n} = P\\CB{X_{n+k} = j \\mid X_{k} = i}, n\\geq 0, i,j \\geq0$. And to compute the probabilities, the ***Chapman-Kolmogorov Equations*** says that\n",
    "\n",
    "$$\\begin{align}\n",
    "P_{ij}^{\\:\\!n+m} &= P\\CB{X_{n+m} = j \\mid X_0 = i}\\\\\n",
    "&=\\sum_{k=0}^{\\infty} P\\CB{X_{n+m} = j, X_n = k \\mid X_0 = i} \\\\\n",
    "&= \\sum_{k=0}^{\\infty} P\\CB{X_{n+m} = j \\mid X_n = k , X_0 = i} P\\CB{X_n = k \\mid X_0 = i} \\\\\n",
    "&= \\sum_{k=0}^{\\infty} P_{ik}^{\\:\\!n}P_{kj}^{\\:\\!m}\\end{align}$$\n",
    "\n",
    "for all $n,m\\geq 0$ and all $i,j$.\n",
    "\n",
    "If we let $\\mathbf{P}^{\\P{n}}$ denote the matrix of $n$-step transition probabilities $P_{ij}^{\\:\\!n}$, then the last equation is to say $\\mathbf{P}^{\\P{n+m}} = \\mathbf{P}^{\\P{n}} \\cdot \\mathbf{P}^{\\P{m}}$. And thus by induction we have $\\mathbf{P}^{\\P{n}} = \\mathbf{P}^n$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Remark$\n",
    "\n",
    ">时齐性 combined with c-k equation, we have\n",
    ">\n",
    ">$$P\\CB{X_{n+k} = j \\mid X_n = i} = P\\CB{X_k = j \\mid X_0 = i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.9** e.g.4 revisited\n",
    "\n",
    "Given it rained on Monday and Tuesday, what's the probability that it will rain on Thursday?\n",
    "\n",
    "> $\\bspace\\begin{align}\n",
    "\\mathbf{P}^{\\P{2} } &= \\mathbf{P}^2 \\\\\n",
    "&= \\bspace\\begin{Vmatrix}\n",
    "0.7 & 0 & 0.3 & 0 \\\\\n",
    "0.5 & 0 & 0.5 & 0 \\\\\n",
    "0 & 0.4 & 0 & 0.6 \\\\\n",
    "0 & 0.2 & 0 & 0.8\n",
    "\\end{Vmatrix}\\cdot \\begin{Vmatrix}\n",
    "0.7 & 0 & 0.3 & 0 \\\\\n",
    "0.5 & 0 & 0.5 & 0 \\\\\n",
    "0 & 0.4 & 0 & 0.6 \\\\\n",
    "0 & 0.2 & 0 & 0.8\n",
    "\\end{Vmatrix} \\\\[0.7em]\n",
    "&= \\bspace\\begin{Vmatrix}\n",
    "0.49 & 0.12 & 0.21 & 0.18 \\\\\n",
    "0.35 & 0.20 & 0.15 & 0.30 \\\\\n",
    "0.20 & 0.12 & 0.20 & 0.48 \\\\\n",
    "0.10 & 0.16 & 0.10 & 0.64\n",
    "\\end{Vmatrix}\n",
    "\\end{align}$\n",
    ">\n",
    ">And to rain on Thursday is equivalent to the process being in either state $1$ or $2$ (here the state number are from $1$ to $4$). Thus the answer is $P_{11}^{2} + p_{12}^2 = 0.49 + 0.12 = 0.61$.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.10** \n",
    "\n",
    "An urn always contains $2$ balls, red or blue. At each stage a ball is randomly chosen and then replaced by a new ball, which with probability $0.8$ is the same color, and with probability $0.2$ is the opposite color, as the ball it replaces. If initially both balls are red, find the probability that the fifth ball selected is red.\n",
    "\n",
    ">We use $X_n$, the number of red balls in the urn after $n$-th selection, to construct the Markov Chian: $X_n, n \\geq 0$ with states $0,1,2$ and the transition matrix $\\mathbf{P}=\\begin{Vmatrix}\n",
    "0.8 & 0.2 & 0 \\\\\n",
    "0.1 & 0.8 &0.1\\\\\n",
    "0 & 0.2 & 0.8\n",
    "\\end{Vmatrix}$. Then we can calculate the desired probability:\n",
    ">\n",
    ">$$\\begin{align}\n",
    "P\\CB{\\text{fifth selection is red}} &= P\\CB{\\text{fifth selection is red} \\mid X_0 = 2}\\\\\n",
    "&= \\sum_{i=0}^{2} P\\CB{\\text{fifth selection is red} \\mid X_4 = i} \\cdot P\\CB{X_4 = i \\mid X_0 = 2}\\\\\n",
    "&= 0 \\times P_{2,0}^{4} + 0.5\\times P_{2,1}^{4} + 1 \\times P_{2,2}^{4}\\\\\n",
    "&= \\cdots = 0.7048\n",
    "\\end{align}$$\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.11** \n",
    "\n",
    "Suppose that balls are successively distributed among $8$ urns, with each ball being equally likely to be put in any of these urns. What is the probability that there will be exactly $3$ nonempty urns after $9$ balls have been distributed?\n",
    "\n",
    ">Let $X_n$ be the $r.v.$ that is mentioned in the context, the nonempty urns after $n$-th distribution. Then we have the transition probabilities: $P_{i,i} = i/8 = 1-P_{i,i+1}$ for $i=0,1,\\dots,8$. Everything is easy until we're going to find $\\mathbf{P}^8$. It's complicated, so here's how to simplify the calculation.\n",
    ">\n",
    ">The probability we need is $P_{03}^{9}$, or even simpler, $P_{13}^{\\:\\!8}$ because it's destined to go from $0$ urns to $1$ urn in the first distribution. After that, the first and the last $4$ columns and rows in $\\mathbf{P}$ are not gonna help with finding the answer, so we combine them together, and obtain\n",
    ">\n",
    ">$$\\begin{array}{rrc}\n",
    "&& X_{n+1} \\\\\n",
    "&& \\begin{array}{cccc}\n",
    "\\:1\\: & \\:\\:2 & \\:\\:\\,3\\!\\! & \\:\\,\\geq4\n",
    "\\end{array}\\\\\n",
    "X_n & \\begin{array}{c}\n",
    "1 \\\\\n",
    "2 \\\\\n",
    "3 \\\\\n",
    "\\geq 4\n",
    "\\end{array} & \\begin{Vmatrix}\n",
    "1/8 & 7/8 & 0 & 0 \\\\\n",
    "0 & 2/8 & 6/8 & 0 \\\\\n",
    "0 & 0 & 3/8 & 5/8 \\\\\n",
    "0 & 0 & 0 & 1\n",
    "\\end{Vmatrix}\n",
    "\\end{array} \\Rightarrow \\mathbf{P}^4 = \\begin{Vmatrix}\n",
    "0.00012 & 0.0256 & 0.2563 & 0.7178 \\\\\n",
    "0 & 0.0039 & 0.0952 & 0.9009 \\\\\n",
    "0 & 0 & 0.0198 & 0.9802 \\\\\n",
    "0 & 0 & 0 & 1\n",
    "\\end{Vmatrix}$$\n",
    ">\n",
    ">hence $P_{13}^{\\:\\!8} = 0.0002\\times0.2563+0.0256\\times0.0952+0.02563\\times0.0198+0.7178\\times0 = 0.00756$\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\mathscr{A}$ be a set of states.To determine $\\beta=P\\CB{X_k \\in \\mathscr{A} \\text{ for some }k=1,\\dots,m \\mid X_0 = i}$ for $i\\notin\\mathscr{A}$ we need a new Markov Chain $\\CB{W_n,n\\geq 0}$ whose states are the states that are not in $\\mathscr{A}$ plus an additional state, namely, $A$. This one is different from what we've learned before is evidenced by the fact that:\n",
    "\n",
    "$\\bspace$Once the $\\CB{W_n}$ enters state $A$, it remains there *forever*.\n",
    "\n",
    "Here's a more formal definition: Let $X_n$ denote the state at time $n$ of the Markov Chain with transition probabilities $P_{i,j}$ (not $P_{ij}$ any more), define $N = \\min\\CB{n:X_n \\in \\mathscr{A}}$, or $N = \\infty$ if $X_n \\notin \\mathscr{A}$ for all $n$. In words, $N$ is the ***first time***, called the ***hitting time***, the Markov chain enters the set of states $\\mathscr{A}$. Now, define:\n",
    "\n",
    "$\\bspace W_n = \\begin{cases}\n",
    "X_n, &\\text{if }n < N \\\\\n",
    "A,   &\\text{if }n \\leq N\n",
    "\\end{cases}$\n",
    "\n",
    "Its transition probabilities are: $\\begin{cases}\n",
    "Q_{i,j} = P_{i,j}, &\\text{if } i \\notin \\mathscr{A}, j \\notin \\mathscr{A} \\\\[0.7em]\n",
    "Q_{i,A} = \\d{\\sum_{j \\in \\mathscr{A}} P_{i,j}}, &\\text{if } i \\notin \\mathscr{A} \\\\\n",
    "Q_{A,A} = 1\n",
    "\\end{cases}$\n",
    "\n",
    "Also notice that the original Markov chain will have entered a state in $\\mathscr{A}$ by time $m$ $iff$ the state at time $m$ of the new Markov chain is $\\mathscr{A}$, we see that\n",
    "\n",
    "$$P\\CB{X_k \\in \\mathscr{A} \\text{ for some } k =1,2,\\dots,m\\mid X_0 =i}\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\ \\\\\n",
    "=P\\CB{W_m = A \\mid X_0 = i} = P\\CB{W_m = A \\mid W_0 = i} = Q_{i,A}^{m}$$\n",
    "\n",
    "Thus the desired probablity is equal to an $m$-step transition probability of the new chain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.** \n",
    "\n",
    "In a sequence of independent flips of a fair coin, let $N$ denote the number of flips until there is a run of $3$ consecutive heads. Find $P\\CB{N \\leq 8}$ and $P\\CB{N = 8}$.\n",
    "\n",
    ">The desired state is $3$ consecutive thus we need states $0,1,2,3$ in Markov China implying how many consecutive heads we've so far reached. Then the transition matrix:\n",
    ">\n",
    ">$$\\mathbf{P} = \\begin{Vmatrix}\n",
    "0.5 & 0.5 & 0 & 0 \\\\\n",
    "0.5 & 0 & 0.5 & 0 \\\\\n",
    "0.5 & 0 & 0 & 0.5 \\\\\n",
    "0 & 0 & 0 & 1\n",
    "\\end{Vmatrix}$$\n",
    ">\n",
    ">Then the desired probablity is $P\\CB{N = 8} = P^{8}_{0,3} = 107/256$\n",
    ">\n",
    ">Then $P\\CB{N \\leq 8} = P\\CB{N \\leq 8} - P\\CB{N \\leq 7} = P^{8}_{0,3} - P^7_{0,3} = \\cdots$\n",
    ">***\n",
    ">Or another way using the preceding discussion\n",
    ">\n",
    ">We now assign FOUR states. $i = 0,1,2$ keep unchanged while $i=3$ means that the consecutive $3$ heads has just occurred and $i=4$ means that it has happened in the past.\n",
    ">\n",
    ">$$\\mathbf{Q} = \\begin{Vmatrix}\n",
    "0.5 & 0.5 & 0 & 0 & 0\\\\\n",
    "0.5 & 0 & 0.5 & 0 & 0\\\\\n",
    "0.5 & 0 & 0 & 0.5 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 1 \\\\\n",
    "0 & 0 & 0 & 0 & 1\n",
    "\\end{Vmatrix}$$\n",
    ">\n",
    ">Here $\\mathscr{A}$ is $\\CB{\\text{consecutive $3$ heads happend}}$. Then $P\\CB{N = 8} = Q_{0,3}^8$\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose now that we want to compute the probability that the $\\CB{X_n, n \\geq 0}$ chain, starting from state $i$ to $j$ at time $m$ without ever entering any of the states in $\\mathscr{A}$, where neither $i$ or $j$ is in $\\mathscr{A}$. Consider $\\alpha = P\\CB{X_m = j, X_k \\notin \\mathscr{A}, k = 1,2,\\dots,m-1\\mid X_0 = i}$ and the $W_m$ we've defined before, we have\n",
    "\n",
    "$$\\alpha = P\\CB{ W_m = j \\mid X_0 = i} = P\\CB{ W_m = j \\mid W_0 = i} = Q_{i,j}^{m}$$\n",
    "\n",
    "Refer **e.g.11** for a better understanding.\n",
    "***\n",
    "Now when $i \\notin \\mathscr{A}$ but $j$ is! Then what's $\\CB{X_m = j,X_k \\notin \\mathscr{A}, k = 0,1,\\dots,m-1 \\mid X_0 = i}$ for $j \\in \\mathscr{A}$. We can easily get that\n",
    "\n",
    "$$\\begin{align}\n",
    "\\alpha &= \\sum_{r \\notin \\mathscr{A}} P_{r,j} \\cdot Q_{i,r}^{m-1} \\\\\n",
    "&= \\sum_{r \\notin \\mathscr{A}}P\\CB{X_m = j, X_{m-1} = r, X_k \\notin \\mathscr{A}, k = 0,1,\\dots,m-2 \\mid X_0 = i}\\\\\n",
    "&= \\sum_{r \\notin \\mathscr{A}} \\big( P\\CB{X_m = j \\mid X_{m-1} = r, X_k \\notin \\mathscr{A}, k = 0,1,\\dots,m-2, X_0 = i}\\\\\n",
    "&\\bspace\\bspace\\bspace \\times P\\CB{X_{m-1} = r, X_k \\notin \\mathscr{A}, k = 0,1,\\dots,m-2\\mid X_0 = i}\\big)\n",
    "\\end{align}$$\n",
    "***\n",
    "And when $i \\in \\mathscr{A}$, we have $\\alpha = \\d{\\sum_{r\\notin\\mathscr{A}} P_{i,r} \\cdot Q_{r,j}^{m-1}}$.\n",
    "***\n",
    "And the last one, for when given that the chain starts in state $i$ and has not entered any state in $\\mathscr{A}$ by time $n$. Then for $i,j \\notin \\mathscr{A}$ we have:\n",
    "\n",
    "$$P\\CB{X_n = j \\mid X_0 = i, X_k \\notin \\mathscr{A}, k = 1,2,\\dots,k-1} = \\ffrac{Q_{i,j}^n} {\\d{\\sum_{r\\notin\\mathscr{A}}} Q_{i,r}^{n}}$$\n",
    "\n",
    "$Remark$\n",
    "\n",
    ">$$P\\CB{X_n = j} = \\sum_{i=0}^{\\infty} P\\CB{X_n = j \\mid X_0 = i} \\cdot P\\CB{X_0 = i} = \\sum_{i=0}^{\\infty} P_{ij}^{n} \\cdot P\\CB{X_0 = i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification of States\n",
    "\n",
    "State $j$ is said to be ***accessible***  from state $i$ if $P_{ij}^{n}>0$ for some $n \\geq 0 $. When two states are **accessible** to each other, they're said to be ***communicate***, written as $i \\leftrightarrow j$.\n",
    "\n",
    "- State $i$ communicates with state $i$ itself for all $i \\geq 0$.\n",
    "- If state $i$ communicates with state $j$, then state $j$ communicates with state $i$.\n",
    "- If state $i$ communicates with state $j$, and state $j$ communicates with state $k$, then state $i$ communicates with state $k$.\n",
    "\n",
    "Two states that communicate are said to be in the same ***class***. An obvious conclusion is that \n",
    "\n",
    "$\\bspace$Any two classes of states are EITHER identical OR disjoint, NO other possible relations.\n",
    "\n",
    "The Markov chain is said to be ***irreducible*** if there is only one class, that is, if all states communicate with each other.\n",
    "***\n",
    "For any state $i$ we let $f_i$ denote the probability that, starting in state $i$, the process will ever *reenter* state $i$. State $i$ is said to be ***recurrent*** if $f_i = 1$ and ***transient*** if $f_i < 1$. 不如就叫复发态和暂住态\n",
    "\n",
    "If **recurrent**, then state $i$ will happen *infinitely often*. However, if **transient**, each time the process enters state $i$ there will be a positive probability $1-f_i$ that it will *never* again enter that state again. Then the number of time periods that the process will be in state $i$ has a geometric distribution with finite mean $\\ffrac{1}{1-f_i}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we claim that state $i$ is *recurrent* $iff$ starting in state $i$, the expected number of time periods that the process is in state $i$ is *infinite*. But letting, $I_n = \\begin{cases}\n",
    "1, & \\text{if }X_n = i\\\\\n",
    "0, & \\text{if }X_n \\neq i\n",
    "\\end{cases}$, we have \n",
    "\n",
    "$$\\begin{align}\n",
    "\\EE{\\d{\\sum_{n=0}^{N}I_n} \\mid X_0 = i} &= \\d{\\sum_{n=0}^{N}\\EE{I_n\\mid X_0 = i}} \\\\\n",
    "&=\\sum_{n=0}^{\\infty} P\\CB{X_n = i \\mid X_0 = i} \\\\\n",
    "&= \\sum_{n=0}^{\\infty} P_{ii}^{n}\n",
    "\\end{align}$$\n",
    "\n",
    "$Proposition.1$\n",
    "\n",
    "$\\bspace$State $i$ is **recurrent** if $\\d{\\sum_{n=1}^{\\infty}}P_{ii}^{n} = \\infty$ and **transient** if $\\d{\\sum_{n=1}^{\\infty}}P_{ii}^{n} < \\infty$\n",
    "\n",
    "$Remark$\n",
    "\n",
    ">This argument also shows that a transient state will only be visited a finite number of times. And the straightforward conslusion is that in a finite-state Markov chain not all states can be transient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Corollary.2$\n",
    "\n",
    "If state $i$ is **recurrent**, and state $i$ communicates with state $j$, then state $j$ is **recurrent**.\n",
    "\n",
    "$Proof$\n",
    "\n",
    ">Since state $i$ communicates with state $j$, there exist integers $k$ and $m$ such that $P_{ij}^k > 0$ and $P_{ji}^m > 0$. Now, for any integer $n$: $P_{jj}^{m+n+k} \\geq P_{ji}^{m} P_{ii}^n P_{ij}^k \\Rightarrow \\d{\\sum_{n=1}^{\\infty} P_{jj}^{m+n+k}} \\geq P_{ji}^m P_{ij}^{k} \\d{\\sum_{n=1}^{\\infty} P_{ii}^n} = \\infty$\n",
    ">\n",
    ">Since $P_{ji}^{m} P_{ij}^{k} > 0$, and $\\sum\\limits_{n=1}^{\\infty} P_{ii^{n}}$ is infinite since state $i$ is **recurrent**. Thus, by $Proposition.1$ we have state $j$ is also **recurrent**.\n",
    "\n",
    "$Remark$\n",
    "\n",
    ">The preceding corollary also implies that transience is a class property. For if state $i$ is **transient** and communicates with state $j$, then state $j$ must also be **transient**.\n",
    ">\n",
    ">Also, \"not all states in a *finite Markov chain* can be transient\" $\\Rightarrow$ \"all states of a *finite irreducible Markov chain* are recurrent\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.16** \n",
    "\n",
    "Let the Markov chain consisting of the states $0, 1, 2, 3$ have the transition probability matrix\n",
    "\n",
    "$\\bspace\\mathbf{P} = \\begin{Vmatrix}\n",
    "0 & 0 & 0.5 & 0.5 \\\\ \n",
    "1 & 0 & 0 & 0 \\\\ \n",
    "0 & 1 & 0 & 0 \\\\ \n",
    "0 & 1 & 0 & 0\n",
    "\\end{Vmatrix}$\n",
    "\n",
    "Determine which states are transient and which are recurrent.\n",
    "\n",
    "> It is a simple matter to check that all states communicate and, hence, since this is a finite chain, all states must be recurrent.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.17** \n",
    "\n",
    "Consider the Markov chain having states $0, 1, 2, 3, 4$ and \n",
    "\n",
    "$\\bspace\\mathbf{P} = \\begin{Vmatrix}\n",
    "0.5 & 0.5 & 0 & 0 & 0\\\\ \n",
    "0.5 & 0.5 & 0 & 0 & 0\\\\ \n",
    "0 & 0 & 0.5 & 0.5 & 0\\\\\n",
    "0 & 0 & 0.5 & 0.5 & 0\\\\\n",
    "0.25 & 0.25 & 0 & 0 & 0.5\n",
    "\\end{Vmatrix}$\n",
    "\n",
    "Determine the recurrent state.\n",
    "\n",
    "> This chain consists of the three classes: $\\CB{0,1}, \\CB{2,3},\\CB{4}$. Easy to find that the first two classes are recurrent and the third transient.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.18** A Random Walk\n",
    "\n",
    "Consider a Markov chain whose state space consists of the integers $i = 0, \\pm1, \\pm2, \\dots$, and has transition probabilities given by $P_{i,i+1} = p = 1 - P_{i,i-1}$ for all $i$ where $1>p>0$. Since all states clearly communicate, so they all are either **transient**, or **recurrent**. So we are going to consider $\\sum_{n=1}^{\\infty} P^n_{00}$ whether it's finite or infinite.\n",
    "\n",
    "> First thing to notice is that $n$ has to be even so that $P_{00}^{2n-1}=0$. Then for even time transition we have\n",
    ">\n",
    ">$$P_{00}^{2n}=\\binom{2n}{n}p^n\\P{1-p}^{n}=\\ffrac{\\P{2n}!}{n!n!}\\P{p-p^2}^n$$\n",
    ">\n",
    ">By ***Stirling formula***: ($n! \\sim n^{n+0.5} e^{-n} \\sqrt{2\\pi} $),\n",
    ">\n",
    ">$$\\sum_{n=1}^{\\infty}P_{00}^{2n} \\sim\\sum_{n=1}^{\\infty}\\ffrac{\\P{4p-4p^2}^n} {\\sqrt{\\pi n}}, \\begin{cases}\n",
    "\\text{recurrent},&\\text{if the value is }\\infty\\\\\n",
    "\\text{transient},&\\text{if the value is }<\\infty\n",
    "\\end{cases}$$\n",
    ">\n",
    ">Applying some knowledge from number serise, since $4p\\P{1-p} < 1$ unless $p=0.5$, we assert that the chain is recurrent only when $p=0.5$.\n",
    "\n",
    "$Remark$\n",
    "\n",
    ">For this special case it's got a special name ***symmetric random walk***. And we could expand this to the second dimension where\n",
    ">\n",
    "> $$\\mathbf{P}_{\\P{i,j},\\P{i+1,j}} = \\mathbf{P}_{\\P{i,j},\\P{i-1,j}} = \\mathbf{P}_{\\P{i,j},\\P{i,j+1}} = \\mathbf{P}_{\\P{i,j},\\P{i,j+1}} = 0.25$$\n",
    ">\n",
    ">And this is also recurrent, using the same method and we can prove this. Find how in the textbook, wolalalalalalala!\n",
    ">\n",
    ">And that's the end, no higher dimensions have the same property. Oh my sad drunk man.\n",
    "\n",
    "$Remark$\n",
    "\n",
    ">For one-dimensional random walk as discussed in **e.g.18**, an direct argument can be made for establishing recurrence in the symmetric case, and for determining the probability that it ever returns to state $0$ in the nonsymmetric case. The drunk man starts at $0$, and we first let $\\beta = P\\CB{\\text{ever return to }0}$. Then we write\n",
    ">\n",
    ">$$\\beta = P\\CB{\\text{ever return to }0 \\mid X_1 =1} p + P\\CB{\\text{ever return to }0 \\mid X_1 =-1}\\P{1- p}$$\n",
    "\n",
    ">Now let $\\alpha$ denote the probability that the Markov chain will ever return to $0$ given that it is currently in $1$. Condition on the next transition we obtain\n",
    ">\n",
    ">$$\\begin{align}\n",
    "\\alpha &=P\\CB{\\text{ever return to }0 \\mid X_1 = 1} \\\\\n",
    "&= P\\CB{\\text{ever return to }0\\mid X_1 = 1,X_2 = 0}\\P{1-p} + P\\CB{\\text{ever return to }0\\mid X_1 = 1,X_2 = 2}p\\\\\n",
    "&= 1\\times\\P{1-p} + P\\CB{\\text{ever return to }0 \\mid X_1 = 1} \\cdot P\\CB{\\text{ever return to }1 \\mid X_1 = 2}p\\\\\n",
    "&= 1-p + p\\alpha^2\n",
    "\\end{align}$$\n",
    ">\n",
    ">The roots are $1$ and $\\P{1-p}/p$. When $p=0.5$ we have $\\alpha = 1$. Then by symmetry, we have\n",
    ">\n",
    ">$$\\beta = 1 \\times p + 1 \\times \\P{1-p} = 1$$\n",
    ">\n",
    ">While when $p\\neq 0.5$ we have $\\alpha = \\ffrac{1-p} {p}$, say $p>0.5$, then we first have $P\\CB{\\text{ever return to }0 \\mid X=-1} = 1$ so that $\\beta =\\alpha p + 1-p = 2\\P{1-p}$. And similarly we have when $p < 0.5$, $\\beta = 2p$ so we conclude that\n",
    ">\n",
    ">$$\\beta = P\\CB{\\text{ever return to }0} = 2\\min\\P{p,1-p}$$\n",
    "***\n",
    "\n",
    "**e.x.19** On the Ultimate Instability of the Aloha Protocol\n",
    "\n",
    ">This problem is interesting. But time sucks! Skipped!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long-Run Proportions and Limiting Probabilities\n",
    "\n",
    "For pairs of states $i \\neq j$, let $f_{i,j}$ denote the probability that the Markov chain, starting in state $i$, will ever make a transition into state $j\\newcommand{\\Exp}{\\mathrm{E}}\n",
    "\\newcommand{\\RR}{\\mathbb{R}}\n",
    "\\newcommand{\\EE}{\\mathbb{E}}\n",
    "\\newcommand{\\NN}{\\mathbb{N}}\n",
    "\\newcommand{\\ZZ}{\\mathbb{Z}}\n",
    "\\newcommand{\\QQ}{\\mathbb{Q}}\n",
    "\\newcommand{\\PP}{\\mathbb{P}}\n",
    "\\newcommand{\\AcA}{\\mathcal{A}}\n",
    "\\newcommand{\\FcF}{\\mathcal{F}}\n",
    "\\newcommand{\\AsA}{\\mathscr{A}}\n",
    "\\newcommand{\\FsF}{\\mathscr{F}}$. That is,\n",
    "\n",
    "$$f_{i,j} = P\\CB{X_n = j \\text{ for some }n>0 \\mid X_0 = i}$$\n",
    "\n",
    "$Proposition.3$ \n",
    "\n",
    "If $i$ is recurrent and $i$ communicates with $j$ , then $f_{i,j} = 1$.\n",
    "\n",
    "$Proof$\n",
    "\n",
    ">Since $i$ and $j$ communicate there's a value $n$ $s.t.$ $P_{i,j}^{n} > 0$. Let $X_0 = i$ and say that the first opportunity is a success if $X_n = j$, with probability $P_{i,j}^{n} > 0$. Since state $i$ is recurrent, thus every time the chain enters state $i$ we start to check whether $n$ steps later it stops at state $j$. And this success is with probability $P_{i,j}^{n} > 0$. Till it happens, we will stop. Then it's a geometric distribution! Then it follows that with probability $1$ a success will eventually occur and so, with probability $1$, state $j$ will eventually be entered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Def$\n",
    "\n",
    "If state $j$ is recurrent, let $m_j$ denote the expected number of transitions that it takes the Markov chain when starting in state $j$ to return to that state. That is, with $N_j = \\min\\CB{n>0:X_n = j}$, this value equal to the number of transitions until the Markov chain makes a transition into state $j$, $m_j = \\Exp\\SB{N_j \\mid X_0 = j}$. Then the recurrent state $j$ is ***positive recurrent*** if $m_j < \\infty$ and ***null recurrent*** if $m_j = \\infty$\n",
    "\n",
    "Letting $\\pi_j$ denote the long-run proportion of time that the Markov chain is in state $j$, we have the following proposition.\n",
    "\n",
    "$Proposition.4$\n",
    "\n",
    "If the Markov chain is irreducible and recurrent, then for any initial state $\\pi_j = \\ffrac{1} {m_j}$.\n",
    "\n",
    "$Proof$\n",
    "\n",
    ">Suppose that the Markov chain starts in state $i$, and let $T_n$ for $n \\geq 2$ is the number of transitions between the $\\P{n-1}$th and the $n$th transition into state $j$, that is\n",
    ">\n",
    ">- $T_1$: the number of transitions until the chain enters state $j$\n",
    ">- $T_2$: the additional number of transitions from time $T_1$ until the Markov chain next enters state $j$\n",
    ">- $T_3$: the additional number of transitions from time $T_1 + T_2$ until the Markov chain next enters state $j$, and so on.\n",
    ">\n",
    ">By $Proposition.4$, $w.p.$ $1$ a transition into $j$ will eventually occur. And by the property of markov chain, $T_i$ are independent and identically distributed with mean $m_j$. After all these definitions we have\n",
    ">\n",
    ">$$\\begin{align}\n",
    "\\pi_j &= \\lim_{n \\to \\infty} \\ffrac{n} {\\d{\\sum_{i=1}^{n} T_i}} \\\\\n",
    "&= \\lim_{n \\to \\infty} \\ffrac{1} {\\ffrac{T_1} {n} + \\ffrac{T_2 + T_3+ \\cdots + T_n} {n}}\n",
    "\\end{align}$$\n",
    ">\n",
    ">Notice that $\\lim\\limits_{n\\to\\infty}\\ffrac{T_1} {n} = 0$ and by the strong law of large numbers, we have \n",
    ">\n",
    ">$$\\lim_{n\\to\\infty} \\ffrac{T_2 + \\cdots + T_n} {n} = \\lim_{n\\to\\infty} \\ffrac{T_2 + \\cdots + T_n} {n-1}\\cdot\\ffrac{n-1} {n} = m_j$$\n",
    ">\n",
    ">Thus $\\pi_j = 1/m_j >0$\n",
    "\n",
    "$Remark$\n",
    "\n",
    ">It follows from the preceding that state $j$ is **positive recurrent** $iff$ $\\pi_j > 0$ which is also equivalent to $m_j < \\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Proposition.5$\n",
    "\n",
    "If $i$ is **positive recurrent** and $i \\leftrightarrow j$ then $j$ is also **positive recurrent**.\n",
    "\n",
    "$Proof$\n",
    "\n",
    ">Let $n$ be such that $P_{i,j}^{n}>0$. Because $\\pi_j$ is the long-run proportion of time that the chain is in state $i$, and $P_{i,j}^{n}$ is the long-run proportion of time when the Markov chain is in state $i$ that it will be in state $j$ after $n$ transitions. Then:\n",
    "\n",
    ">$$\\begin{align}\n",
    "\\pi_i \\cdot P_{i,j}^{n} &= \\text{long-run proportion of time the chain is in } i \\\\\n",
    "&\\bspace\\text{and will be in $j$ after $n$ transitions}\\\\\n",
    "&= \\text{long-run proportion of time the chain is in } i \\\\\n",
    "&\\bspace\\text{and will be in $j$ before $n$ transitions}\\\\\n",
    "& \\leq \\text{long-run proportion of time the chain is in } i\n",
    "\\end{align}$$\n",
    ">\n",
    ">Hence, $\\pi_j \\geq \\pi_j\\cdot P_{i,j}^n > 0$, showing that j is positive recurrent.\n",
    "\n",
    "$Remark$\n",
    "\n",
    "> **positive recurrent** is a class property, well, so is the **null recurrent**. Since being recurrent and being positive recurrent are both class properties.\n",
    ">\n",
    ">Also, *an **irreducible finite** state Markov chain must be **positive recurrent***. Since we've already known that such a chain  must be recurrent; hence, all its states are either positive recurrent or null recurrent. If they were null recurrent then all the long run proportions would equal $0$, which is impossible when there are only a finite number of states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then from $\\pi_i$ we move to $\\pi_j$ by summing over all $i$: $\\pi_j = \\sum \\pi_i P_{i,j}$\n",
    "\n",
    "$Theorem.1$\n",
    "\n",
    "Consider an irreducible Markov chain. If the chain is **positive recurrent** then the long-run proportions are the unique solution of the equations\n",
    "\n",
    "$$\\begin{cases}\n",
    "\\pi_j = \\d{\\sum_i \\pi_i \\cdot P_{i,j}},\\bspace j \\geq 1\\\\\n",
    "\\d{\\sum_j \\pi_j = 1}\n",
    "\\end{cases}$$\n",
    "\n",
    "Moreover, if there is no solution of the preceding linear equations, then the Markov chain is either **transient** or **null recurrent** and all $\\pi_j = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.20**\n",
    "\n",
    "Assume that if it rains today, then it will rain tomorrow with probability $\\alpha$; and if it does not rain today, then it will rain tomorrow with probability $\\beta$. If we say that the state is $0$ when it rains and $1$ when it does not rain. What's $\\pi_0$ and $\\pi_1$?\n",
    "\n",
    ">The equations are\n",
    ">\n",
    ">$$\\begin{cases}\n",
    "\\pi_0 = \\alpha \\pi_0 + \\beta \\pi_1 \\\\[0.6em]\n",
    "\\pi_1 = \\P{1-\\alpha} \\pi_0 + \\P{1-\\beta} \\pi_1 \\\\[0.6em]\n",
    "\\pi_0 + \\pi_1 = 1\n",
    "\\end{cases} \\Rightarrow \\begin{cases}\n",
    "\\pi_0 = \\ffrac{\\beta} {1+\\beta-\\alpha}\\\\\n",
    "\\pi_1 = \\ffrac{1-\\alpha} {1+\\beta -\\alpha}\n",
    "\\end{cases}$$\n",
    "***\n",
    "\n",
    "**e.g.23** The Hardy–Weinberg Law and a Markov Chain in Genetics\n",
    "\n",
    "Consider a large population of individuals. Assume that the proportions of individuals whose gene pairs are $AA$, $aa$, or $Aa$ are, respectively, $p_0$, $q_0$, and $r_0$ where ($p_0 + q_0 + r_0 = 1$). We are interested in determining the proportions of individuals in the next generation whose genes are $AA$, $aa$, or $Aa$. Calling these proportions $p$, $q$, and $r$. \n",
    "\n",
    ">First we can calculate the probability of a randomly chosen gene will be type $A$ in the next generation:\n",
    ">\n",
    ">$$P\\CB{A} = P\\CB{A\\mid AA}p_0 + P\\CB{A\\mid aa}q_0+P\\CB{A\\mid Aa}r_0 = p_0 + \\ffrac{r_0} {2}$$\n",
    ">\n",
    ">And similarly we have $P\\CB{a} = q_0 + \\ffrac{r_0}{2}$. Thus we have, under random mating, \n",
    ">\n",
    ">$$p=P\\CB{A}P\\CB{A}, q = P\\CB{a}P\\CB{a}, r = 2P\\CB{A}P\\CB{a}$$\n",
    ">\n",
    ">And an interesting fact is that the fraction of its genes that are $A$, will be unchanged from the previous generation. One way is by arguing that the total gene pool has not changed from generation to\n",
    "generation or by the following simple algebra:\n",
    ">\n",
    ">$$\\begin{align}\n",
    "p + \\ffrac{r}{2} &= \\P{p_0 +r_0/2}^2 + \\P{p_0 + r_0/2}\\P{q_0 + r_0/2} \\\\\n",
    "&= \\P{p_0 +r_0/2}\\SB{p_0 + r_0/2 + q_0 + r_0/2}\\\\[0.6em]\n",
    "&\\bspace\\text{since }{p_0+q_0+r_0 = 1}\\\\[0.6em]\n",
    "&= p_0 + r_0/2 = P\\CB{A}\n",
    "\\end{align}$$\n",
    ">\n",
    ">And then for a given individual, let $X_n$ denote the genetic state of her descendant in the $n$th generation. The transition probability matrix of this Markov chain, namely,\n",
    ">\n",
    ">$$\\begin{Vmatrix}\n",
    "p + \\ffrac{r}{2} & 0 & q+ \\ffrac{r}{2}\\\\\n",
    "0 & q + \\ffrac{r}{2} & p + \\ffrac{r}{2}\\\\\n",
    "\\ffrac{p} {2} + \\ffrac{r} {4} & \\ffrac{q} {2} + \\ffrac{r} {4} & \\ffrac{p} {2} + \\ffrac{q} {2} + \\ffrac{r} {2}\n",
    "\\end{Vmatrix}$$\n",
    ">\n",
    ">And if we want to find the limiting probabilities, here's the equations\n",
    ">\n",
    ">$$\\begin{cases}\n",
    "p = p\\P{p+\\ffrac{r} {2}} + r\\P{\\ffrac{p} {2}+\\ffrac{r} {4}} = \\P{p+\\ffrac{r} {2}}^2\\\\\n",
    "q = q\\P{q+\\ffrac{r} {2}} + r\\P{\\ffrac{q} {2}+\\ffrac{r} {4}} = \\P{q+\\ffrac{r} {2}}^2\\\\\n",
    "p+q+r = 1\n",
    "\\end{cases}$$\n",
    ">\n",
    ">And these equations are just the same with the discussion before for just the next generation.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.24**\n",
    "\n",
    "Suppose that a production process changes states in accordance with an irreducible, positive recurrent Markov chain having transition probabilities $P_{ij}, i,j = 1,\\dots, n$, and suppose that certain of the states are considered acceptable and the remaining unacceptable. Let $A$ denote the acceptable states and $A^c$ the unacceptable ones. If the production process is said to be \"up\" when in an acceptable state and \"down\" when in an unacceptable state, determine\n",
    "\n",
    "1. the rate at which the production process goes from up to down\n",
    "2. the average length of time the process remains down when it goes down\n",
    "3. the average length of time the process remains up when it goes up\n",
    "\n",
    ">Let $\\pi_k$ denote the long-run proportions. Now for $i \\in A$ and $j \\in A^c$, the rate at which the process enters state $j$ from state $i$ is: $\\pi_i P_{ij}$ and thus\n",
    ">\n",
    ">$\\bspace\\text{rate of entering }j \\text{ from }A = \\d{\\sum_{i \\in A} \\pi_i P_{ij} \\Rightarrow \\text{rate of breakdowns} = \\sum_{j \\in A^c} \\sum_{i \\in A} \\pi_i P_{ij}}$\n",
    ">***\n",
    ">Then let $\\bar U$ and $\\bar D$ denote the average time the process remains up when it goes up and down when it goes down. Because there is a single breakdown every $\\bar U + \\bar D$ time units on the average, it follows heuristically启发式地 that the rate at which breakdowns occur is $1/ \\P{\\bar U + \\bar D}$.\n",
    ">\n",
    ">$$\\sum_{j \\in A^c} \\sum_{i \\in A} \\pi_i P_{ij} = \\ffrac{1} {\\bar U + \\bar D}$$\n",
    ">\n",
    ">And the second equation comes from the thoughts on the percentage of time the process is up, which should be $\\sum\\limits_{i\\in A} \\pi_i$. Then, by the definitine of $\\bar U$ and $\\bar D$, we have \n",
    ">\n",
    ">$$\\sum_{i\\in A} \\pi_i = \\ffrac{\\bar U} {\\bar U + \\bar D}$$\n",
    ">\n",
    ">Combine the two equations and solve it, we have:\n",
    ">\n",
    ">$$\\bar U = \\ffrac{\\sum_{i\\in A} \\pi_i} {\\sum_{j \\in A^c} \\sum_{i \\in A} \\pi_i P_{ij}}, \\bar D = \\ffrac{1 - \\sum_{i\\in A} \\pi_i} {\\sum_{j \\in A^c} \\sum_{i \\in A} \\pi_i P_{ij}} = \\ffrac{\\sum_{i\\in A^c} \\pi_i} {\\sum_{j \\in A^c} \\sum_{i \\in A} \\pi_i P_{ij}}$$\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The long run proportions $\\pi_j,j\\geq 0$ are often called ***stationary probabilities***. The reason being that if the initial state is chosen according to the probabilities $\\pi_j,j\\geq 0$, then the probability of being in state $j$ at any time $n$ is also equal to $\\pi_j$:\n",
    "\n",
    "$\\bspace P\\CB{X_0 = j} = \\pi_j,j\\geq0 \\Rightarrow P\\CB{X_n = j} = \\pi_j, j\\geq 0, \\forall n$\n",
    "\n",
    "And this seems obvious by induction. If it's true when $n=0$ and suppose it true for $n-1$ then we can write:\n",
    "\n",
    "$\\bspace\\begin{align}\n",
    "P\\CB{X_n =j}&= \\sum_i P\\CB{X_n = j \\mid X_{n-1} = j }\\cdot P\\CB{X_{n-1} = i}\\\\\n",
    "&= \\sum_i P_{ij} \\pi_i \\\\\n",
    "&= \\pi_j\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.25** A conprehensive example\n",
    "\n",
    "Numbers of people check in the hotel on successive days  are independent Poisson $r.v.$s with mean $\\lambda$. Number of days one stay in the hotel is a geometric $r.v.$ with parameter $P$, $0<p<1$. (Thus no matter how long he has stayed in the hotel, the probability that he left tomorrow is still $p$). If $X_n$ denotes the number of people that are checked in the hotel at the beginning of day $n$ then $\\CB{X_n:n \\geq 0}$ is a Markov chain.\n",
    "\n",
    "1. Find the transition probabilities\n",
    "2. Find $\\Exp\\SB{X_n \\mid X_0 = i}$\n",
    "3. Find the stationary probabilities\n",
    "\n",
    "> Let $R_i$ be the number of perople that remain another day, and we can say that it is actually a binomial $r.v.$ with parameter $i$ and $1-P$. And let $N$ be the number of new people that check in that day, we see that\n",
    ">\n",
    ">$$\\begin{align}\n",
    "P_{i,j} &= P\\CB{R_i + N = j}\\\\\n",
    "&= \\sum_{k=0}^{i} P\\CB{R_i + N = j \\mid R_i = k} \\cdot\\binom{i} {k} \\P{1-p}^{k}p^{i-k}\\\\\n",
    "&= \\sum_{k=0}^{\\min\\P{i,j}} P\\CB{N = j-k} \\cdot\\binom{i} {k} \\P{1-p}^{k}p^{i-k}\\\\\n",
    "&= \\sum_{k=0}^{\\min\\P{i,j}} e^{-\\lambda}\\ffrac{\\lambda^{j-k}} {\\P{j-k}!} \\cdot\\binom{i} {k} \\P{1-p}^{k}p^{i-k}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    ">\n",
    "> Notice that $\\Exp\\SB{X_n \\mid X_{n-1} = i} = \\Exp\\SB{R_i + N} = iq + \\lambda, q = 1-p$. Consequently, $\\Exp\\SB{X_n \\mid X_{n-1}} = X_{n-1} q + \\lambda$. And take the expectation again yielding that\n",
    ">\n",
    ">$$\\Exp\\SB{X_n} = \\lambda + q\\Exp\\SB{X_{n-1}}$$\n",
    ">\n",
    ">Iterating the preceding gives $\\Exp\\SB{X_n} = \\lambda\\P{1+q+q^2+\\cdots+q^{n-1}} +q^n\\Exp\\SB{X_0} $. Thus, we have $\\Exp\\SB{X_n \\mid X_0 = i} = \\ffrac{\\lambda\\P{1-q^n}} {p} + q^n i $\n",
    ">***\n",
    ">As for the stationary probability, the fact is that using the result in the first problem and solve that equation set, is just too complicated. Rather we will make use of the fact that the **stationary probability distribution** is the *only distribution on the initial state* that results in the *next state* having the *same distribution*. \n",
    ">\n",
    ">Consider the initial state $X_0$ and the next one, we need to assume a distribution for the initial number of people checked. Intuitively, it should be a Poisson $r.v.$ since the number of people check in the next day is a Poisson $r.v.$ with parameter $\\lambda$.\n",
    ">\n",
    ">So now we assume the initial state $X_0$ has a Poisson distribution with mean $\\alpha$, then the people left in the next day is also a poisson $r.v.$ with mean $\\alpha \\cdot q$. So that to find the $\\pi_i$, we have\n",
    ">\n",
    ">$$\\alpha = \\lambda + \\alpha \\cdot q \\Rightarrow \\alpha = \\ffrac{\\lambda} {p}$$\n",
    ">\n",
    ">So that the distribution of all states are Poisson Distribution with parameter $\\ffrac{\\lambda} {p}$, which is exactly the desired stationary distribution. Then the stationary probabilities are\n",
    ">\n",
    ">$$\\pi_i = \\exp\\CB{-\\ffrac{\\lambda} {p}} \\P{\\ffrac{\\lambda} {p}}^{i} \\ffrac{1} {i!},\\bspace i \\geq 0$$\n",
    "\n",
    "$Remark$\n",
    "\n",
    ">The generalization of this example and other examples are skipped... sad\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Proposition.6$\n",
    "\n",
    "Let $\\CB{X_n, n \\geq 1}$ be an irreducible Markov chain with stationary probabilities $\\pi_j$ for $j\\geq0$, and let $r$ be a bounded function on the state space. Then, $w.p. 1$, we have\n",
    "\n",
    "$$\\lim_{N\\to\\infty} \\ffrac{\\sum\\limits_{n=1}^{N} r\\P{X_N}}{N} = \\sum_{j=0}^{\\infty} r\\P{j} \\pi_j$$\n",
    "\n",
    "$Proof$\n",
    "\n",
    ">If we let $a_{j}\\P{N}$ be the amount of time the Markov chain spends in state $j$ during time periods $1,\\dots,N$, then\n",
    ">\n",
    ">$$\\sum_{n=1}^{N} r\\P{X_N} = \\sum_{j=0}^\\infty a_j\\P{N} r\\P{j}$$\n",
    ">\n",
    ">Since $\\ffrac{a_j\\P{N}} {N} \\to \\pi_j$ the result follows from the preceding upon dividing by $N$ and then letting $N\\to\\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limiting Probabilities\n",
    "\n",
    "A chain that can only return to a state in a multiple of $d>1$ steps is said to be ***periodic*** and does not have limiting probabilities. One example could be the chain where $P_{1,0} =P_{0,1} = 1$ so that\n",
    "\n",
    "$$P_{0,0}^{\\P{n}} = \\begin{cases}\n",
    "1,&\\text{if $n$ is even}\\\\\n",
    "0,&\\text{if $n$ is odd}\n",
    "\\end{cases}$$\n",
    "\n",
    "and it doesn't have a limiting probabilities as $n\\to \\infty$.\n",
    "\n",
    "However, for an irreducible chain that is not **periodic**, and such chains are called ***aperiodic***, the limiting probabilities will always exist and will not depend on the initial state, with its value $\\pi_j$ for state $j$, same with the long-run proportion of time the chain is in state $j$. We can find the result by first letting $\\alpha_j = \\d{\\lim _{n\\to\\infty} P\\CB{X_n = j} }$; then since $\\d{\\sum_{i=0}^{\\infty} P\\CB{X_n= i}=1}$ and \n",
    "\n",
    "$$P\\CB{X_{n+1} = j} = \\sum_{i=0}^{\\infty}P\\CB{X_{n+1}= j \\mid X_n = i} \\cdot P\\CB{X_n = j} = \\sum_{i=0}^{\\infty} P_{ij} P\\CB{X_n = i}$$\n",
    "\n",
    "letting $n\\to\\infty$ in the preceding two equations yields, upon assuming that we can bring the limit inside the summation, that\n",
    "\n",
    "$$\\alpha_j = \\sum_{i=0}^{\\infty} \\alpha_i   P_{ij} ,\\bspace 1 = \\sum_{i=0}^{\\infty} \\alpha_j$$\n",
    "\n",
    "And these're the SAME equations for $\\pi_j$, showing that actually $\\alpha_j = \\pi_j, j \\geq 0$. An **irreducible**, **positive recurrent**, **aperiodic** Markov chain is said to be ***ergodic***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>**Summary**</center>\n",
    "\n",
    "- $i \\to j$: **accessible** $\\iff$ $P_{ij}>0\\\\[0.6em]$\n",
    "- $i \\leftrightarrow j$: **communicate** $\\iff$ $P_{ij}>0, P_{ji}>0\\\\[0.6em]$\n",
    "- **Irreducible**, if there's only $1$ class, meaning that all states are **communicate** with each other.$\\\\[1em]$\n",
    "- $f_{ij} = P\\CB{\\text{ever return }j \\mid X_0 = i}$\n",
    "    - $f_{ii} = P\\CB{\\text{ever return }i \\mid X_0 = i} \\begin{cases}\n",
    "    =1,&\\text{state } i \\text{ is recurrent}\\\\\n",
    "    <1,&\\text{state } i \\text{ is transient}\\\\\n",
    "    \\end{cases}\\\\[0.9em]$\n",
    "- **Hitting time**终于说出口了没错就是他，*Stopping time*: $\\tau_j = \\inf\\CB{n\\geq 0:X_n=j}$, or a stronger version: $\\tau^+=\\inf\\CB{n>0:X_n=j}$. \n",
    "- The *expected* **hitting time**: $m_{ij} = \\Exp\\SB{\\tau_j^+ \\mid X_0 = i} = \\sum\\limits_{n=1}^{\\infty} n\\cdot P\\CB{\\tau_j^+ = n\\mid X_0 = i} = \\sum\\limits_{n=1}^{\\infty} n\\cdot f^{\\P{n}}_{ij}$\n",
    "- Here the $f^{\\P{n}}_{ij}$ is another probability defined as: \n",
    "$$f^{\\P{n}}_{ij} = P\\CB{X_n = j,X_m\\neq j,1\\leq m < n} = P_i\\CB{ \\tau_j^+ = n }$$\n",
    "- And then we have $f_{ij} = \\sum\\limits_{n=1}^{\\infty} f_{ij}^{\\P{n}} = P\\CB{\\exists\\, n>0, \\space s.t.\\space X_n=j\\mid X_0=i}$\n",
    "- Also in $homework.6$ we've proved that $P_{ij}^{\\P{n}} = \\d{\\sum_{m=1}^{n} f_{ij}^{\\P{m}}\\cdot P_{ij}^{\\P{n-m}} }$. With this we can further infer that (or use the indicator method like in $Proposition4.1$)\n",
    "$$\\begin{align}\n",
    "f_{ii} = 1 &\\iff \\sum\\nolimits_{i=0}^{\\infty} P_{ii}^{\\P{n}} = \\infty\\\\\n",
    "f_{ii} < 1 &\\iff \\sum\\nolimits_{i=0}^{\\infty} P_{ii}^{\\P{n}} = \\ffrac{1} {1-f_{ii}} < \\infty\n",
    "\\end{align}$$\n",
    "- And finally we have \n",
    "$$\\begin{align}\n",
    "f_{ii}<1, \\text{transient} &\\Rightarrow m_{ii} = \\infty \\\\\n",
    "f_{ii}=1, \\text{recurrent} &\\Rightarrow \\begin{cases}\n",
    "m_{ii} = \\infty, &\\text{null recurrent}\\\\[0.7em]\n",
    "m_{ii} < \\infty, &\\text{positive recurrent}\n",
    "\\end{cases}\n",
    "\\end{align}\\\\[0.5em]$$\n",
    "\n",
    "- The **long-run proportion of time**, or the **stationary probability**, is $\\pi_j = 1/m_{jj}$. Meaning that at later time, $\\forall \\;n$, $P\\CB{X_n=j} = \\pi_j = P\\CB{X_0 = j}, j\\geq 0$.\n",
    "- **stationary probability** may not be the **limiting probability**, which will exist only when the Markov Chain is **not periodic (aperiodic)**, and **irreducible**.\n",
    "- **ergodic** means **irreducible**, **positive  recurrent**, and **aperiodic**. \n",
    "- More on **periodic**. If **irreducible**, and for one state that $d_j=1$, we then have all states with period $1$. And then **aperiodic**.\n",
    "- ***Limit Theorem***: An **irreducible**, **aperiodic** Markov Chain belongs to one of the following classes\n",
    "    - Either the states are all **transient** or all **null recurrent**, where $p_{ij}^{\\P{n}} \\to 0$ as $n\\to\\infty$ for all $i,j$ and there exists no stationary distribution\n",
    "    - Or else, all states are **positive recurrent**, that is when we have the stationary distribution\n",
    "    \n",
    "$$\\lim_{n\\to\\infty} p_{ij}^{\\P{n}} = \\pi_j >0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Applications\n",
    "### The Gambler's Ruin Problem\n",
    "\n",
    "Consider a gambler who at each play of the game has probability $p$ of winning one unit and probability $q = 1 − p$ of losing one unit. Assuming that successive plays of the game are independent, what is the probability that, starting with $i$ units, the gambler’s fortune will reach $N$ before reaching $0$?\n",
    "\n",
    "Let $X_n$ denote the player's fortune at time $n$, then the process $\\CB{X_n,n =0,1,2,\\dots}$ is a Markov chain with transition probabilities $P_{00} = P_{NN} = 1$ and $P_{i,i+1} = p = 1-P_{i,i-1}$ for $i=1,2,\\dots,N-1$.\n",
    "\n",
    "There're $3$ classes and $\\CB{N}$ and $\\CB{0}$ are recurrent and $\\CB{1,2,\\dots, N-1}$ is transient. Since each transient state is visited only finitely often, it follows that, after some finite amount of time, the gambler\n",
    "will either attain his goal of $N$ or go broke.\n",
    "\n",
    "Let $P_i$, $i = 0, 1, \\dots, N$, denote the probability that, starting with $i$, the gambler’s fortune will eventually reach $N$. Condition on the intial play, we obtain:\n",
    "\n",
    "$$P_i = p\\cdot P_{i+1} + q\\cdot P_{i-1} \\stackrel{p+1=1}{\\Longrightarrow} P_{i+1} - P_i = \\ffrac{q} {p}\\P{P_i - P_{i-1}}, \\bspace i=1,2,\\dots,N-1$$\n",
    "\n",
    "Solve this progression we have\n",
    "\n",
    "$$P_i = P_1\\SB{1+\\P{\\ffrac{q} {p}} + \\P{\\ffrac{q} {p}}^2 + \\cdots + \\P{\\ffrac{q} {p}}^{i-1}} = \\begin{cases}\n",
    "i P_1, &\\text{if }\\ffrac{q} {p}=1\\\\\n",
    "\\ffrac{1-\\P{\\ffrac{q} {p}}^i} {1-\\ffrac{q} {p}} P_1, &\\text{if }\\ffrac{q} {p}\\neq1\n",
    "\\end{cases}$$\n",
    "\n",
    "then by the fact that $P_N=1$, we have\n",
    "\n",
    "$$P_1 = \\begin{cases}\n",
    "\\ffrac{1}{N}, &\\text{if }\\ffrac{q} {p}=1\\\\\n",
    "\\ffrac{1-\\ffrac{q} {p}}{1-\\P{\\ffrac{q} {p}}N}, &\\text{if }\\ffrac{q} {p}\\neq1 \n",
    "\\end{cases}\\bspace P_i = \\begin{cases}\n",
    "\\ffrac{i}{N}, &\\text{if }\\ffrac{q} {p}=1 \\iff p=0.5\\\\\n",
    "\\ffrac{1-\\P{\\ffrac{q} {p}}^i} {1-\\P{\\ffrac{q} {p}}^N}, &\\text{if }\\ffrac{q} {p}\\neq1 \\iff p\\neq 0.5\n",
    "\\end{cases}$$\n",
    "\n",
    "As $N\\to\\infty$, we have $P_i = 0$ when $p\\leq 0.5$, and $P_i = 1-\\P{\\ffrac{q} {p}}^i$ when $p>0.5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An application of this, the drug testing. Suppose that two new drugs have been developed for treating a certain disease. Drug $i$ has a cure rate $P_i$ for $i=1,2$, in the sense that each patient treated with drug $i$ will be cured with $P_i$, however two unknown rates. Our interest is to determine whether $P_1 >P_2$ or $P_2>P_1$. We'll test paris of patients where one for drug $1$ and one for the other. Let\n",
    "\n",
    "$$X_j = \\begin{cases}\n",
    "1,&\\text{if the patient in the $j$th pair to receive drug number $1$ is cured}\\\\\n",
    "2,&\\ow\n",
    "\\end{cases}\\\\\n",
    "Y_j = \\begin{cases}\n",
    "1,&\\text{if the patient in the $j$th pair to receive drug number $2$ is cured}\\\\\n",
    "2,&\\ow\n",
    "\\end{cases}$$\n",
    "\n",
    "For a predetermined positive integer $M>0$ the test stops after pair $N$ where $N$ is the first value of $n$ such that \n",
    "\n",
    "$$\\sum_{i=1}^{n}X_i - \\sum_{i=1}^{n}Y_i = \\pm M$$\n",
    "\n",
    "For positive right side we asset that $P_1>P_2$ and the inverse one otherwise. Then what's the probability that the test will incorrectly assert $P_1>P_2$ when actually $P_1<P_2$?\n",
    "\n",
    "Note that after each pair is checked the cumulative difference of cures using drug $1$ versus drug $2$ will either go up by $1$ with probability $P_{1}\\P{1-P_2}$ or go down by $1$ with probability $\\P{1-P_1}P_2$, or remain the same otherwise. Hence, if we only consider those pairs in which the cumulative difference changes, then the difference will go up $1$ with probability $p$ and down $1$ with probability $1-p$ where\n",
    "\n",
    "$$p=P\\CB{\\text{up }1 \\mid \\text{up $1$ or down $1$}} = \\ffrac{P_1\\P{1-P_2}} {P_1\\P{1-P_2}+\\P{1-P_1}P_2}$$\n",
    "\n",
    "Hence, the probability that the test will assert that $P_2 > P_1$ is equal to the probability that a gambler who wins each bet for one unit with probability $P$ will go down $M$ before going up $M$. Thus, let $i=M$ and $N=2M$ showing that this probability is given by\n",
    "\n",
    "$$P\\CB{\\text{test asserts that $P_2>P_1$}} =1-\\ffrac{1-\\P{\\ffrac{q} {p}}^M} {1-\\P{\\ffrac{q} {p}}^{2M}}=\\ffrac{1} {1+\\P{\\ffrac{q} {p}}^{M}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Time Spent in Transient States\n",
    "\n",
    "Consider now a finite state Markov chain and suppose that the states are numbered so that $T ={1, 2,\\dots ,t}$ denotes the set of **transient states**. Let\n",
    "\n",
    "$$\\mathbf{P}_{T} = \\begin{bmatrix}\n",
    "P_{11} & P_{12} & \\cdots & P_{1t} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "P_{t1}& P_{t2} & \\cdots & P_{tt}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Notice that this matrix only indicate the transition probabilities from transient states to transient states and thus the row sum could be less than $1$. For transient states $i$ and $j$, let $s_{ij}$ denote the expected number of time periods that the Markov chain is in state $j$, given that it starts in state $i$. Then let $\\delta_{i,j}=1$ when $i=j$ and $0$ otherwise. Condition on the initial transition to obtain\n",
    "\n",
    "$$\\begin{align}\n",
    "s_{ij} &= \\delta_{i,j} + \\sum_{k} P_{ik} s_{kj}\\\\\n",
    "&= \\delta_{i,j} + \\sum_{k=1}^{t} P_{ik} s_{kj}\n",
    "\\end{align}$$\n",
    "\n",
    "where the final equality follows since it is impossible to go from a recurrent to a transient state, implying that $s_{kj}=0$ when $k$ is a recurrent state. And we also define the matrix $\\mathbf S$:\n",
    "\n",
    "$$\\mathbf S = \\begin{bmatrix}\n",
    "s_{11} & s_{12} & \\cdots & s_{1t} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "s_{t1}& s_{t2} & \\cdots & s_{tt}\n",
    "\\end{bmatrix} = \\mathbf{I}_t + \\mathbf{P}_T \\mathbf S \\Longrightarrow \\mathbf S = \\P{\\mathbf{I}_t - \\mathbf{P}_T}^{-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.30**\n",
    "\n",
    "Consider the gambler’s ruin problem with $p = 0.4$ and $N = 7$. Starting with $3$ units, determine the expected amount of time the gambler has $5$ units\n",
    "\n",
    ">First we can write the matrix for $\\mathbf{P}_T$, which specifies $P_{ij}$, $i,j\\in \\CB{1,2,3,4,5,6}$.\n",
    ">\n",
    ">$$\\mathbf{P}_T = \\begin{bmatrix}\n",
    "0 & 0.4 & 0 & 0 & 0 & 0\\\\\n",
    "0.6 & 0 & 0.4 & 0 & 0 & 0 \\\\\n",
    "0 & 0.6 & 0 & 0.4 & 0 & 0 \\\\\n",
    "0 & 0 & 0.6 & 0 & 0.4 & 0 \\\\\n",
    "0 & 0 & 0 & 0.6 & 0 & 0.4 \\\\\n",
    "0 & 0 & 0 & 0 & 0.6 & 0\\\\\n",
    "\\end{bmatrix}$$\n",
    ">\n",
    ">And then we invert $\\mathbf{I}_6 - \\mathbf{P}_T$ to find $S$:\n",
    "\n",
    ">$$\\mathbf{S} = \\P{\\mathbf{I}_6 - \\mathbf{P}_T}^{-1} = \\begin{bmatrix}\n",
    "1.6149 & 1.0248 & 0.6314 & 0.3691 & 0.1943 & 0.0777\\\\\n",
    "1.5372 & 2.5619 & 1.5784 & 0.9228 & 0.4857 & 0.1943\\\\\n",
    "1.4206 & 2.3677 & 2.9990 & 1.7533 & \\mathbf{0.9228} & 0.3691\\\\\n",
    "1.2458 & 2.0763 & 2.6299 & 2.9990 & 1.5784 & 0.6314\\\\\n",
    "0.9835 & 1.6391 & 2.0763 & 2.3677 & 2.5619 & 1.0248\\\\\n",
    "0.5901 & 0.9835 & 1.2458 & 1.4206 & 1.5372 & 1.6149\n",
    "\\end{bmatrix}$$\n",
    ">\n",
    ">Hence, $s_{3,5} = 0.9228$\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we will derive the formula combining $s_{ij}$ and $f_{ij}$, the probability that the Markov chain ever makes a transition into state $j$ given that it starts in state $i$.\n",
    "\n",
    "$$\\begin{align}\n",
    "s_{ij} &= \\Exp\\SB{\\text{time in $j$} \\mid \\text{start in $i$, ever transit to $j$}}\\cdot f_{ij} + \\Exp\\SB{\\text{time in $j$} \\mid \\text{start in $i$, never transit to $j$}}\\cdot \\P{1-f_{ij}}\\\\\n",
    "&= \\P{\\delta_{i,j} +s_{jj}}\\cdot f_{ij} + \\delta_{i,j}\\cdot \\P{1-f_{ij}}\\\\\n",
    "&= \\delta_{i,j} + f_{ij} s_{jj}\n",
    "\\end{align}$$\n",
    "\n",
    "And with this we have $f_{ij} = \\ffrac{s_{ij} - \\delta_{i,j}} {s_{jj}}$.\n",
    "\n",
    "**e.g.31** extended **e.g.30**\n",
    "\n",
    "What is the probability that the gambler ever has a fortune of $1$?\n",
    "\n",
    ">$$f_{3,1} = \\ffrac{s_{3,1} - \\delta_{3,1}} {s_{1,1}} = \\ffrac{1.4206} {1.6149} = 0.8797$$\n",
    ">***\n",
    ">And we can also check this answer with the conclusion obtained in the last section.\n",
    ">\n",
    ">For this gambler, $f_{3,1}$ is just the probability that a gambler starting with $3$ reaches $1$ before $7$. And that the same with, by the fact that MC is **Time-homogenous**, the probability that a gambler starting with $2$ will go down to $0$ before reaching $6$. \n",
    ">\n",
    ">$$f_{3,1} = 1 - \\ffrac{1-\\P{\\ffrac{0.6} {0.4}}^2}{1-\\P{\\ffrac{0.6} {0.4}}^6}=0.8797$$\n",
    "***\n",
    "\n",
    "And the expected time until the Markov chain enters some sets of states $A$, can be obtained by letting all probabilities of states of $A$, $P_{ij}$, to be $1$ if $i=j$ and $0$ otherwise, meaning, an absorbing state. This process changes all states of $A$ into recurrent states, and changes all states outside of $A$ from which an eventual transition into $A$ is possible, into a transient state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Branching Processes\n",
    "\n",
    "Consider a population consisting of individuals able to produce offspring of the same kind. Suppose that each individual will, by the end of its lifetime, have identically produced $j$ new offspring with probability $P_j <1$ with $j\\geq0$, independent of the numbers produced by other individuals. The number of individuals initially is $X_0$, called the ***size of the zeroth generation***. And also let $X_n$ represent the size of the $n$th generation. It follows that $\\CB{X_n,n=0,1,\\dots}$ is a Markov chain.\n",
    "\n",
    "Note that state $0$ is a recurrent state since clearly $P_{00}=1$ since clearly no individual makes no further generations. Also if $P_0 = P\\CB{X_1 = 0 \\mid X_0=1}>0$, all other states are transient, and this follows since $P_{i0} = P_0^i$. Moreover, since any finite set of transient states $\\CB{1,2,\\dots,n}$ will be visited only finitely often if $P_0>0$, the population will either *die out* or its size will *converge to infinity*.\n",
    "\n",
    "$Remark$\n",
    "\n",
    ">Compare this with random walk, their difference lies in the \"jump-free\" property where random walk can only move to the neighbour states, either left or right, however in branching process, the length of step to the right (more generations) is unlimited.\n",
    ">\n",
    ">Also $X_0=1$ is always supposed to be true.\n",
    "\n",
    "Let $\\mu = \\d{\\sum_{j=0}^{\\infty} j\\cdot P_j}$ denote the mean number of offspring of a single individual, and the variance is\n",
    "\n",
    "$$\\sigma^2 = \\sum_{j=0}^{\\infty} \\P{j-\\mu}^2\\cdot P_j$$\n",
    "\n",
    "Now, let $Z_i^{\\P{n-1}}$ denote the number of offspring of the $i$th individual of the $(n − 1)$st generation. And with this we define\n",
    "\n",
    "$$X_n = \\sum_{i=1}^{X_{n-1}}Z_i^{\\P{n-1}}$$\n",
    "\n",
    "By conditioning on $X_{n-1}$ we have\n",
    "\n",
    "$$\\begin{align}\n",
    "\\Exp\\SB{X_n} &= \\Exp\\SB{\\Exp\\SB{X_n\\mid X_{n-1}}}\\\\\n",
    "&= \\Exp\\SB{\\Exp\\SB{\\sum_{i=1}^{X_{n-1}}Z_i^{\\P{n-1}} \\mid X_{n-1}}}\\\\\n",
    "&= \\Exp\\SB{\\mu\\cdot X_{n-1}}\\\\\n",
    "&= \\mu\\Exp\\SB{X_{n-1}}\\\\\n",
    "&\\text{iteration}\\\\\n",
    "\\Longrightarrow \\Exp\\SB{X_n} &= \\mu^{n}\\Exp\\SB{X_0}\n",
    "\\end{align}$$\n",
    "\n",
    "Then by the fact that $\\Exp\\SB{X_0} = 1$, $\\Exp\\SB{X_n} =\\mu^{n}$. And similarly we have the variance formula\n",
    "\n",
    "$$\\begin{align}\n",
    "\\Var{X_n} &= \\Exp\\SB{\\Var{X_n\\mid X_{n-1}}} + \\Var{\\Exp\\SB{X_n\\mid X_{n-1}}}\\\\\n",
    "&= \\Exp\\SB{X_{n-1}\\sigma^2} + \\Var{X_{n-1}\\mu}\\\\\n",
    "&= \\sigma^2 \\mu^{n-1} + \\mu^2 \\Var{X_{n-1}}\\\\\n",
    "&\\text{iteration}\\\\\n",
    "\\Longrightarrow \\Var{X_n} &= \\begin{cases}\n",
    "n\\sigma^2, &\\text{if }\\mu = 1\\\\\n",
    "\\sigma^2\\mu^{n-1} \\P{\\ffrac{1-\\mu^n} {1-\\mu}}, &\\text{if }\\mu \\neq 1\n",
    "\\end{cases}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\pi_0$ denote the probability that the population will eventually die out (under the assumption that $X_0 = 1$). More formally,\n",
    "\n",
    "$$\\pi_0 = \\lim_{n\\to \\infty} P\\CB{X_n = 0 \\mid X_0 = 1}$$\n",
    "\n",
    "If $\\mu = \\d{\\sum_{j=0}^{\\infty} j\\cdot P_j} <1$, we assert that $\\pi_0 = 1$, because\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mu^n &= \\Exp\\SB{X_n}\\\\\n",
    "&= \\sum_{j=0}^{\\infty} j\\cdot P_j \\\\\n",
    "&\\geq \\sum_{j=0}^{\\infty} 1\\cdot P_j \\\\\n",
    "&= P\\CB{X_n \\geq 1}\n",
    "\\end{align}$$\n",
    "\n",
    "And obviously $\\mu^n \\to 1$ when $n$ goes to infinity, we have $P\\CB{X_n = 0} = 1 - P\\CB{X_n \\geq 1} \\to 1$. And more interestly, $\\pi_0 = 1$ even when $\\mu = 1$. Only when $\\mu >1$ can we get a $\\pi_0$ less than $1$. To derive this we condition the probability of dying out on the number of offspring of the initial individual and obtain\n",
    "\n",
    "$$\\begin{align}\n",
    "\\pi_0 &= P\\CB{\\text{population dies out}} \\\\\n",
    "&= \\sum_{j=0}^{\\infty} P\\CB{\\text{population dies out} \\mid X_1 = j}\\cdot P_j \\\\\n",
    "&\\bspace\\text{each family is assumed to act independently}\\\\\n",
    "&= \\sum_{j=0}^{\\infty} \\pi_0^j \\cdot P_j\n",
    "\\end{align}$$\n",
    "\n",
    "And actually the smallest positive number satisfying this equation is $\\pi_0$ given $\\mu >1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.32** \n",
    "\n",
    "$P_0 = 0.5$, $P_1 = 0.25$, $P_2 = 0.25$. Find $\\pi_0$\n",
    "\n",
    ">$$\\mu = 0\\times0.5 + 1 \\times 0.25 + 2 \\times 0.25 = 0.75 < 1$$\n",
    ">\n",
    ">Thus $\\pi_0 = 1$\n",
    "\n",
    "***\n",
    "\n",
    "**e.g.33**\n",
    "\n",
    "$P_0 = 0.25$, $P_1 = 0.25$, $P_2 = 0.5$. Find $\\pi_0$\n",
    "\n",
    ">We can write the equation $\\pi_0 = 0.25 \\times \\pi_0^0 + 0.25 \\times \\pi_0^1 + 0.5 \\times \\pi_0^2$ and solve this we have $\\pi_0 = 0.5$.\n",
    "\n",
    "***\n",
    "\n",
    "**e.g.34**\n",
    "\n",
    "What is the probability that the population will die out if it initially consists of $n$ individuals?\n",
    "\n",
    ">$\\pi_0^n$, since the population will die out if and only if the families of each of the members of the initial generation die out.\n",
    "\n",
    "***\n",
    "\n",
    "$Remark$\n",
    "\n",
    ">$\\mu > 1$ is called the super critical; $\\mu = 1$ is called the critical and $\\mu<1$ is called the sub critial.\n",
    ">\n",
    ">Also, you will encounter some complex equation like $\\pi_0 = \\P{1-p}^2 + 2p\\P{1-p}\\pi_0 + p^2\\pi_0$. Don't be afraid. First notice that $\\pi_0 = 1$, then use ***Viere Theorem*** that $x_1x_2 = c/a$.\n",
    "\n",
    "$Theorem$\n",
    "\n",
    "Suppose that $p_0>0$ and $p_0 + p_1<1$, then $\\pi_0$ is the **smallest positive** number satisfying\n",
    "\n",
    "$$\\pi_0 = \\sum_{j=0}^{\\infty} \\pi_0^j p_j$$\n",
    "\n",
    "And it is $\\pi_0 = 1\\iff \\mu \\leq 1$\n",
    "\n",
    "$Proof$\n",
    "\n",
    ">Let $\\pi$ satisfy the equation. By induction we can show $\\pi \\geq P\\CB{X_n = 0}$ for all $n$. Now\n",
    ">\n",
    ">$$\\pi =\\sum_{j=0}^{\\infty} \\pi^j p_j \\geq \\pi^0 p_0 = P\\CB{X_1 = 0} $$\n",
    ">\n",
    ">Assuming $\\pi \\geq P\\CB{X_n = 0}$, we have\n",
    ">\n",
    ">$$\\begin{align}\n",
    "P\\CB{X_{n+1} =0} &= \\sum_j P\\CB{X_{n+1} = 0 \\mid X_1 = j} \\cdot p_j \\\\\n",
    "&= \\sum_j \\P{ P\\CB{X_n = 0} }^j \\cdot p_j \\\\\n",
    "&\\leq \\sum_{j=0}^{\\infty} \\pi^j p_j = \\pi\n",
    "\\end{align}$$\n",
    ">\n",
    ">Letting $n\\to\\infty$ we have $\\pi\\geq \\d{\\lim_{n\\to\\infty} P\\CB{X_n = 0}} = \\pi_0$\n",
    ">\n",
    ">When $\\mu\\leq 1$, we first define the generating function $\\phi\\P{s} = \\sum_{j=0}^{\\infty} s^j \\cdot p_j$. Since $p_0 + p_1 <1$, we have, for all $s \\in \\P{0,1}$\n",
    ">\n",
    ">$$\\phi'\\P{s} = \\sum_{j=0}^{\\infty} j \\cdot s^{j-1} p_j > 0\\\\\n",
    "\\phi''\\P{s} = \\sum_{j=0}^{\\infty} j\\P{j-1} \\cdot s^{j-2} p_j > 0$$\n",
    ">\n",
    ">Notice that $\\phi\\P{\\pi_0} = \\pi_0$, and $\\phi'\\P{1} = \\mu$. Consider the intercept between $\\phi\\P{s}$ and $s$ from $\\P{0,1}$. So that in order to intercept, it must satisfy that $\\mu = \\phi'\\P{1} \\leq 1$.\n",
    "\n",
    "$Remark$\n",
    "\n",
    ">Here $P\\CB{X_{n+1} = 0 \\mid X_1 = j} = \\P{ P\\CB{X_n = 0} }^j$ is easy to understand, one thing to notice is that if we define $\\tau = \\inf\\CB{n,X_n = 0}$, then $P\\CB{\\tau = k \\mid X_1 = j} \\neq \\P{ P\\CB{\\tau = k \\mid X_1 = j} }^j$. However,\n",
    ">\n",
    ">$$P\\CB{\\tau \\leq k \\mid X_1 = j} = \\P{ P\\CB{\\tau \\leq k \\mid X_1 = j} }^j$$\n",
    "***\n",
    "\n",
    "$Remark$\n",
    "\n",
    ">Suppose that the population becomes extinct for the first time in $\\tau$-th generation. Then given that $X_0=1$ we have $P\\CB{\\tau = n} = P\\CB{\\tau\\leq n} - P\\CB{\\tau \\leq n-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Reversible Markov Chains\n",
    "\n",
    "Consider a **stationary ergodic** Markov chain having transition probabilities $P_{ij}$ and stationary probabilities $\\pi_i$, and suppose that starting at some time we trace the sequence of states going *backward* in time. Or to say:\n",
    "\n",
    "Starting at time $n$, consider the sequence of states $X_n, X_{n-1},X_{n-2},\\dots$. It turns out that this sequence of states is itself a *Markov chain* with transition probabilities $Q_{ij}$ defined by\n",
    "\n",
    "$$\\begin{align}\n",
    "Q_{ij} &= P\\CB{X_m = j\\mid X_{m+1} = i}\\\\\n",
    "&= \\ffrac{P\\CB{X_m = j, X_{m+1} = i}} {P\\CB{X_{m+1} = i}} \\\\\n",
    "&= \\ffrac{P\\CB{X_m = j}\\cdot P\\CB{X_{m+1} = i \\mid X_m = j}}{P\\CB{X_{m+1} = i}}\\\\\n",
    "&= \\ffrac{\\pi_j P_{ji}} {\\pi_i}\n",
    "\\end{align}$$\n",
    "\n",
    "We need to verify that $P\\CB{X_m = j \\mid X_{m+1} = i,X_{m+2},X_{m+3},\\dots} = P\\CB{X_m = j \\mid X_{m+1} = i}$. To see this we suppose currently at time $m+1$. Since $X_0,X_1,\\dots$ is a Markov Chain, it follows that the conditional distribution of the future $X_{m+2},X_{m+3},\\dots$ given the present state $X_{m+1}$ is independent of the past state $X_m$. HOWEVER, *independence* is a **symmetric** relationship, meaning that given $X_{m+1}$, $X_m$ is independent of $X_{m+2},X_{m+3},\\dots$. Done. And thus,\n",
    "\n",
    "$$Q_{ij} = \\ffrac{\\pi_j P_{ji}} {\\pi_i} $$\n",
    "\n",
    "Besides, if $Q_{ij} = P_{ij} \\iff \\pi_i P_{ij} = \\pi_j P_{ji}$ for all $i,j$, then the Markov chain is said to be ***time reversible***. And this can also be stated that, for all states $i$ and $j$, the rate at which the process goes from $i$ to $j$, $\\pi_i P_{ij}$, is equal to the rate at which it goes from $j$ to $i$, $\\pi_j P_{ji}$.\n",
    "\n",
    "And an obvious conclusion is that, \n",
    "\n",
    "- the rate at which the *forward* process makes a transition from $j$ to $i$\n",
    "- the rate at which the *reverse* process makes a transition from $i$ to $j$\n",
    "- if **time reversible**, then the *forward* process makes a transition from $i$ to $j$\n",
    "\n",
    "Or, we can find the solution of:\n",
    "\n",
    "$$\\begin{cases}\n",
    "x_iP_{ij} = x_jP_{ji},&\\text{for all }i,j \\\\\n",
    "\\sum\\limits_i x_i = 1\n",
    "\\end{cases}$$\n",
    "\n",
    "Also if we summing over $j$ for the first one, it leads to \n",
    "\n",
    "$$\\begin{align}\n",
    "\\sum\\limits_i x_iP_{ij} &= \\sum\\limits_i x_jP_{ji}\\\\\n",
    "&= x_j\\sum_i P_{ji} = x_j\n",
    "\\end{align}$$\n",
    "\n",
    "Then it's obvious that $x_i = \\pi_i$ for all $i$, is the *unique solution* of the preceding, and it's just the stationary probabilities, or the limiting probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Remark$\n",
    "\n",
    ">***weak symmetric (可配称)***: $x_i P_{ij} = x_jP_{ji}$, $\\forall \\;i,j$. And moreover, if $\\sum\\nolimits_i x_i < \\infty$, meaning that the Markov Chain is **可合的** then it's **time-reversible**\n",
    ">\n",
    ">**Time-reversible**: either **ergodic**, or ***weak symmetric***. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.35**\n",
    "\n",
    "Consider a random walk with states $0, 1,\\dots, M$ and transition probabilities\n",
    "\n",
    "$$\\begin{cases}\n",
    "P_{i,i+1} = \\alpha_i = 1 - P_{i,i-1}, & i = 1,\\dots,M-1\\\\\n",
    "P_{0,1} = \\alpha_0 = 1-P_{0,0}\\\\\n",
    "P_{M,M} = \\alpha_M = 1-P_{M,M-1}\n",
    "\\end{cases}$$\n",
    "\n",
    "This is surely a Markov Chain, but it's also **time reversible**. Since any two transitions from $i$ to $i+1$ there must be one from $i+1$ to $i$, and true conversely. How can you move from state $i$ to $j$ *twice* without coming back?\n",
    "\n",
    "Hence, it follows that the rate of transitions from $i$ to $i + 1$ equals the rate from $i + 1$ to $i$, and so the process is time reversible. Then the limiting probabiliies, by equating for each state, $0,1,\\dots,M-1$, the rate at which the process goes from $i$ to $i + 1$ with the rate at which it goes from $i + 1$ to $i$.\n",
    "\n",
    "$$\\left\\{\\begin{align}\n",
    "\\pi_0\\alpha_0 &= \\pi_1\\P{1- \\alpha_1}\\\\\n",
    "\\pi_1\\alpha_1 &= \\pi_2\\P{1- \\alpha_2}\\\\\n",
    "&\\;\\vdots\\\\\n",
    "\\pi_i\\alpha_i &= \\pi_{i+1}\\P{1- \\alpha_{i+1}}\n",
    "\\end{align}\\right. \\Longrightarrow \\pi_i = \\ffrac{\\alpha_{i-1} \\cdots \\alpha_0}{\\P{1-\\alpha_i}\\cdots\\P{1-\\alpha_1}}\\pi_0, i = 1,2,\\dots,M$$\n",
    "\n",
    "Then since $\\sum_{0}^{M} \\pi_i = 1$, we obtain\n",
    "\n",
    "$$\\pi_0 = \\SB{1+\\sum_{j=1}^{M} \\ffrac{\\alpha_{j-1} \\cdots \\alpha_0}{\\P{1-\\alpha_j}\\cdots\\P{1-\\alpha_1}}}^{-1}$$\n",
    "\n",
    "And, in some special case, $\\alpha \\equiv \\alpha$, then $\\pi_0 = \\ffrac{1-\\beta}{1-\\beta^{M+1}}$ where $\\beta = \\ffrac{\\alpha}{1-\\alpha}$. And $\\pi_i = \\ffrac{\\beta^i \\P{1-\\beta}} {1-\\beta^{M+1}}$\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another special case is for the urn model where $\\alpha_i = \\ffrac{M-i}{M}$ for $i = 0,1,\\dots,M$. Hence\n",
    "\n",
    "$$\\pi_0 = \\SB{1+\\sum_{j=1}^{M}\\ffrac{\\P{M-j+1}\\cdots\\P{M-1}M}{j\\P{j-1}\\cdots 1}}^{-1} = \\ffrac{1}{\\d{\\sum_{j=0}^{M}\\binom{M}{j}}} = \\P{\\ffrac{1}{2}}^M\\\\\n",
    "\\pi_i = \\binom{M}{i}\\P{\\ffrac{1}{2}}^M$$\n",
    "\n",
    "And this result is quite intuitive that in the long run, the positions of each of the $M$ balls are independent and each one is equally likely to be in either urn.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we look back to the equation $x_i P_{ij} = x_j P_{ji}$. Could it turns out that no solution exists?\n",
    "\n",
    "$$\\begin{cases}\n",
    "x_i P_{ij} = x_j P_{ji}\\\\\n",
    "x_k P_{kj} = x_j P_{jk}\\\\\n",
    "x_i P_{ik} = x_k P_{ki}\n",
    "\\end{cases}\\Longrightarrow \\ffrac{x_i}{x_k} = \\ffrac{P_{ji}P_{kj}}{P_{ij}P_{jk}} = \\ffrac{P_{ki}} {P_{ik}}, \\bspace \\text{if }P_{ij}P_{jk}>0$$\n",
    "\n",
    "Thus a necessary condition for time reversibility is that $P_{ik}P_{kj}P_{ji} = P_{ij}P_{jk}P_{ki}$, for all $i,j,k$. And now we summrize this into a Theorem\n",
    "\n",
    "$Theorem.2$\n",
    "\n",
    "An ergodic Markov chain for which $P_{i j} = 0$ whenever $P_{ji} = 0$ is **time reversible** $iff$ starting in state $i$, any path back to $i$ has the same probability as the reversed path. That is, if\n",
    "\n",
    "$$P_{i,i_1}P_{i_1,i_2} \\cdots P_{i_k,i} = P_{i,i_k}P_{i_k,i_{{k-1}}} \\cdots P_{i_1,i} $$\n",
    "\n",
    "for all states $i,i_1,\\dots,i_k$.\n",
    "\n",
    "$Proof$\n",
    "\n",
    ">We have already proven necessity. To prove sufficiency, fix states $i$ and $j$ in the series of states, and rewrite\n",
    ">\n",
    ">$$P_{i,i_1}P_{i_1,i_2} \\cdots P_{i_k,j}P_{j,i} = P_{i,j}P_{j,i_k}P_{i_k,i_{{k-1}}} \\cdots P_{i_1,i}$$\n",
    ">\n",
    ">Then sum both sides over all states $i_1,i_2,\\dots,i_k$ yielding that in $k+1$ transitions,\n",
    ">\n",
    ">$$P_{ij}^{k+1} P_{ji} = P_{ij}P_{ji}^{k+1}$$\n",
    ">\n",
    ">Then letting $k\\to\\infty$ yields $\\pi_j P_{ji} = P_{ij}\\pi_i$. Time reversible!\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e..g.37**\n",
    "\n",
    "Good example though, skipped.\n",
    "***\n",
    "$Proposition.9$ \n",
    "\n",
    "Consider an irreducible Markov chain with transition probabilities $P_{ij}$. If we can find positive numbers $\\pi_i,i\\geq 0$, summing to one, and a transition $\\mathbf{Q} = \\SB{Q_{ij}}$ such that\n",
    "\n",
    "$\\bspace\\pi_iP_{ij} = \\pi_j Q_{ji}$\n",
    "\n",
    "then the $Q_{ij}$ are the transition probabilities of the reversed chain and the $\\pi_i$ are the stationary probabilities both for the original and reversed chain.\n",
    "\n",
    "**e.g.38**\n",
    "\n",
    "When the light bulb in use fails, it is replaced by a new one at the beginning of the next day. Let $X_n$ equal $i$ if the bulb in use at the beginning of day $n$ is in its $i$th day of use (that is, if its present age is\n",
    "$i$). For instance, if a bulb fails on day $n − 1$, then a new bulb will be put in use at the beginning of day $n$ and so $X_n = 1$. If we suppose that each bulb, independently, fails on its $i$th day of use with probability $p_i$, $i \\geq 1$, then it is easy to see that ${ X_n , n \\geq 1}$ is a Markov chain. Let $L$ be the $r.v.$ representing the bulb's life so that $P\\CB{L=i} = p_i$, then the transition probabilities are as follows:\n",
    "\n",
    "$$\\begin{align}\n",
    "P_{i,1} &= P\\CB{\\text{bulb which is on its $i$th day of use fails}}\\\\\n",
    "&= P\\CB{\\text{life of bulb $= i$}\\mid\\text{life of bulb $\\geq i$}}\\\\\n",
    "&= \\ffrac{P\\CB{L = i}}{P\\CB{L\\geq i}}\\\\\n",
    "P_{i,i+1}&=1-P_{i,1}\n",
    "\\end{align}$$\n",
    "\n",
    "Suppose now that this chain has been in operation for a long (in theory, an infinite) time and consider the sequence of states going backward in time. The reverse chain will always decrease by $1$ until it reaches $1$\n",
    "and then it will jump to a random value representing the lifetime of the (in real time) previous bulb. Then the transition probabilities for the reverse chain:\n",
    "\n",
    "$\\bspace\\begin{align}\n",
    "Q_{i,i-1} &= 1,&i>1\\\\\n",
    "Q_{i,1} &=p_i,&i\\geq1\n",
    "\\end{align}$\n",
    "\n",
    "To check this and meantime, find the stationary probabilities, let's see whether positive $\\pi_i$ exist, such that\n",
    "\n",
    "$\\bspace \\pi_i P_{i,j} = \\pi_j Q_{j,i}$\n",
    "\n",
    "First let $j=1$ and there we have $\\pi_i\\ffrac{P\\CB{L=i}}{P\\CB{L\\geq i}} = \\pi_1 P\\CB{L=i}$ or equivalently $\\pi_i = \\pi_1 P\\CB{L \\geq i}$. Summing over all $i$ yields \n",
    "\n",
    "$\\bspace 1=\\d{\\sum_{i=1}^{\\infty} \\pi_i = \\pi_1 \\sum_{i=1}^{\\infty} P\\CB{L\\geq i}} = \\pi_1\\Exp\\SB{L} \\Longrightarrow \\pi_i = \\ffrac{P\\CB{L\\geq i}}{\\Exp\\SB{L}}, \\bspace i\\geq 1$\n",
    "\n",
    "After this, we need to check $\\pi_i P_{i,i+1} = \\pi_{i+1} Q_{i+1,i}$, which is equivalent to \n",
    "\n",
    "$\\bspace\\ffrac{P\\CB{L\\geq i}}{\\Exp\\SB{L}} \\P{1-\\ffrac{P\\CB{L = i}}{P\\CB{L\\geq i}}} = \\ffrac{P\\CB{L \\geq i+1}}{\\Exp\\SB{L}}\\cdot1$\n",
    "\n",
    "And this equation holds since $P\\CB{L \\geq i} - P\\CB{L = i} = P\\CB{L\\geq i+1}$. Done!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "262px",
    "width": "320px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
