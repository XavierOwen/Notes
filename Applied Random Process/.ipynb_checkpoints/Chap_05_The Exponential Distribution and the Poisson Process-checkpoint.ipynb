{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Exponential Distribution and the Poisson Process\n",
    "## Introduction\n",
    "\n",
    "One simplifying assumption that is often made is to assume that certain $r.v.\\DeclareMathOperator*{\\argmin}{argmin}\n",
    "\\DeclareMathOperator*{\\argmax}{argmax}\n",
    "\\DeclareMathOperator*{\\plim}{plim}\n",
    "\\newcommand{\\using}[1]{\\stackrel{\\mathrm{#1}}{=}}\n",
    "\\newcommand{\\ffrac}{\\displaystyle \\frac}\n",
    "\\newcommand{\\asim}{\\overset{\\text{a}}{\\sim}}\n",
    "\\newcommand{\\space}{\\text{ }}\n",
    "\\newcommand{\\bspace}{\\;\\;\\;\\;}\n",
    "\\newcommand{\\QQQ}{\\boxed{?\\:}}\n",
    "\\newcommand{\\void}{\\left.\\right.}\n",
    "\\newcommand{\\Tran}[1]{{#1}^{\\mathrm{T}}}\n",
    "\\newcommand{\\d}[1]{\\displaystyle{#1}}\n",
    "\\newcommand{\\CB}[1]{\\left\\{ #1 \\right\\}}\n",
    "\\newcommand{\\SB}[1]{\\left[ #1 \\right]}\n",
    "\\newcommand{\\P}[1]{\\left( #1 \\right)}\n",
    "\\newcommand{\\abs}[1]{\\left| #1 \\right|}\n",
    "\\newcommand{\\norm}[1]{\\left\\| #1 \\right\\|}\n",
    "\\newcommand{\\dd}{\\mathrm{d}}\n",
    "\\newcommand{\\Exp}{\\mathrm{E}}\n",
    "\\newcommand{\\RR}{\\mathbb{R}}\n",
    "\\newcommand{\\EE}{\\mathbb{E}}\n",
    "\\newcommand{\\II}{\\mathbb{I}}\n",
    "\\newcommand{\\NN}{\\mathbb{N}}\n",
    "\\newcommand{\\ZZ}{\\mathbb{Z}}\n",
    "\\newcommand{\\QQ}{\\mathbb{Q}}\n",
    "\\newcommand{\\PP}{\\mathbb{P}}\n",
    "\\newcommand{\\AcA}{\\mathcal{A}}\n",
    "\\newcommand{\\FcF}{\\mathcal{F}}\n",
    "\\newcommand{\\AsA}{\\mathscr{A}}\n",
    "\\newcommand{\\FsF}{\\mathscr{F}}\n",
    "\\newcommand{\\Var}[2][\\,\\!]{\\mathrm{Var}_{#1}\\left[#2\\right]}\n",
    "\\newcommand{\\Avar}[2][\\,\\!]{\\mathrm{Avar}_{#1}\\left[#2\\right]}\n",
    "\\newcommand{\\Cov}[2][\\,\\!]{\\mathrm{Cov}_{#1}\\left(#2\\right)}\n",
    "\\newcommand{\\Corr}[2][\\,\\!]{\\mathrm{Corr}_{#1}\\left(#2\\right)}\n",
    "\\newcommand{\\I}[1]{\\mathrm{I}\\left( #1 \\right)}\n",
    "\\newcommand{\\N}[1]{\\mathcal{N} \\left( #1 \\right)}\n",
    "\\newcommand{\\ow}{\\text{otherwise}}$ are exponentially distributed.\n",
    "\n",
    "- easy to work with\n",
    "- good approximation to the actual distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Exponential Distribution\n",
    "### Definition\n",
    "\n",
    "A continuous $r.v.$ $X$ is said to have an exponential distribution with parameter $\\lambda, \\lambda>0$, if its probability density function is given by\n",
    "\n",
    "$\\bspace f\\P{x} = \\begin{cases}\n",
    "\\lambda e^{-\\lambda x}, &\\text{if } x \\geq 0\\\\\n",
    "0 , &\\text{if }x<0\n",
    "\\end{cases}$\n",
    "\n",
    "or *equivalently*, if its cumulative density function is given by\n",
    "\n",
    "$\\bspace F\\P{x} = \\d{\\int_{-\\infty}^{x} f\\P{s}\\;\\dd s} = \\begin{cases}\n",
    "1-e^{-\\lambda x}, &\\text{if } x \\geq 0\\\\\n",
    "0 , &\\text{if }x<0\n",
    "\\end{cases}$\n",
    "\n",
    "Its mean, $\\Exp\\SB{X}$ is given by\n",
    "\n",
    "$\\bspace\\begin{align}\n",
    "\\Exp\\SB{X} &= \\int_{-\\infty}^{\\infty} xf\\P{x}\\;\\dd x\\\\\n",
    "&= \\int_{0}^{\\infty} x\\cdot \\lambda e^{-\\lambda x}\\;\\dd x\\\\\n",
    "&= \\left.\\P{\\ffrac{1}{\\lambda} - x}e^{-\\lambda x}\\right|_{0}^\\infty = \\ffrac{1}{\\lambda}\n",
    "\\end{align}$\n",
    "\n",
    "Its moment generating function $\\phi\\P{t}$ is given by\n",
    "\n",
    "$\\bspace\\begin{align}\n",
    "\\phi\\P{t} &= \\Exp\\SB{e^{tX}}\\\\\n",
    "&= \\int_{0}^{\\infty} e^{tx}\\cdot \\lambda e^{-\\lambda x}\\;\\dd x\\\\\n",
    "&= \\left. \\ffrac{\\lambda}{t-\\lambda}e^{\\P{t-\\lambda} x} \\right|_{0}^\\infty = \\ffrac{-\\lambda}{t-\\lambda}, \\bspace t<\\lambda\n",
    "\\end{align}$\n",
    "\n",
    "Other useful moments and variance, from $\\text{mgf}$, we have $\\Exp\\SB{X^2} = 2/\\lambda^2$ and $\\Var{X} = 1/\\lambda^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.1** Exponential Random Variables and Expected Discounted Returns\n",
    "\n",
    "Here's the short version. $R\\P{x}$ is the random rate at which you are receiving rewards at time $x$. For the discounted rate $\\alpha$ we have the total discounted reward\n",
    "\n",
    "$\\bspace R = \\d{\\int_0^\\infty e^{-\\alpha x} R\\P{x}\\;\\dd x} \\Rightarrow \\Exp\\SB{R} = \\int_0^\\infty e^{-\\alpha x}\\Exp\\SB{R\\P{x}}\\;\\dd x$\n",
    "\n",
    "If we let $T$ be an exponential random variable with parameter $\\alpha$, which is independent of all other $r.v.$. Prove $\\Exp\\SB{R} = \\Exp\\SB{\\d \\int_0^T R\\P{x}\\;\\dd x}$\n",
    "\n",
    "> First define, for each $x \\geq 0$, a $r.v.$ $I\\P{x}$ by\n",
    ">\n",
    ">$\\bspace I\\P{x} = \\begin{cases}\n",
    "1,&\\text{if } x\\leq T\\\\\n",
    "0,&\\text{if } x > T\n",
    "\\end{cases} \\Rightarrow \\d \\int_0^T R\\P{x}\\;\\dd x = \\int_{0}^\\infty R\\P{x}I\\P{x} \\;\\dd x$\n",
    ">\n",
    ">Thus,\n",
    ">\n",
    ">$\\bspace\\begin{align}\n",
    "\\Exp\\SB{\\d \\int_0^T R\\P{x}\\;\\dd x} &= \\Exp\\SB{\\d \\int_0^\\infty R\\P{x}I\\P{x}\\;\\dd x}\\\\\n",
    "&= \\d \\int_0^\\infty \\Exp\\SB{R\\P{x}}\\Exp\\SB{I\\P{x}}\\;\\dd x\\\\\n",
    "&= \\int_0^\\infty \\Exp\\SB{R\\P{x}}P\\CB{T\\geq x}\\;\\dd x\\\\\n",
    "&= \\int_0^\\infty \\Exp\\SB{R\\P{x}}\\cdot e^{-\\alpha x}\\;\\dd x = \\Exp\\SB{R}\n",
    "\\end{align}$\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties of the Exponential Distribution\n",
    "\n",
    "A $r.v.$ $X$ is said to be *without memory*, or ***memoryless***, if \n",
    "\n",
    "$\\bspace P\\CB{X>s+t \\mid X>t} = P\\CB{X>s} \\iff \\ffrac{P\\CB{X > s+t,X>t}}{P\\CB{X>t}} = P\\CB{X>s},\\bspace \\forall s,t \\geq 0$\n",
    "\n",
    "Consider the expondentially distributed $X$ which is certainly satisfy this condition since\n",
    "\n",
    "$\\bspace e^{-\\lambda t} \\cdot e^{-\\lambda s} = e^{-\\lambda\\P{s+t}}$\n",
    "\n",
    "To illustrate this see the following examples.\n",
    "\n",
    "**e.g.2** \n",
    "\n",
    "Suppose that the amount of time one spends in a bank is exponentially distributed with mean ten minutes, that is $\\lambda = 0.1$. What is the probability that a customer will spend more than fifteen minutes in the bank given that she is still in the bank after ten minutes?\n",
    "\n",
    ">Let $X$ represents the amount of time that the customer spends in the bank.\n",
    ">\n",
    ">$P\\CB{X > 15 \\mid X>10} = P\\CB{X>5} = e^{-5\\lambda} \\approx 0.604$\n",
    "\n",
    "***\n",
    "\n",
    "**e.g.4** \n",
    "\n",
    "The dollar amount of damage of an accident is exponentially distributed with mean $1000$. However the insurance company only pays that amount exceeding $400$. Find the mean and the variance of the amount the insurance company pays per accident.\n",
    "\n",
    ">Let $X$ denote the dollar amount of damage resulting from an accident, then payment from insurance company is $\\P{X-400}^+$, or define a indicator $r.v.$ $I = \\begin{cases}\n",
    "1,&\\text{if } X>400\\\\\n",
    "0,&\\text{if } X \\leq 400\n",
    "\\end{cases}$\n",
    "\n",
    ">Then letting $Y = \\P{X-400}^+$, we have $\\Exp\\SB{Y\\mid I} = 1000\\cdot I$, $\\Var{Y\\mid I} = 10^6\\cdot I$. Then\n",
    ">\n",
    ">$\\bspace \\Exp\\SB{Y} = \\Exp\\SB{\\Exp\\SB{Y\\mid I}} = 10^3\\Exp\\SB{I} = 10^3\\cdot P\\CB{X>400} = 10^3\\cdot \\exp\\CB{-\\ffrac{1}{1000}\\cdot 400} \\approx 670.32$\n",
    ">\n",
    ">$\\bspace \\Var{Y} = \\Exp\\SB{\\Var{Y\\mid I}} + \\Var{\\Exp\\SB{Y \\mid I}} = 10^6 \\cdot e^{-0.4} + 10^6\\cdot e^{-0.4}\\P{1-e^{-0.4}}$\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that exponential distribution is the only memoryless one. To see this, suppose $X$ is memoryless and let $\\bar F\\P{X} = P\\CB{X>x} = 1 - F\\P{X}$. Then by the definition we have $\\bar F\\P{s+t} = \\bar F\\P{s} \\bar F\\P{t}$, meaning that $\\bar F$ satisfies the functional equation $g\\P{s+t} = g\\P{s}g\\P{t}$. However, it turns out that the only *right continuous* solution of this functional equation is $g\\P{x} = e^{-\\lambda x}$.\n",
    "\n",
    "> Given the equation we have $g\\P{m/n} = g^m\\P{1/n}$. Especially, we have $g\\P{1} = g^n\\P{1/n}$. Hence\n",
    ">\n",
    ">$\\bspace g\\P{m/n} = g^{m/n}\\P{1}$\n",
    ">\n",
    ">which implies, since $g$ is *right continuous*, that $g\\P{x} = \\P{g\\P{1}}^x$. Since $g\\P{1} = g^2\\P{0.5}\\geq0$, we obtain $g\\P{x} = e^{-\\lambda x}$ where $\\lambda = -\\log \\P{g\\P{1}}$.\n",
    "\n",
    "So we must have $\\bar F\\P{x} = e^{-\\lambda x}$, and consequently, $F\\P{x} = 1-e^{-\\lambda x}$, which shows that $X$ is exponentially distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define the ***failure rate function***: $r\\P{t} = \\ffrac{f\\P{t}}{1-F\\P{t}}$. And to interpret this, suppose that an item with lifetime $X$, has survived for $t$ hours, then what's the probability that it doesn't survive for an additional time $\\dd t$?\n",
    "\n",
    "$\\bspace \\begin{align}\n",
    "P\\CB{X \\in\\P{t,t+\\dd t}\\mid X>t} &= \\ffrac{P\\CB{X \\in\\P{t,t+\\dd t},X>t}}{P\\CB{X>t}}\\\\\n",
    "&= \\ffrac{P\\CB{X \\in\\P{t,t+\\dd t}}}{P\\CB{X>t}}\\approx \\ffrac{f\\P{t}\\dd t}{1-F\\P{t}} = r\\P{t}\\dd t\n",
    "\\end{align}$\n",
    "\n",
    "$\\bspace r\\P{t}$ *represents the conditional probability density that a* $t$*-year-old item will fail.*\n",
    "\n",
    "And by the memoryless property, for the exponential distribution, we have \n",
    "\n",
    "$\\bspace r\\P{t} = \\ffrac{f\\P{t}}{1-F\\P{t}} = \\ffrac{\\lambda e^{-\\lambda t}}{e^{-\\lambda t}} = \\lambda$\n",
    "\n",
    "Here $\\lambda$ is often referred to as the ***rate*** of the distribution (also as the *reciprocal倒数* of the mean $1/\\lambda$). Besides, this **rate function** uniquely determine the cdf $F$:\n",
    "\n",
    "$\\bspace r\\P{t} = \\ffrac{\\ffrac{\\dd}{\\dd t}F\\P{t}}{1-F\\P{t}}\\Rightarrow \\log\\P{1-F\\P{t}}= -\\d{\\int_0^t r\\P t\\;\\dd t + k} \\Rightarrow 1-F\\P t = e^k \\exp\\CB{-\\d \\int_0^t r\\P t\\;\\dd t}$\n",
    "\n",
    "And $k$ can be determined by letting $t=0$, so that the final solution is\n",
    "\n",
    "$\\bspace F\\P t = 1-\\exp\\CB{-\\d\\int_0^t r\\P t\\;\\dd t}$\n",
    "\n",
    "More than that, by letting $r\\P t = \\lambda$, we have $F\\P t = 1 - e^{-\\lambda t}$ showing that this $r.v.$ is exponential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.6** Hyperexponential $r.v.$\n",
    "\n",
    "Let $X_1,\\dots,X_n$ be *independent* exponential random variables with respective rates $\\lambda_1,\\dots,\\lambda_n$, where $\\lambda_i \\neq \\lambda_j$ when $i\\neq j$. Then let $T$ be independent of all of them and suppose that \n",
    "\n",
    "$\\bspace \\d\\sum_{j=1}^n P_j = 1$, where $P_j = P\\CB{T = j}$\n",
    "\n",
    "Then $X_T$ is said to be a ***hyperexponential*** $r.v.$. Then what's its distribution? Surely we condition on $T$ and this yields\n",
    "\n",
    "$\\bspace \\begin{align}\n",
    "1-F\\P t &= P\\CB{X>t}\\\\\n",
    "&= \\sum_{i=1}^{n} P\\CB{X>t \\mid T = i}\\cdot P\\CB{T = i}\\\\\n",
    "&= \\sum_{i=1}^{n} P_i \\cdot e^{-\\lambda_i t}\n",
    "\\end{align}$\n",
    "\n",
    "Then we differentiate the equation on $t$ leading to the pdf and failure function\n",
    "\n",
    "$\\bspace f\\P t = \\d\\sum_{i=1}^n \\lambda_i P_i e^{-\\lambda_i t},\\bspace r\\P t = \\ffrac{\\d{\\sum_{i=1}^n \\lambda_i P_i e^{-\\lambda_i t}}}{\\d{\\sum_{i=1}^n P_i e^{-\\lambda_i t}}}$\n",
    "\n",
    "Also note that \n",
    "\n",
    "$\\bspace\\begin{align}\n",
    "P\\CB{T = j \\mid X>t}&= \\ffrac{P\\CB{X>t \\mid T=j}\\cdot P\\CB{T=j}}{P\\CB{X>t}}\\\\\n",
    "&= \\ffrac{P_j e^{-\\lambda_j t}}{{\\sum_{i=1}^n P_i e^{-\\lambda_i t}}}\\\\\n",
    "\\Longrightarrow r\\P{t} &= \\d{\\sum_{j=1}^n\\lambda_j} P\\CB{T=j\\mid X>t}\n",
    "\\end{align}$\n",
    "\n",
    "And one last conclusion, that if $\\lambda_1<\\lambda_i, \\forall\\; i>1$, then\n",
    "\n",
    "$\\bspace\\begin{align}\n",
    "P\\CB{T=1\\mid X>t}&= \\ffrac{P_1 e^{-\\lambda_1 t}}{P_1 e^{-\\lambda_1 t} + \\sum_{i=1}^n P_i e^{-\\lambda_i t}}\\\\\n",
    "&= \\ffrac{P_1}{P_1 + \\sum_{i=1}^n P_i e^{-\\P{\\lambda_i -\\lambda_1} t}} \\to 1,\\bspace t\\to \\infty\n",
    "\\end{align}$\n",
    "\n",
    "So that actually $\\d{\\lim_{t\\to\\infty} r\\P t = \\min_i \\lambda_i}$\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Properties of the Exponential Distribution\n",
    "\n",
    "Let $X_1,\\dots,X_n$ be *independent* exponential random variables with common mean $1/\\lambda$. Following the discussion in Chap_02, we know that the distribution of their sum has a *gamma* distribution with parameters $n$ and $\\lambda$. Here's another proof by using *mathematical induction*.\n",
    "\n",
    "The conclusion holds true at $n=1$, so now assume that $X_1 + X_2+\\cdots+X_{n-1}$ has density given by\n",
    "\n",
    "$\\bspace\\d{f_{X_1+ X_2+\\cdots+X_{n-1}}\\P t = \\lambda e^{-\\lambda t}\\ffrac{ \\P{\\lambda t}^{n-2}}{\\P{n-2}!}}$\n",
    "\n",
    "Hence, the $n$ sum is, by the independency,\n",
    "\n",
    "$\\bspace\\begin{align}\n",
    "f_{X_1+ X_2+\\cdots+X_{n-1} \\:+ X_n}\\P t &= \\int_0^\\infty f_{X_n}\\P{t-s} f_{X_1+ X_2+\\cdots+X_{n-1}}\\P s \\;\\dd s\\\\\n",
    "&= \\int_0^t \\lambda e^{-\\lambda\\P{t-s}} \\lambda e^{-\\lambda s} \\ffrac{\\P{\\lambda s}^{n-2}}{\\P{n-2}!} \\;\\dd s = \\lambda e^{-\\lambda t} \\ffrac{\\P{\\lambda t}^{n-1}}{\\P{n-1}!}\n",
    "\\end{align}$\n",
    "\n",
    "***\n",
    "\n",
    "Another useful thing, that is to determine the probability that one exponenetial $r.v.$ is smaller than another. Suppose that $X_1$ and $X_2$ are independent exponential $r.v.$ with respective rates $\\lambda_1$ and $\\lambda_2$, then what's $P\\CB{X_1<X_2}$? Still, we condition on $X_1$ to find the result.\n",
    "\n",
    "$\\bspace\\begin{align}\n",
    "P\\CB{X_1<X_2}&= \\int_0^\\infty P\\CB{X_1<X_2\\mid X_1 = x}\\cdot P\\CB{X_1 = x}\\;\\dd x\\\\\n",
    "&= \\int_0^\\infty P\\CB{x<X_2}\\cdot \\lambda_1 e^{-\\lambda_{\\void_1} x}\\;\\dd x\\\\\n",
    "&= \\int_0^\\infty e^{-\\lambda_{\\void_2} x}\\cdot \\lambda_1 e^{-\\lambda_{\\void_1} x}\\;\\dd x\\\\\n",
    "&= \\ffrac{\\lambda_1}{\\lambda_1 + \\lambda_2}\n",
    "\\end{align}$\n",
    "\n",
    "More than this, we have the minimum of them is a exponential with a rate equal to the sum of all individual rates.\n",
    "\n",
    "$\\bspace\\begin{align}\n",
    "P\\CB{\\min_i\\P{X_i}>x} &= P\\CB{X_1>x,X_2>x,\\dots,X_n>x}\\\\\n",
    "&= \\prod_{i=1}^n P\\CB{X_i>x} = \\prod_{i=1}^n e^{-\\lambda_{\\void_i}x} = \\exp\\CB{-\\P{\\sum_i \\lambda_i} x}\n",
    "\\end{align}$\n",
    "\n",
    "**e.g.7** Analyzing Greedy Algorithms for the Assignment Problem\n",
    "\n",
    "Attractive! Skipped, though\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the two preceding results, we have\n",
    "\n",
    "$\\bspace \\d{P\\CB{X_i = \\min_j X_j} = P\\CB{X_i <\\min_{j\\neq i}X_j} = \\ffrac{\\lambda_i}{\\lambda_i + \\sum_{j\\neq i} \\lambda_j} = \\ffrac{\\lambda_i}{\\sum_j \\lambda_j}}$\n",
    "\n",
    "$Proposition$\n",
    "\n",
    "If $X_1,\\dots,X_n$ are independent exponential $r.v.$ with respective rates $\\lambda_1,\\dots,\\lambda_n$, then $\\d{\\min_i X_i}$ is exponential with rate $\\d{\\sum_{i=1}^{n}\\lambda_i}$. Further, $\\d{\\min_i X_i}$ and the rank order of the variables $X_1,\\dots,X_n$ are independent.\n",
    "\n",
    ">The only thing we need to prove here is the last conclusion, the independency. And that can be easily seen using the property that it's **memoryless**\n",
    ">\n",
    ">$\\bspace \\d{P\\CB{X_{i_1}<\\cdots X_{i_n}\\mid \\min_i X_i >t} = P\\CB{X_{i_1}-t<\\cdots X_{i_n}-t\\mid \\min_i X_i >t} = P\\CB{X_{i_1}<\\cdots X_{i_n}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.9** and **e.g.10** are also very interesting, and comprehensive. The strategies basically, are taking conditional probability, using recursions, or breaking one $r.v.$ to the sum of $k$ $r.v.$s.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutions of Exponential $r.v.$ (skipped)\n",
    "\n",
    "The sum of $n$ independent exponential $r.v.$ with different respective rates $\\lambda_i$, is said to be a ***hypoexponential*** $r.v.$. To calculate its pdf, let's start with case $n=2$. Now,\n",
    "\n",
    "$\\bspace\\begin{align}\n",
    "f_{X_1+X_2}\\P t &= \\int_0^t f_{X_1} \\P s f_{X_2} \\P{t-s}\\;\\dd s\\\\\n",
    "&= \\int_0^t \\lambda_1 e^{-\\lambda_{\\void_1} s} \\lambda_2 e^{-\\lambda_{\\void_2} \\P{t-s}}\\;\\dd s\\\\\n",
    "&= \\lambda_1\\lambda_2e^{-\\lambda_{\\void_2} t} \\int_0^t e^{-\\P{\\lambda_{\\void_1}-\\lambda_{\\void_2}}s}\\;\\dd s\\\\\n",
    "&= \\ffrac{\\lambda_1\\lambda_2e^{-\\lambda_{\\void_2} t}}{\\lambda_1-\\lambda_2} \\P{1-e^{-\\P{\\lambda_{\\void_1}-\\lambda_{\\void_2}}t}} = \\ffrac{\\lambda_1}{\\lambda_1-\\lambda_2}\\lambda_2 e^{-\\lambda_{\\void_2}t} + \\ffrac{\\lambda_2}{\\lambda_2-\\lambda_1}\\lambda_1 e^{-\\lambda_{\\void_1}t}\n",
    "\\end{align}$\n",
    "\n",
    "And similarly, we have when $n=3$,\n",
    "\n",
    "$\\bspace\\d{f_{X_1+X_2+X_3}\\P t = \\sum_{i=1}^3 \\P{\\lambda_i e^{-\\lambda_{\\void_i} t} \\prod_{j\\neq i} \\ffrac{\\lambda_j}{\\lambda_j-\\lambda_i} }}$\n",
    "\n",
    "which suggests the general result, letting $S = X_1 + \\cdots+X_n$,\n",
    "\n",
    "$\\bspace \\d{f_{S}\\P t = \\sum_{i=1}^n \\P{\\lambda_i e^{-\\lambda_{\\void_i}t} \\prod_{j\\neq i} \\ffrac{\\lambda_j}{\\lambda_j - \\lambda_i}}}$\n",
    "\n",
    "This can be proved by induction on $n$, still very complex through. Skipped. Alright, now we integrate both sides of the expression for $f_{S}$ from $t$ to $\\infty$. The ***tail distribution function*** of $S$ is given by\n",
    "\n",
    "$\\d{\\bspace P\\CB{S>t} = \\sum_{i=1}^n C_{i,n} e^{-\\lambda_{\\void_i}t},\\bspace C_{i,n} = \\prod_{j\\neq i} \\ffrac{\\lambda_j}{\\lambda_j - \\lambda_i}}$\n",
    "\n",
    "And the following **failure rate function** of $S$, is given by\n",
    "\n",
    "$\\bspace r_S\\P t = \\ffrac{\\sum_{i=1}^n C_{i,n}\\lambda_i e^{-\\lambda_{\\void_i}t}}{\\sum_{i=1}^n C_{i,n} e^{-\\lambda_{\\void_i}t}}$\n",
    "\n",
    "In addition, if we let $\\lambda_j = \\d{\\min_j \\lambda_j}$, then it follows that $\\d{\\lim_{t\\to\\infty}r_S\\P t = \\lambda_j}$\n",
    "\n",
    "$Remark$\n",
    "\n",
    ">Although\n",
    ">\n",
    ">$\\bspace \\d{1 = \\int_0^\\infty f_S\\P t\\;\\dd t = \\sum_{i=1}^n C_{i,n} = \\sum_{i=1}^n \\prod_{i\\neq j} \\ffrac{\\lambda_j}{\\lambda_j - \\lambda_i}}$\n",
    ">\n",
    ">$C_{i,n}$ are NOT probabilities. Some of them may be negative. Also, this is far from same with **hyperexponential** density.\n",
    "\n",
    "***\n",
    "\n",
    "**e.g.11** Coxian $r.v.$\n",
    "\n",
    "Whatever, skipped\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Poisson Process\n",
    "### Counting Processes\n",
    "\n",
    "A stochastic process $\\CB{N\\P t , t\\geq 0}$ is said to be a ***counting process*** if $N\\P t$ represents the total number of \"events\" that occur by time $t$. And we can see this process must satisfy\n",
    "\n",
    "1. $N\\P{t}\\geq 0$\n",
    "2. $N\\P t$ is integer valued\n",
    "3. If $s<t$, then $N\\P s \\leq N\\P t$\n",
    "4. For $s<t$, $N\\P t - N\\P s$ equals the number of events that occur in $(s,t]$\n",
    "\n",
    "A counting process is said to possess ***independent increments*** if the numbers of events that occur in disjoint time intervals are independent, like $N\\P{10}$ and $\\P{N\\P{15} - N\\P{10}}$. And a counting process is said to possess ***stationary increments*** if the distribution of the number of events that occur in any interval of time depends only on the *length* of the time interval, say for all $s$ the number of events in the interval $\\P{s,s+t}$ has the same distribution.\n",
    "\n",
    "### Definition of the Poisson Process\n",
    "\n",
    "$Def.1$ $o\\P h$\n",
    "\n",
    "The function $f\\P \\cdot$ is said to be $o\\P h$ if $\\d{\\lim_{h\\to\\infty} \\ffrac{f\\P h}{h} = 0}$\n",
    "\n",
    "And with this we could now write\n",
    "\n",
    "$\\bspace\\begin{align}\n",
    "P\\P{t < X <t+h} \\approx f\\P t \\cdot h \\Longrightarrow f\\P t \\cdot h + o\\P h\\\\\n",
    "P\\P{t < X <t+h\\mid X>t} \\approx r\\P t \\cdot h \\Longrightarrow r\\P t \\cdot h + o\\P h\n",
    "\\end{align}$\n",
    "\n",
    "$Def.2$\n",
    "\n",
    "The counting process $\\CB{N\\P t, t\\geq 0}$ is said to be a ***Poisson process*** with rate $\\lambda >0$ if the following *axioms* hold:\n",
    "\n",
    "1. $N\\P 0 = 0$\n",
    "2. $\\CB{N\\P t, t\\geq 0}$ has independent increments\n",
    "3. $P\\CB{N\\P{t+h} - N\\P t = 1} = \\lambda \\cdot h + o\\P h$\n",
    "4. $P\\CB{N\\P{t+h} - N\\P t \\geq 2} = o\\P h$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Theorem.1$\n",
    "\n",
    "If $\\CB{N\\P t, t\\geq 0}$ is a **Poisson process** with rate $\\lambda>0$, then for all $s>0$, $t>0$, $N\\P{s+t} - N\\P{s}$ is a Poisson $r.v.$ with mean $\\lambda t$. That is to say, that the number of events in *any* interval of length $t$ is a Poisson $r.v.$ with mean $\\lambda t$.\n",
    "\n",
    "$Proof$\n",
    "\n",
    ">We begin by deriving $\\Exp\\SB{e^{-uN\\P t}}$, the ***Laplace transform*** of $N\\P t$. First fix $u>0$ and define $g\\P t = \\Exp\\SB{e^{-uN\\P t}}$. Then a differential equation\n",
    ">\n",
    ">$\\bspace\\begin{align}\n",
    "g\\P{t+h} &= \\Exp\\SB{e^{-uN\\P{t+h}}}\\\\\n",
    "&= \\Exp\\SB{e^{-u\\P{N\\P{t+h} +N\\P t - N\\P t }}}\\\\\n",
    "&= \\Exp\\SB{e^{-uN\\P{t}} \\cdot e^{-u\\P{N\\P{t+h} - N\\P t}}}\\\\\n",
    "&= \\Exp\\SB{e^{-uN\\P{t}}} \\cdot \\Exp\\SB{e^{-u\\P{N\\P{t+h} - N\\P t}}} = g\\P t \\cdot \\Exp\\SB{e^{-u\\P{N\\P{t+h} - N\\P t}}}\n",
    "\\end{align}$\n",
    ">\n",
    ">Then from the last two axioms, we have \n",
    ">\n",
    ">$\\bspace\\begin{align}P\\CB{N\\P{t+h} - N\\P t = 0} &= 1 -P\\CB{N\\P{t+h} - N\\P t = 1} -P\\CB{N\\P{t+h} - N\\P t \\geq 2}\\\\\n",
    "&= 1 - \\lambda\\cdot h + o\\P h\\\\\n",
    "\\Longrightarrow \\Exp\\SB{e^{-u\\P{N\\P{t+h} - N\\P t}}} &= 1-\\lambda \\cdot h + o\\P h + e^{-u}\\P{\\lambda \\cdot h + o\\P h} + o\\P h\\\\\n",
    "&= 1-\\lambda \\cdot h + \\lambda e^{-u} \\cdot h + o\\P h\n",
    "\\end{align}$\n",
    "\n",
    ">Therefore, $g\\P{t+h} = g\\P t \\big(1+\\lambda\\P{e^{-u}-1}h + o\\P h\\big)$. Then solve this by first rewrite\n",
    ">\n",
    ">$\\bspace \\ffrac{g\\P{t+h} - g\\P t}{h} = g\\P t \\lambda \\P{e^{-u}-1} + \\ffrac{o\\P h}{h}$\n",
    ">\n",
    ">Letting $h\\to0$ yields the differential equation\n",
    ">\n",
    ">$\\bspace g'\\P t = g\\P t \\lambda \\P{e^{-u} - 1 } \\Longrightarrow \\log\\big(g\\P t\\big) = \\lambda \\P{e^{-u} - 1}t + C$\n",
    ">\n",
    ">Noting that $g\\P 0 = \\Exp\\SB{e^{-uN\\P 0}} = \\Exp\\SB{e^0} = 1$, it follows that $C=0$, and so the **Laplace transform** of $N\\P t$ is\n",
    ">\n",
    ">$\\bspace\\Exp\\SB{e^{-uN\\P t}} = g\\P t = e^{\\lambda \\P{e^{-u} \\;-1}t}$\n",
    ">\n",
    ">Comparing this to the **Laplace transform** of a typical Poisson $r.v.$ $X$, with mean $\\lambda t$,\n",
    ">\n",
    ">$\\bspace\\Exp\\SB{e^{-uX}} = \\d{\\sum_i e^{-u i} \\cdot\\ffrac{e^{-\\lambda t} \\P{\\lambda t}^i}{i!} = e^{-\\lambda t}\\sum_i \\ffrac{\\P{\\lambda t e^{-u}}^i}{i!} } = e^{-\\lambda t} \\cdot e^{\\lambda t e^{-u}} = e^{\\lambda t \\P{e^{-u}\\; -1}}$\n",
    ">\n",
    ">Like $\\text{mgf}$, **Laplace transform** uniquely determines the distribution, we can thus conclude that $N\\P t$ is **Poisson** $r.v.$ with mean $\\lambda t$.\n",
    ">\n",
    ">Then, to show that $N_s\\P{t} =N\\P{t+s} - N\\P s$ is also Poisson with mean $\\lambda t$, fix $s$ and then $N_s\\P t$ is the number of events in the first $t$ time units when we start our count at time $s$. Then we can verify that $\\CB{N_s\\P t, t\\geq 0}$ satisfies all the axioms for being a Poisson process with rate $\\lambda$. Consequently, using the preceding result, we can conclude that $N_s\\P t$ is a Poisson $r.v.$ with mean $\\lambda t$.\n",
    "\n",
    "$Remark$\n",
    "\n",
    ">Since the distribution of $N\\P{t+s} - N\\P t$ is the same for all $s$, it follows that the **Poisson process** has **stationary increments**.\n",
    ">\n",
    ">Also, this conclusion is a consequnce of the *Poisson approximation to the **binomial** distribution*. To see this we can subdivide the interval $\\SB{0,k}$ into $k$ equal parts. Using $Axiom.4$, as $k$ increases to $\\infty$, the probability of having two or more events in any of the $k$ subintervals goes to $0$. Hence, $N\\P t$ will equal to the number of subintervals in which an event occurs.\n",
    ">\n",
    ">Then by $Axiom.2$, about the independent increments, and the preceding **stationary increments** conclusion, we know that this number will have a binomial distribution with parameters $k$ and $p = \\lambda t/k +o\\P{t/k}$.\n",
    ">\n",
    ">Hence, by the Poisson approximation to the binomial, we see that when $k$ goes to $\\infty$, $N\\P t$ will have a **Poisson distribution** with mean:\n",
    ">\n",
    ">$\\bspace\\d{\\lim_{t\\to\\infty} k\\cdot\\SB{\\ffrac{\\lambda t}{k} + o\\P{\\ffrac{t}{k}}} =\\lambda t + \\lim_{t\\to\\infty} \\ffrac{t\\cdot o\\P{t/k}}{t/k} = \\lambda t }$\n",
    ">\n",
    ">The last step can be seen by: $k\\to \\infty\\Rightarrow t/k \\to 0 \\Rightarrow \\ffrac{o\\P{t/k}}{t/k}\\to 0$\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interarrival and Waiting Time Distributions\n",
    "\n",
    "Consider a Poisson process, and let us denote the time when the first event happened by $T_1$. Further, for $n>1$, let $T_n$ denote the *elapsed time* between the $\\P{n-1}$st and the $n$th event. Then the sequence $\\CB{T_n, n=1,2,\\dots}$ is called the ***sequence of interarrival times***. What's the distribution of $T_n$?\n",
    "\n",
    "First note that $P\\CB{T_1>t} = P\\CB{N\\P t = 0} = e^{-\\lambda t}$. Hence, $T_1$ has an **exponential** distribution with mean $1/\\lambda$. Now\n",
    "\n",
    "$\\bspace\n",
    "P\\CB{T_2 > t} = \\Exp\\big[P\\CB{T_2>t\\mid T_1}\\big] = \\d\\int_0^\\infty P\\CB{T_2>t\\mid T_1 = s} \\cdot P\\CB{T_1 = s} \\;\\dd s$\n",
    "\n",
    "However, $P\\CB{T_2>t\\mid T_1 = s} = P\\CB{0\\text{ events in }(s,s+t]\\mid T_1 = s}$ and because of the independent and stationary increments, we have\n",
    "\n",
    "$\\bspace P\\CB{T_2>t\\mid T_1 = s} = P\\CB{0\\text{ events in }(s,s+t]} = e^{-\\lambda t}\\Rightarrow P\\CB{T_2 > t} = \\d\\int_0^\\infty e^{-\\lambda t} \\cdot \\lambda e^{-\\lambda s} \\;\\dd s = e^{-\\lambda t}$\n",
    "\n",
    "Repeating the same argument yields the following\n",
    "\n",
    "$Proposition.1$\n",
    "\n",
    "$T_n$, $n=1,2,\\dots$ are $i.i.d.$ **exponential** $r.v.$s with mean $1/\\lambda$.\n",
    "\n",
    "$Remark$\n",
    "\n",
    ">Define $S_n$ as the ***waiting time*** until the $n$th events. Of course, $S_n = \\sum\\limits_{i=1}^n T_i$, for $n\\geq 1$. Then as the sum of $i.i.d.$ **exponential** $r.v.$s with common rate $\\lambda$, we have its density:\n",
    ">\n",
    ">$\\bspace f_{S_n}\\P t = \\lambda e^{-\\lambda t}\\ffrac{\\P{\\lambda t}^{n-1}}{ \\P{n-1}! },\\bspace t\\geq 0$\n",
    ">\n",
    ">Another way to derive this is to first notice that $N\\P t \\geq n \\iff S_n \\leq t$, hence\n",
    ">\n",
    ">$\\bspace\\begin{align}\n",
    "F_{S_n}\\P t &= P\\CB{S_n \\leq t} = P\\CB{N\\P t \\geq n} = \\sum_{j=n}^\\infty e^{-\\lambda t}\\ffrac{\\P{\\lambda t}^j}{j!}\\\\\n",
    "f_{S_n}\\P t &= -\\sum_{j=n}^\\infty \\lambda e^{-\\lambda t}\\ffrac{\\P{\\lambda t}^j}{j!} + \\sum_{j=n}^\\infty \\lambda e^{-\\lambda t}\\ffrac{\\P{\\lambda t}^\\P{j-1}}{\\P{j-1}!} \\\\\n",
    "&= -\\sum_{j=n}^\\infty \\lambda e^{-\\lambda t}\\ffrac{\\P{\\lambda t}^j}{j!} + \\sum_{j=n+1}^\\infty \\lambda e^{-\\lambda t}\\ffrac{\\P{\\lambda t}^\\P{j-1}}{\\P{j-1}!} + \\lambda e^{-\\lambda t}\\ffrac{\\P{\\lambda t}^\\P{n-1}}{\\P{n-1}!} = \\lambda e^{-\\lambda t}\\ffrac{\\P{\\lambda t}^\\P{n-1}}{\\P{n-1}!}\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.13**\n",
    "\n",
    "Suppose that people immigrate into a territory at a Poisson rate $\\lambda = 1$ per day.\n",
    "\n",
    "$\\P{\\text{a}}$ What is the expected time until the tenth immigrant arrives?\n",
    "\n",
    "> $\\Exp\\SB{S_{10}} = 10\\cdot \\ffrac{1}{\\lambda} = 10$ days\n",
    "\n",
    "$\\P{\\text{b}}$ What is the probability that the elapsed time between the tenth and the eleventh arrival exceeds two days?\n",
    "\n",
    ">$P\\CB{T_{11}>2} = e^{-\\lambda \\cdot 2} = e^{-2} \\approx 0.133$\n",
    "\n",
    "***\n",
    "\n",
    "And this newly defined $T_n$ leads to a new definition of a counting process. Given a sequence $\\CB{T_n,n\\geq1}$, the $n$th event of this process occurs at time $S_n \\equiv T_1+T_2+\\cdots+T_n$. So we define\n",
    "\n",
    "$\\bspace N\\P t \\equiv \\max\\CB{n:S_n\\leq t}$\n",
    "\n",
    "this resultant counting process $\\CB{N\\P t, t\\geq 0}$ will be Poisson process with rate $\\lambda$.\n",
    "\n",
    "$Remark$\n",
    "\n",
    ">This new definition leads to a new method to find the pdf of $S_n$.\n",
    ">\n",
    ">$\\begin{align}\n",
    "P\\CB{t<S_n<t+h} &= P\\CB{N\\P t = n-1, \\text{another one event in }(t,t+h]} + o\\P h\\\\\n",
    "&= P\\CB{N\\P t = n-1}\\cdot P\\CB{\\text{another one event in }(t,t+h]} + o\\P h\\\\\n",
    "&= e^{-\\lambda t}\\ffrac{\\P{\\lambda t}^{n-1}}{\\P{n-1}!} \\cdot\\P{\\lambda h+o\\P h} + o\\P h\\\\\n",
    "&= \\lambda e^{-\\lambda t}\\ffrac{\\P{\\lambda t}^{n-1}}{\\P{n-1}!}\\cdot h + o\\P h\\\\\n",
    "\\Rightarrow f_{S_n}\\P t &= \\lambda e^{-\\lambda t}\\ffrac{\\P{\\lambda t}^{n-1}}{\\P{n-1}!}\n",
    "\\end{align}$\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Properties of Poisson Processes\n",
    "\n",
    "Consider a Poisson process $\\CB{N\\P t,t\\geq 0}$ having rate $\\lambda$, suppose we can classify the event as type **I** or a type **II** event, with probability $p$ and $1-p$ respectively. Let $N_1\\P t$ and $N_2\\P t$ denote respectively the number of type I and type II events occuring in $\\SB{0,t}$. Then we have\n",
    "\n",
    "$Proposition.2$\n",
    "\n",
    "$N\\P t = N_1\\P t + N_2\\P t$ and actually, $\\CB{N_1\\P t,t\\geq 0}$ and $\\CB{N_2\\P t,t\\geq 0}$ are both Poisson processes having rate $\\lambda p$ and $\\lambda\\P{1-p}$, respectively. Also, the two processes are independent.\n",
    "\n",
    "$Proof$\n",
    "\n",
    ">First we prove that $\\CB{N_i\\P t,t\\geq 0}$ are Poisson process, by checking the definition\n",
    ">\n",
    ">- $N_i\\P{0} = 0$ follows from the fact that $N\\P0 = 0$\n",
    ">- Stationary and independent increment also holds. Whatever, just believe this.\n",
    ">- $\\begin{align}\n",
    "P\\CB{N_1\\P h = 1} &= P\\CB{N_1\\P h = 1\\mid N\\P h = 1} \\cdot P\\CB{N\\P h = 1} \\\\\n",
    "&\\bspace+ P\\CB{N_1\\P h = 1\\mid N\\P h \\geq 2} \\cdot P\\CB{N\\P h \\geq 2} \\\\\n",
    "&= p\\P{\\lambda h + o\\P h} + o\\P{h}\\\\\n",
    "&= \\lambda p h + o\\P h\n",
    "\\end{align}$, and same for $P\\CB{N_2\\P h = 1}$\n",
    ">- $P\\CB{N_1\\P h \\geq 2}\\leq P\\CB{N\\P h \\geq 2} = o\\P{h}$, similar for $P\\CB{N_2\\P h \\geq 2}$\n",
    ">\n",
    ">Since the probability of a type I event in $(t,t+h]$ is independent of those in intervals that do not overlap $\\P{t,t+h}$, therefore, it's independent of knowledge of when type II events occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.14**\n",
    "\n",
    "Easy one.\n",
    "\n",
    "**e.g.15**\n",
    "\n",
    "Interesting, skipped though.\n",
    "\n",
    "***\n",
    "\n",
    "**e.g.16** About dividing into $r$ groups, $r>2$\n",
    "\n",
    "Consider a system in which individuals at any time are classified as being in one of $r$ possible states, and assume that an individual changes states in accordance with a Markov chain having transition probabilities $P_{ij}$, $i , j = 1,\\dots, r$. The individuals move independently. Suppose that the numbers of people initially in states $1,\\dots, r$ are independent Poisson $r.v.$ with respective means $\\lambda_1,\\dots, \\lambda_r$. We are interested in determining the *joint distribution* of the numbers of individuals in states $1,\\dots, r$ at some time $n$.\n",
    "\n",
    ">For fixed $i$, let $N_j\\P i$, $j=1,2,\\dots,r$ denote the number of those individuals initially in state $i$, that are in state $j$ at time $n$. And now each of the Poisson distributed number of people in state $i$ will, independently, be in state $j$ at time $n$ with probability $P_{ij}^n$.\n",
    ">\n",
    ">Hence, $N_j\\P i$, $j=1,2,\\dots,r$ will be independent Poisson random variables with respective means $\\lambda_iP_{ij}^n$, $j=1,2,\\dots,r$. Because *the sum of independent Poisson $r.v.$s is also a Poisson $r.v.$*. it follows that the number of individuals in state $j$ at time $n$, namely, $\\sum\\limits_{i=1}^r N_j\\P i$, will be independent Poisson $r.v.$s with respective means $\\sum\\limits_i \\lambda_i P_{ij}^n$, for $j=1,2,\\dots,r$.\n",
    "***\n",
    "\n",
    "**e.g.17** Coupon collecting problem\n",
    "\n",
    "Interesting, easy to understand, skipped though.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now trying to find the probability that $n$ events occur in one Poisson process before $m$ events have occurred in a second and independent Poisson process. Let $\\CB{N_1\\P t,t\\geq 0}$ and $\\CB{N_2\\P t,t\\geq 0}$ be two independent Poisson processes having respective rates $\\lambda_1$ and $\\lambda_2$. Let $S_n^1$ denote the time of the $n$th event of the first process and $S_m^2$ the second one.\n",
    "\n",
    "What's $P\\CB{S_n^1 < S_m^2}$?\n",
    "\n",
    "The answer is easy to obtain when $n=m=1$, which is $P\\CB{S_1^1 < S_1^2} = \\ffrac{\\lambda_1}{\\lambda_1 + \\lambda_2}$. Follow the intuition, we then consider $P\\CB{S_2^1 < S_1^2}$ which gives us (**memoryless property** will remove the effect of conditioning on $S_1^1 < S_1^2$)\n",
    "\n",
    "$\\bspace\\d{P\\CB{S_2^1 < S_1^2} = \\P{P\\CB{S_1^1 < S_1^2}}^2 = \\P{\\ffrac{\\lambda_1}{\\lambda_1 + \\lambda_2}}^2}$\n",
    "\n",
    "In fact, this reasoning shows that given *all* that has previously occurred, which is independent from the future, each event that occurs is going to be an event of $N_1\\P t$ process with probability $\\ffrac{\\lambda_1}{\\lambda_1 + \\lambda_2}$ or an event of $N_2\\P t$ process $w.p.$ $\\ffrac{\\lambda_2}{\\lambda_1 + \\lambda_2}$. Thus\n",
    "\n",
    "$\\bspace\\d{P\\CB{S_n^1 < S_m^2} = \\sum_{k=n}^{n+m-1} \\binom{n+m-1}{k} \\P{\\ffrac{\\lambda_1}{\\lambda_1 + \\lambda_2}}^k\\P{\\ffrac{\\lambda_2}{\\lambda_1 + \\lambda_2}}^{n+m-1-k}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Distribution of the Arrival Times\n",
    "Given the fact that *exactly one* event has occured by time $t$, what's the distribution of the time at which the event occurred? Owing to the statinary and independent increments, we guess it's uniformly distributed over $\\SB{0,t}$. We now check that, and for $s\\leq k$\n",
    "\n",
    "$\\bspace\\begin{align}\n",
    "P\\CB{T_1< s\\mid N\\P t = 1} &= \\ffrac{P\\CB{T_1< s, N\\P t = 1}}{P\\CB{ N\\P t = 1}}\\\\\n",
    "&= \\ffrac{P\\CB{\\text{one event in }[0,s),\\text{zero event in }\\SB{s,t}}}{P\\CB{ N\\P t = 1}}\\\\\n",
    "&= \\ffrac{P\\CB{\\text{one event in }[0,s)}\\cdot P\\CB{\\text{zero event in }\\SB{s,t}}}{P\\CB{ N\\P t = 1}}\\\\\n",
    "&= \\ffrac{e^{\\lambda s}\\ffrac{\\lambda s}{1!} e^{\\lambda \\P{t-s}}\\ffrac{\\P{\\lambda \\P{t-s}}^0}{0!}}{e^{\\lambda t}\\ffrac{\\lambda t}{1!}} = \\ffrac{s}{t}\n",
    "\\end{align}$\n",
    "\n",
    "$Remark$\n",
    "\n",
    ">About the ***order statistics***: Let $Y_1,\\dots,Y_n$ be $n$ $r.v.$. We say $Y_{\\P{1}},\\dots,Y_{\\P n}$ are the **order statistics** corresponding to $Y_1,\\dots,Y_n$ if $Y_{\\P k}$ is the $k$th smallest value among $Y_1,\\dots,Y_n$. Now if suppose further, that $Y_i$ are $i.i.d.$ continuous $r.v.$ with pdf $f$, we now have the the joint density of the **order statistics**. \n",
    ">\n",
    ">$\\d{f\\P{y_1,\\dots,y_n} = n! \\prod_{i=1}^n f\\P{y_i},\\bspace y_1<y_2<\\cdots<y_n}$\n",
    ">\n",
    ">Especially, when all $Y_i$ are uniformly distributed over $\\P{0,t}$, we have\n",
    ">\n",
    ">$\\d{f\\P{y_1,\\dots,y_n} = \\ffrac{n!}{t^n},\\bspace 0< y_1<y_2<\\cdots<y_n<t}$\n",
    "\n",
    "With this we have the following Theorem\n",
    "\n",
    "$Theorem.2$\n",
    "\n",
    "Given that $N\\P t = n$, the **arrival times** $S_1,S_2,\\dots,S_n$ have the same distribution as the **order statistics** corresponding to $n$ independent $r.v.$ uniformly distributed on the interval $\\P{0,t}$.\n",
    "\n",
    "$Proof$\n",
    "\n",
    ">Note that event $\\P{s_1,s_2,\\dots,s_n, N\\P t = n}$ where $s_n$ marks the arrival time, is equivalent to $\\P{T_1 = s_1,T_2 = s_2,\\dots,T_n = s_n,T_{n+1}>t-s_n}$ where $T_n$ marks the interarrival time. *Since $T_n$ are $i.i.d.$ exponential $r.v.$ with rate $\\lambda$*, we have\n",
    ">\n",
    ">$\\begin{align}\n",
    "f\\P{s_1,s_2,\\dots,s_n\\mid N\\P t = n} &= \\ffrac{f\\P{s_1,s_2,\\dots,s_n,n}}{P\\CB{N\\P t = n}}\\\\\n",
    "&= \\ffrac{f\\P{s_1,s_2,\\dots,s_n,n}}{P\\CB{N\\P t = n}}\\\\\n",
    "&= \\ffrac{\\lambda e^{-\\lambda s_{\\void_1}}\\cdot\\lambda e^{-\\lambda \\P{s_{\\void_2} - s_{\\void_1}}} \\cdots \\lambda e^{-\\lambda \\P{s_{\\void_n} - s_{\\void_{n-1}}\\,}} \\cdot e^{-\\lambda \\P{t-s_{\\void_n}}}}{ e^{-\\lambda t } \\ffrac{ \\P{\\lambda t}^n}{n!}}\\\\\n",
    "&= \\ffrac{n!}{t^n},\\bspace 0<s_1<s_2<\\cdots<s_n < t\n",
    "\\end{align}$\n",
    "\n",
    "$Proposition.3$\n",
    "\n",
    "Assume Possion process $\\CB{N\\P t,t\\geq 0}$ with rate $\\lambda$ and $k$ possible sub types. Further, suppose that if an event occurs at time $y$ then it will be classified as a type $i$ event independently with probability $P_i\\P y$, $i=1,\\dots,k$ where $\\sum_{i=1}^k P_i\\P y = 1$.\n",
    "\n",
    "If $N_i\\P t$, $i=1,\\dots,k$ represents the number of type $i$ events occuring by time $t$, then $N_i\\P t$ are independent Poisson $r.v.$ with means\n",
    "\n",
    "$\\bspace\\d{ \\Exp\\SB{N_i\\P t} = \\lambda \\int_0^t P_i\\P s \\;\\dd s}$\n",
    "\n",
    "Whatever, skipped. **e.g.21** and **e.g.22** are very interesting, though.\n",
    "***\n",
    "$Proposition.1$\n",
    "\n",
    "Given that $S_n = t$, he set $S_1, S_2, \\dots, S_{n-1}$ has the distribution of a set of $n-1$ independent uniform $\\P{0,t}$ $r.v.$.\n",
    "\n",
    "$Proof$\n",
    "\n",
    "> Other than using the same reasoning of how we prove the $Theorem 5.2$, here's a loose proof\n",
    ">\n",
    ">$\\begin{align}\n",
    "&\\P{S_1,S_2\\dots,S_{n-1} \\mid S_n = t}\\\\\n",
    "\\sim\\; & \\P{S_1,S_2\\dots,S_{n-1} \\mid S_n = t, N\\P{t^-} = n-1} \\\\\n",
    "\\sim\\; & \\P{S_1,S_2\\dots,S_{n-1} \\mid N\\P{t^-} = n-1}\n",
    "\\end{align}$\n",
    ">\n",
    ">where $t^-$ is infinitesimally smaller than $t$ and $\\sim$ means \"has the same distribution as\".\n",
    "\n",
    "### Estimating Software Reliability\n",
    "\n",
    "skipped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalizations of the Poisson Process\n",
    "### Nonhomogeneous Poisson Process\n",
    "\n",
    "As is also known as ***nonstationary Poisson Process***, we obtain this by allowing the arrival rate at time $t$ to be a function of $t$, meaning $\\lambda \\P t$\n",
    "\n",
    "$Def.3$\n",
    "\n",
    "The counting process $\\CB{N\\P t, t \\geq 0}$ is said to be a ***Nonhomogeneous Poisson Process*** with *intensity function* $\\lambda \\P t$, $t\\geq 0$, if\n",
    "\n",
    "1. $N\\P 0 = 0$\n",
    "2. $\\CB{N\\P t, t \\geq 0}$ has independent increments\n",
    "3. $P\\CB{N\\P{t+h} - N\\P t \\geq 2} = o\\P h$\n",
    "4. $P\\CB{N\\P{t+h} - N\\P t = 1} = \\lambda\\P t \\cdot h + o\\P h$\n",
    "\n",
    "We also define the ***mean value function*** of a **nonhomogeneous poisson process** as\n",
    "\n",
    "$\\bspace m\\P t = \\d\\int_0^t \\lambda \\P y \\;\\dd y$\n",
    "\n",
    "$Theorem.3$\n",
    "\n",
    "If $\\CB{N\\P t, t \\geq 0}$ is a **nonstationary Poisson process** with **intensity function** $\\lambda \\P t$, $t\\geq 0$, then $N\\P{t+s} - N\\P{s}$ is a Poisson $r.v.$ with *mean* $m\\P{t+s} - m\\P s = \\d\\int_{s}^{t+s} \\lambda \\P y\\;\\dd y$.\n",
    "\n",
    "$Proof$\n",
    "\n",
    ">Mimick the proof of $Theorem5.1$, the stationary one, we have, by letting $g\\P t=\\Exp\\SB{e^{-uN\\P{t}}}$\n",
    ">\n",
    ">$\\bspace g\\P{t+h}=g\\P{t} \\Exp\\SB{e^{-uN_t\\P{h}}}$\n",
    "\n",
    ">where $N_t\\P{h} = N\\P{t+h} - N\\P{t}$. Since $P\\CB{N\\P{t+h} - N\\P t = 0} = 1- \\lambda\\P t \\cdot h + o\\P h$, the preceding is\n",
    ">\n",
    ">$\\bspace\\begin{align}\n",
    "g\\P{t+h} &= g\\P{t} \\big(e^0\\cdot \\P{1- \\lambda\\P t \\cdot h + o\\P h} + e^{-u}\\cdot \\P{\\lambda\\P t \\cdot h + o\\P h} + e^{-2u}\\cdot o\\P h\\big)\\\\\n",
    "&= g\\P t\\P{1-\\lambda \\P t\\cdot h + e^{-u} \\lambda \\P t\\cdot h + o\\P h}\\\\\n",
    "\\Rightarrow g\\P{t+h} - g\\P h &= g\\P{t}\\lambda \\P t\\P{e^{-u} -1}h + o\\P h\n",
    "\\end{align}$\n",
    ">\n",
    ">Dividing by $h$ and letting $h\\to0$ yields the differential equation $g'\\P t = g\\P t \\lambda \\P t \\P{e^{-u} - 1}$. Then we write $\\ffrac{g'\\P t}{g\\P t} = \\lambda\\P t\\P{e^{-u} -1}$. Solve this we have\n",
    ">\n",
    ">$\\bspace g\\P t = \\exp\\CB{m\\P t \\P{e^{-u}-1}}$\n",
    ">\n",
    ">However, $\\exp\\CB{m\\P t \\P{e^{-u}-1}}$ is also the **Laplace transform** of a Poisson $r.v.$ with mean $m\\P t$. We assert that $N\\P t $ is also Poisson with mean $m\\P t$. Then $N\\P{t+s} - N\\P{s} = N_s\\P t$ is a nonstationary Poisson process with **intensity function** $\\lambda_s \\P t = \\lambda \\P{s+t}$, $t>0$. Consequently, $N_s\\P t$ is Poisson with mean\n",
    ">\n",
    ">$\\bspace\\d{\\int_0^t \\lambda_s\\P y \\;\\dd y = \\int_0^t \\lambda\\P{s+y} \\;\\dd y = \\int_s^{s+t} \\lambda\\P x \\;\\dd y}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One special case is the time sampling of an ordinary Poisson process. Consider an ordinary Poisson process with rate $\\lambda$, $\\CB{N\\P t, t\\geq0}$ and suppose that an event occuring at time $t$ is independently of what has occurred prior to $t$. We count the event with probability $p\\P t$ and let $N_c\\P t$ denote the counted events by time $t$, thus, the counting process $\\CB{N_c\\P t, t\\geq 0}$ is a nonhomogeneous Poisson process with **intensity function** $\\lambda \\P t = \\lambda \\cdot p\\P t$. We verify this by noting that this process satisfies the four nonhomogeneous Poisson process axioms.\n",
    "\n",
    "1. $N_c\\P 0 = 0$\n",
    "2. The number of *counted events* in $\\P{s,s+t}$ depends solely on the number of *events* of the Poisson process in $\\P{s,s+t}$, which is independent of what has occurred prior to time $s$. Consequently, the number of *counted events* in $\\P{s,s+t}$ is independent of the process of *counted events* prior to $s$.\n",
    "3. Let $N_c\\P{t,t+h} = N_c\\P{t+h} - N_c\\P{t}$, with a similar definition of $N\\P{t,t+h}$.$\\\\[0.6em]$\n",
    "$\\bspace P\\CB{N_c\\P{t,t+h} \\geq 2} \\leq P\\CB{N\\P{t,t+h} \\geq 2} = o\\P h\\\\[0.6em]$\n",
    "\n",
    "4. Compute $P\\CB{N_c\\P{t,t+h} =1}$ by conditioning on $N\\P{t,t+h}$.$\\\\[0.6em]$\n",
    "$\\bspace\\begin{align}\n",
    "P\\CB{N_c\\P{t,t+h} =1} &= P\\CB{N_c\\P{t,t+h} =1\\mid N\\P{t,t+h} = 1}\\cdot P\\CB{N\\P{t,t+h} =1}\\\\\n",
    "&\\bspace + P\\CB{N_c\\P{t,t+h} =1\\mid N\\P{t,t+h} \\geq 2}\\cdot P\\CB{N\\P{t,t+h} \\geq 2} \\\\\n",
    "&= P\\CB{N_c\\P{t,t+h} =1\\mid N\\P{t,t+h} = 1} \\cdot \\lambda h + o\\P h\\\\\n",
    "&= p\\P t \\cdot \\lambda h+ o\\P h\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.24**\n",
    "\n",
    "Arrivals in a hot dog stores from $8$ A.M. consitute a nonhomogeneous Poisson process with **intensity function** $\\lambda\\P t$ given by\n",
    "\n",
    "$\\bspace\\lambda\\P t = \\begin{cases}\n",
    "0,   & 0 \\leq t 8\\\\\n",
    "5+5\\P{t-8},& 8\\leq t \\leq 11\\\\\n",
    "20,&   11\\leq t \\leq 13\\\\\n",
    "20-2\\P{t-13},& 13\\leq t \\leq 17\\\\\n",
    "0, & 17 < t \\leq 24\n",
    "\\lambda\\P{t-24},& t>24\n",
    "\\end{cases}$\n",
    "\n",
    ">The number of arrivals between $8:30$ A.M. and $9:30$ A.M. will be a Poisson with mean $m\\P{9.5} - m\\P{8.5}$, so that the probability of this number being $0$ is\n",
    ">\n",
    ">$\\bspace \\d{ \\exp\\CB{-\\int_{8.5}^{9.5} 5+5\\P{t-8} \\;\\dd t} = e^{-10}}$\n",
    ">\n",
    ">And the mean number of arrivals is $ \\d{\\int_{8.5}^{9.5} 5+5\\P{t-8} \\;\\dd t = 10}$\n",
    "\n",
    "***\n",
    "\n",
    "We can further suppose that an event at time $s$ is a type $1$ event $w.p.$ $P_1\\P s$ or $2$ for $P_2\\P{s} = 1 - P_1\\P s$. Let $N_i\\P t$, $t\\geq0$ denote the number of type $i$ events by time $t$, then we have the similar result like stationary Poisson proecss that are\n",
    "\n",
    "- $\\CB{N_1\\P t, t\\geq0}$ and $\\CB{N_2\\P t, t\\geq0}$ are independent nonhomogeneous Poisson process with respective **intensity function** $\\lambda_i\\P t = \\lambda\\P t P_i\\P t = \\lambda P_i\\P t$, $i=1,2$.\n",
    "- $N_1\\P t$ and $N_2\\P t$ are independent Poisson $r.v.$ with $\\Exp\\SB{N_i\\P t} = \\lambda \\d{\\int_0^t P_i\\P s \\;\\dd s}$, $i=1,2$\n",
    "\n",
    "If we let $S_n$ denote the time of the $n$th event of the nonhomogeneous Poisson process, then we can obtain its density as follows:\n",
    "\n",
    "$\\bspace\\begin{align}\n",
    "P\\CB{t<S_n < t+h} &= P\\CB{N\\P t = n-1,\\text{ one event in }\\P{t,t+h}} + o\\P h\\\\\n",
    "&= e^{-m\\P t} \\ffrac{\\P{m\\P t}^{n-1}}{\\P{n-1}!} \\cdot P\\CB{N\\P{t+h} - N\\P t = 1}+ o\\P h\\\\\n",
    "&= \\lambda \\P t e^{-m\\P t} \\ffrac{\\P{m\\P t}^{n-1}}{\\P{n-1}!}+ o\\P h\n",
    "\\end{align}$\n",
    "\n",
    "which implies that $f_{S_n}\\P t = \\lambda \\P t e^{-m\\P t} \\ffrac{\\P{m\\P t}^{n-1}}{\\P{n-1}!}$, where $m\\P t = \\d\\int_0^t \\lambda \\P s\\;\\dd s$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compound Poisson Process\n",
    "A stochastic process $\\CB{X\\P t, t\\geq 0}$ is said to be a ***compound Poisson process*** if it can be represented as $X\\P t = \\sum_{i=1}^{N\\P t} Y_i$, $t\\geq0$ where\n",
    "\n",
    "- $\\CB{N\\P t,t\\geq0}$ is a Poisson process\n",
    "- $\\CB{Y_i,i\\geq1}$ is a family off $i.i.d.$ that is also independent of $\\CB{N\\P t,t\\geq0}$\n",
    "\n",
    "And the result in Chap_03 tells us that $\\Exp\\SB{X\\P t} = \\lambda t \\Exp\\SB{Y_1}$, $\\Var{X\\P t} = \\lambda t\\Exp\\SB{Y_1^2}$.\n",
    "\n",
    "A lot of contents are skipped and then it comes to another result, that if $\\CB{X\\P t, t\\geq0}$ and $\\CB{Y\\P t, t\\geq0}$ are independent compound Poisson processes with respective Poisson parameters and distributions $\\lambda_1$, $F_1$ and $\\lambda_2$, $F_2$, then $\\CB{X\\P t + Y\\P t, t\\geq0}$ is also a compound Poisson process, with Poisson parameter $\\lambda_1 + \\lambda_2$ and distribution function $F$ given by\n",
    "\n",
    "$\\bspace F\\P x = \\ffrac{\\lambda_1}{\\lambda_1 + \\lambda_2}F_1\\P x +\\ffrac{\\lambda_2}{\\lambda_1 + \\lambda_2}F_2\\P x$\n",
    "\n",
    "### Conditional or Mixed Poisson Processes\n",
    "\n",
    "skipped\n",
    "\n",
    "## Random Intensity Functions and Hawkes Processes\n",
    "skipped"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "129px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
