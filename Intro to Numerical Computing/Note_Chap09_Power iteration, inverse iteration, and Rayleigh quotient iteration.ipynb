{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rayleigh quotient\n",
    "Consider a linear least squares problem. Given a vector $\\vec{b} \\in \\mathrm{R}^m$ and a matrix $A \\in \\mathrm{R} ^{m\\times n}$, the best fit to $\\vec{b}$ from the range of $A$ is $A\\vec{x}$, where $\\vec{x}$ is the linear least square solution and can be found through solving the normal equation\n",
    "\n",
    "$$\\newcommand{\\RR}{\\mathbb{R}}\\newcommand{\\ffrac}{\\displaystyle \\frac}\\newcommand{\\Tran}[1]{{#1}^{\\mathrm{T}}}\\Tran{A}A\\vec{x} = \\Tran{A}\\vec{b}$$\n",
    "\n",
    "Now we let $A$ be a real symmetric $n \\times n$ matrix. Given an arbitary vector $\\vec{x} \\in \\RR^n$, which may not be the eigenvector of $A$, we try to best fit that to $A\\vec{x}$ among all multiple of $\\vec{x}$.\n",
    "\n",
    "$$\\alpha^* = \\arg \\min_{\\alpha\\in \\RR} \\left\\| \\vec{x}\\alpha - A\\vec{x} \\right\\|_2$$\n",
    "\n",
    "so that the solution $\\alpha^*$ satisfies the equation:\n",
    "\n",
    "$$\\Tran{\\vec{x}}\\vec{x} \\alpha^* = \\Tran{\\vec{x}}A\\vec{x} \\Longrightarrow \\alpha^* = \\frac{\\Tran{\\vec{x}}A\\vec{x}} {\\Tran{\\vec{x}}\\vec{x}}$$\n",
    "\n",
    "And it's obvious that if $\\vec{x}$ is indeed an eigenvector of matrix $A$, then\n",
    "\n",
    "$$\\alpha^* = \\frac{\\Tran{\\vec{x}}A\\vec{x}} {\\Tran{\\vec{x}}\\vec{x}} = \\frac{\\Tran{\\vec{x}}\\lambda\\vec{x}} {\\Tran{\\vec{x}}\\vec{x}} = \\lambda$$\n",
    "\n",
    "And in particular, given a real symmetric matrix $A \\in \\RR^{n \\times n}$, we call **Rayleigh quotient** of $A$ at any vector $\\vec{x} \\in \\RR^n$\n",
    "\n",
    "$$r(\\vec{x})= \\frac{\\Tran{\\vec{x}}A\\vec{x}} {\\Tran{\\vec{x}}\\vec{x}}$$\n",
    "***\n",
    "\n",
    "Then we do Taylor expansion of multiple variables.\n",
    "\n",
    "$$r\\left(\\vec{x}\\right) = r\\left(\\vec{q}\\right) + \\nabla \\Tran{r\\left(\\vec{q}\\right)} \\left( \\vec{x} - \\vec{q} \\right) + \\frac{\\Tran{\\left( \\vec{x} - \\vec{q} \\right)} \\nabla^2 r\\left(\\tilde{\\vec{q}}\\right)\\left( \\vec{x} - \\vec{q} \\right)} {2}$$\n",
    "\n",
    "Here $\\vec{q}$ is an eigenvector of $A$ corresponding to eigenvalue $\\lambda$. $\\nabla r\\left(\\vec{q}\\right)$ is the Jacobian of $r\\left(\\vec{x}\\right)$ at $\\vec{x} = \\vec{q}$; $\\nabla^2 r\\left(\\tilde{\\vec{q}}\\right)$ is the Hessian matrix of the function $r\\left(\\vec{x}\\right)$ at $\\tilde{\\vec{q}}$ which is close to $\\vec{q}$.\n",
    "\n",
    "$\\nabla r\\left(\\vec{q}\\right)$ is a column vector, and contains the partial drivative of $r\\left(\\vec{x}\\right)$ with respect to the components of $\\vec{x}$. From the definition $r(\\vec{x})= \\ffrac{\\Tran{\\vec{x}}A\\vec{x}} {\\Tran{\\vec{x}}\\vec{x}}$, we have\n",
    "\n",
    "$$\n",
    "\\frac{\\partial r\\left(\\vec{x}\\right)} {\\partial x_k} = \\frac{\\frac{\\partial \\left(\\Tran{\\vec{x}}A\\vec{x} \\right)} {\\partial {x_{k}}} \\cdot \\left(\\Tran{\\vec{x}}\\vec{x}\\right) - \\left(\\Tran{\\vec{x}} A \\vec{x}\\right) \\cdot \\frac{ \\partial \\left(\\Tran{\\vec{x}}\\vec{x}\\right)} {\\partial {x_{k}}}} {\\left( \\Tran{\\vec{x}}\\vec{x} \\right)^2}\n",
    "$$\n",
    "\n",
    "One way to derive the formula for $\\ffrac{\\partial \\left(\\Tran{\\vec{x}}A\\vec{x} \\right)} {\\partial {x_{k}}}$ is to decomposite it completely, another way is shown below\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial \\left(\\Tran{\\vec{x}}A\\vec{x} \\right)} {\\partial {x_{k}}} &= \\frac{\\partial \\left(\\Tran{\\vec{x}}A \\right)} {\\partial {x_{k}}} \\cdot \\vec{x} + \\Tran{\\vec{x}} \\cdot \\ffrac{\\partial \\left(A\\vec{x} \\right)} {\\partial {x_{k}}} \\\\\n",
    "&= 2 \\cdot \\frac{\\partial \\left(\\Tran{\\vec{x}}A \\right)} {\\partial {x_{k}}} \\cdot \\vec{x} \\\\\n",
    "&= 2 \\cdot \\frac{\\partial \\left(\n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "\\Tran{\\vec{x}} \\vec{a}_1^c & \\Tran{\\vec{x}} \\vec{a}_2^c & \\cdots & \\Tran{\\vec{x}} \\vec{a}_n^c\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\right)} {\\partial {x_{k}}} \\cdot \\vec{x} \\\\\n",
    "&= 2 \\cdot \\left[\n",
    "\\begin{array}{cccc}\n",
    "a_{k1} & a_{k2} & \\cdots & a_{kn}\n",
    "\\end{array}\n",
    "\\right] \\cdot \\vec{x} \\\\\n",
    "&= 2\\cdot \\vec{a}_k ^r \\cdot \\vec{x} \\\\\n",
    "&= 2 \\cdot k\\text{-th row of }A\\vec{x}\n",
    "\\end{align}$$\n",
    "\n",
    "Then we have\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial r\\left(\\vec{x}\\right)} {\\partial x_k} &= \\frac{\\frac{\\partial \\left(\\Tran{\\vec{x}}A\\vec{x} \\right)} {\\partial {x_{k}}} \\cdot \\left(\\Tran{\\vec{x}}\\vec{x}\\right) - \\left(\\Tran{\\vec{x}} A \\vec{x}\\right) \\cdot \\frac{ \\partial \\left(\\Tran{\\vec{x}}\\vec{x}\\right)} {\\partial {x_{k}}}} {\\left( \\Tran{\\vec{x}}\\vec{x} \\right)^2} \\\\\n",
    "&= \\frac{2\\cdot \\left( A \\vec{x} \\right)_{k\\text{-th row}}\\left(\\Tran{\\vec{x}}\\vec{x} \\right) - \\left(\\Tran{\\vec{x}} A \\vec{x}\\right)\\cdot \\left( 2x_k \\right)} {\\left( \\Tran{\\vec{x}}\\vec{x} \\right)^2} \\\\\n",
    "&= \\frac{2} {\\Tran{\\vec{x}}\\vec{x}} \\left( A\\vec{x} - r\\left(\\vec{x}\\right) \\vec{x} \\right)_{k\\text{-th row}}\n",
    "\\end{align}$$\n",
    "\n",
    "So that\n",
    "\n",
    "$$\\nabla r\\left( \\vec{x} \\right) = \\Tran{\\left[\n",
    "\\begin{array}{cccc}\n",
    "\\frac{\\partial r\\left(\\vec{x}\\right)} {\\partial x_1}&\\frac{\\partial r\\left(\\vec{x}\\right)} {\\partial x_2} & \\cdots & \\frac{\\partial r\\left(\\vec{x}\\right)} {\\partial x_n}\n",
    "\\end{array}\n",
    "\\right]} = \\frac{2} {\\Tran{\\vec{x}}\\vec{x}} \\left( A\\vec{x} - r\\left(\\vec{x}\\right) \\vec{x} \\right)$$\n",
    "\n",
    "We can plug in eigenvector $\\vec{q}$ for check\n",
    "\n",
    "$$\\nabla r\\left( \\vec{q} \\right) = \\frac{2} {\\Tran{\\vec{q}}\\vec{q}} \\left( A\\vec{q} - \\lambda \\vec{q} \\right) = \\vec{0}$$\n",
    "\n",
    "Then the Taylor expansion of $r\\left( \\vec{q} \\right)$ at $\\vec{q}$ becomes\n",
    "\n",
    "$$\\boxed{ r\\left( \\vec{q} \\right) = \\lambda + \\frac{\\Tran{\\left( \\vec{x} - \\vec{q} \\right)} \\nabla^2 r\\left(\\tilde{\\vec{q}}\\right)\\left( \\vec{x} - \\vec{q} \\right)} {2} }$$\n",
    "\n",
    "Assuming that $\\left\\| \\nabla^2 r\\left( \\tilde{\\vec{q}} \\right) \\right\\|$ is bounded by a constant, then we have\n",
    "\n",
    "$$\\left| r\\left( \\vec{x} \\right) - \\lambda \\right| = O \\left( \\left\\| \\vec{x} - \\vec{q} \\right\\|^2 \\right)$$\n",
    "\n",
    "$Conclusion$\n",
    "\n",
    "Given a symmetric matrix $A$, if $\\vec{x}$ is an approximation of an eigenvector of $A$, then its Rayleigh quotient $r\\left(\\vec{x}\\right)$ approximates the corresponding eigenvalue of $A$. Besides, the accuracy is even better than the approximation of the eigenvector.\n",
    "\n",
    "What's more, if $\\vec{x}$ is normalized, $i.e.$, if $\\left\\|\\vec{x}\\right\\|_2 = 1$, then the Rayleigh quotient of $\\vec{x}$ is $r\\left( \\vec{x} \\right) = \\Tran{\\vec{x}}A \\vec{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A few properties of eigenvalues\n",
    "## Inversion\n",
    "If $A$ is nonsingular and $A\\vec{x} = \\lambda \\vec{x}$, for $\\vec{x} \\neq \\vec{0}$, then $A^{-1} \\vec{x} = \\ffrac{1} {\\lambda} \\vec{x}$. In another word, if $\\lambda$ is an eigenvalue of $A$, then $\\lambda^{-1}$ is the eigenvalue of $A^{-1}$, and they share the same eigenvectors.\n",
    "\n",
    "## Powers\n",
    "If $A\\vec{x} = \\lambda \\vec{x}$, then for any integer $k$, we have $A^k \\vec{x} = \\lambda^k \\vec{x}$, $i.e.$,  taking the $k\\text{-th}$ power of a matrix also takes the $k\\text{-th}$ power of its eigenvalues, while the eigenvectors are the same.\n",
    "\n",
    "## Shift\n",
    "If $A\\vec{x} = \\lambda \\vec{x}$, the for any value $\\sigma$, $\\left( A - \\sigma I\\right) \\vec{x} = \\left( \\lambda  - \\sigma \\right) \\vec{x}$. Thus, the eigenvalues of the matrix $A - \\sigma I$ are shifted from the eigenvalues of\n",
    "$A$ by $\\sigma$, and again their eigenvectors are unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Power iteration\n",
    "Given a real symmetrix $n\\times n$ matrix $A$, it has $n$ real eigenvalues $\\lambda_1,\\lambda_2,\\dots, \\lambda_n$ and a set of orthonormal eigenvectors $\\vec{q}_1, \\vec{q}_2, \\dots, \\vec{q}_n$, which satisfy $A\\vec{q}_i = \\lambda_i \\vec{q}_i$ for $i = 1,2,\\dots, n$. And by default, the eigenvalues follow the order $\\left|\\lambda_1\\right| \\geq \\left|\\lambda_2\\right| \\geq \\cdots \\geq \\left|\\lambda_n\\right|$.\n",
    "\n",
    "Since now $\\vec{q}_1, \\vec{q}_2, \\dots, \\vec{q}_n$ is also a base of $n\\text{-dimensional}$ vector space, so that any vector $\\vec{v}^{(0)}$ can be written as \n",
    "\n",
    "$$\n",
    "\\begin{array}{rl}\n",
    "\\vec{v}^{(0)}\\!\\!\\! & =\\; a_1 \\vec{q}_1 + a_2 \\vec{q}_2 + \\cdots a_n \\vec{q}_n \\\\\n",
    "A\\vec{v}^{(0)}\\!\\!\\! & =\\;  A \\left( a_1 \\vec{q}_1 + a_2 \\vec{q}_2 + \\cdots a_n \\vec{q}_n \\right) \\\\\n",
    "& =\\;  a_1 \\lambda_1 \\vec{q}_1 + a_2 \\lambda_2 \\vec{q}_2 + \\cdots a_n \\lambda_n \\vec{q}_n \\\\\n",
    "A^2\\vec{v}^{(0)}\\!\\!\\! & =\\;  A \\left( a_1 \\lambda_1 \\vec{q}_1 + a_2 \\lambda_2 \\vec{q}_2 + \\cdots a_n \\lambda_n \\vec{q}_n \\right)\\\\\n",
    "& =\\;  a_1 \\lambda_1^2 \\vec{q}_1 + a_2 \\lambda_2^2 \\vec{q}_2 + \\cdots a_n \\lambda_n^2 \\vec{q}_n \\\\\n",
    "& \\;  \\vdots \\\\\n",
    "A^k\\vec{v}^{(0)}\\!\\!\\! &= \\;  A \\left( a_1 \\lambda_1^{k-1} \\vec{q}_1 + a_2 \\lambda_2^{k-1} \\vec{q}_2 + \\cdots a_n\\lambda_n^{k-1} \\vec{q}_n \\right) \\\\\n",
    "& =\\;  a_1 \\lambda_1^k \\vec{q}_1 + a_2 \\lambda_2^k \\vec{q}_2 + \\cdots a_n \\lambda_n^k \\vec{q}_n \\\\\n",
    "& =\\;  a_1 \\lambda_1^k \\left( \\vec{q}_1 + \\frac{a_2} {a_1} \\left( \\frac{\\lambda_2} {\\lambda_1} \\right)^k \\vec{q}_2 + \\cdots + \\frac{a_n} {a_1} \\left( \\frac{\\lambda_n} {\\lambda_1}\\right)^k \\vec{q}_n \\right)\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Now we need two stronger prerequisites: \n",
    "1. $\\left|\\lambda_1\\right| > \\left|\\lambda_2\\right| \\geq \\cdots \\geq \\left|\\lambda_n\\right|$\n",
    "2. the initial vector $\\vec{v}^{(0)}$ has a nonzero component in the direction of eigenvector $\\vec{q}_1$.\n",
    "\n",
    "$$\\lim_{k \\to \\infty} \\frac{A^k \\vec{v}^{(0)}} {a_1 \\lambda_1^k} \\to \\vec{q}_1$$\n",
    "\n",
    "And the speed of convergence is determined by \n",
    "\n",
    "$$\\left\\| \\frac{A^k \\vec{v}^{(0)}} {a_1 \\lambda_1^k} - \\vec{q}_1 \\right\\| = O\\left( \\left| \\frac{\\lambda_2} {\\lambda_1} \\right|^k \\right)$$\n",
    "\n",
    "meaning that the convergence will be faster if $\\left| \\lambda_2 \\right|$ is much smaller than $\\left| \\lambda_1 \\right|$, and slower if $\\left| \\lambda_2 \\right|$ is closer to $\\left| \\lambda_1 \\right|$.\n",
    "\n",
    "In addition, we can also see that $\\left\\| \\ffrac{A^{k-1} \\vec{v}^{(0)}} {a_1 \\lambda_1^{k-1}} - \\vec{q}_1 \\right\\| = O\\left( \\left| \\ffrac{\\lambda_1} {\\lambda_2} \\right|^{k-1} \\right)$, so that\n",
    "\n",
    "$$\\left\\| \\frac{A^k \\vec{v}^{(0)}} {a_1 \\lambda_1^k} - \\vec{q}_1 \\right\\| = \\left\\| \\frac{A^{k-1} \\vec{v}^{(0)}} {a_1 \\lambda_1^{k-1}} - \\vec{q}_1 \\right\\| \\cdot O\\left( \\left| \\frac{\\lambda_2} {\\lambda_1} \\right| \\right)$$\n",
    "\n",
    "which implies that *the convergence rate is linear* and is reduced by a factor of $\\ffrac{\\lambda_2} {\\lambda_1}$ after each step. So that we define the sequence to approximate the eigenvector corresponding the largest eigenvalue of $A$.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{Initial } \\vec{v}^{(0)}, \\left\\| \\vec{v}^{(0)}\\right\\| &= 1 \\\\\n",
    "\\vec{v}^{(1)} &= \\frac{A \\vec{v}^{(0)}} {\\left\\| A \\vec{v}^{(0)} \\right\\|} \\\\\n",
    "\\vec{v}^{(2)} &= \\frac{A \\vec{v}^{(1)}} {\\left\\| A \\vec{v}^{(1)} \\right\\|} \\\\\n",
    "& \\vdots \\\\\n",
    "\\vec{v}^{(k)} &= \\frac{A \\vec{v}^{(0)}} {\\left\\| A \\vec{v}^{(0)} \\right\\|} \\\\\n",
    "& \\vdots \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "so that $\\vec{v}^{(k)}$ converges to the eigenvector $\\vec{q}_1$ corresponding the largest eigenvalue of $A$.\n",
    "\n",
    "$Algorithm$\n",
    "\n",
    "Given an initial $\\vec{v}^{(0)}$ with $\\left\\| \\vec{v}^{(0)}\\right\\| = 1$\n",
    "\n",
    "```\n",
    "for k = 1:n\n",
    "    v = A v ^{(k − 1)}\n",
    "    v ^{(k)} = v / \\|v\\|\n",
    "    \\lambda ^{(k)} = v^{(k)}' A v^{(k)}\n",
    "end\n",
    "```\n",
    "\n",
    "We can check $\\left\\| A \\vec{v}^{(k)} - \\lambda^{(k)}\\vec{v}^{(k)} \\right\\|$ so that jump out of the loop when it's sufficiently small.\n",
    "***\n",
    "$$\\left\\| \\vec{v}^{(k)} - \\vec{q}_1 \\right\\| = \\left\\| \\vec{v}^{(k-1)} - \\vec{q}_1 \\right\\|\\cdot O\\left( \\left| \\frac{\\lambda_2} {\\lambda_1} \\right| \\right) \\Longrightarrow \\left\\| \\vec{v}^{(k)} - \\vec{q}_1 \\right\\| = O\\left( \\left| \\frac{\\lambda_2} {\\lambda_1} \\right|^{k} \\right)$$\n",
    "\n",
    "$$\\left| \\lambda^{(k)} - \\lambda_1 \\right| = \\left| \\lambda^{(k-1)} - \\lambda_1 \\right|\\cdot O\\left( \\left| \\frac{\\lambda_2} {\\lambda_1} \\right|^{2} \\right) \\Longrightarrow \\left| \\lambda^{(k)} - \\lambda_1 \\right| = O\\left( \\left| \\frac{\\lambda_2} {\\lambda_1} \\right|^{2k} \\right)$$\n",
    "\n",
    "$\\dagger$ Here there's a square and it's because the conclusion from the Rayleigh Quotient. Same for the Rayleigh quotient iteration.$\\Join$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inverse iteration\n",
    "This is used to approximate an eigenvector corresponding to the eigenvalue closest to an initially specified value.\n",
    "\n",
    "Given real value $\\mu$, the eigenvalues of the matrix $\\left( A - \\mu I \\right)^{-1}$ are simply $\\left( \\lambda_i - \\mu \\right)^{-1}$ where $\\lambda_i$ are the eigenvalues of $A$. Assuming that $\\lambda_j$ is the closest one to $\\mu$, and the next one $\\lambda_l$, $i.e.$, we have the following relation\n",
    "\n",
    "$$\\left| \\frac{1} {\\lambda_j - \\mu} \\right| > \\left| \\frac{1} {\\lambda_l - \\mu} \\right| \\geq \\cdots$$\n",
    "\n",
    "So we apply the power iteration to matrix $\\left( A - \\mu I \\right)^{-1}$ and it will converge to the eigenvector of the matrix $\\left( A - \\mu I \\right)^{-1}$ corresponding to its largest eigenvalue $\\frac{1} {\\lambda_j- \\mu}$, which is the exact eigenvector $\\lambda_j$ of $A$.\n",
    "\n",
    "$Algorithm$\n",
    "\n",
    "Given an initial $\\vec{v}^{(0)}$ with $\\left\\| \\vec{v}^{(0)}\\right\\| = 1$, and a number $\\mu$\n",
    "\n",
    "```\n",
    "for k = 1:n\n",
    "    v = (A - \\mu I) ^{-1} v ^{(k − 1)}\n",
    "    v ^{(k)} = v / \\|v\\|\n",
    "    \\lambda ^{(k)} = v^{(k)}' A v^{(k)}\n",
    "end\n",
    "```\n",
    "***\n",
    "And with same logic we have the following\n",
    "\n",
    "$$\\left\\| \\vec{v}^{(k)} - \\vec{q}_j \\right\\| = \\left\\| \\vec{v}^{(k-1)} - \\vec{q}_j \\right\\|\\cdot O\\left( \\left| \\frac{\\lambda_j - \\mu} {\\lambda_l - \\mu} \\right| \\right) \\Longrightarrow \\left\\| \\vec{v}^{(k)}-\\vec{q}_j \\right\\| = O\\left( \\left| \\frac{\\lambda_j - \\mu} {\\lambda_l - \\mu} \\right|^{k} \\right)$$\n",
    "\n",
    "$$\\left| \\lambda^{(k)} - \\lambda_j \\right| = \\left| \\lambda^{(k-1)} - \\lambda_j \\right|\\cdot O\\left( \\left| \\frac{\\lambda_j - \\mu} {\\lambda_l - \\mu} \\right|^{2} \\right) \\Longrightarrow \\left| \\lambda^{(k)} - \\lambda_j \\right| = O\\left( \\left| \\frac{\\lambda_j - \\mu} {\\lambda_l - \\mu} \\right|^{2k} \\right)$$\n",
    "\n",
    "This process is more expensive because more calculations are required when solving the linear system, the first step of the iteration, $\\vec{v} = \\left(A - \\mu I \\right) ^{-1} \\cdot \\vec{v} ^{(k − 1)} \\Longleftrightarrow \\left(A - \\mu I \\right)\\vec{v} = \\vec{v} ^{(k − 1)}$. But luckly we only need to implement $LU$or Cholesky  factorization for once the entire time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rayleigh quotient iteration\n",
    "In the inverse iteration, $\\lambda^{(k)}$ converges to $\\lambda_j$ and the convergence rate depends on $\\mu$. The convergence will become faster if $\\mu$ is much closer to $\\lambda_j$.\n",
    "\n",
    "So that we replace $\\mu$ with the latest $\\lambda^{(k)}$ at each step of the inverse iteration.\n",
    "\n",
    "$Algorithm$\n",
    "\n",
    "Given an initial $\\vec{v}^{(0)}$ with $\\left\\| \\vec{v}^{(0)}\\right\\| = 1$, and a number $\\mu$\n",
    "\n",
    "```\n",
    "\\lambda ^{(0)} = v^{(0)}' A v^{(0)}\n",
    "\n",
    "for k = 1:n\n",
    "    v = (A - \\lambda ^{(k-1)} I) ^{-1} v ^{(k − 1)}\n",
    "    v ^{(k)} = v / \\|v\\|\n",
    "    \\lambda ^{(k)} = v^{(k)}' A v^{(k)}\n",
    "end\n",
    "```\n",
    "***\n",
    "$$\\begin{align}\n",
    "\\left\\| \\vec{v}^{(k)} - \\vec{q}_j \\right\\| &= \\left\\| \\vec{v}^{(k-1)} - \\vec{q}_j \\right\\|\\cdot O\\left( \\left| \\frac{\\lambda_j - \\mu} {\\lambda_l - \\mu} \\right| \\right) \\\\\n",
    "&= \\left\\| \\vec{v}^{(k-1)} - \\vec{q}_j \\right\\|\\cdot O\\left( \\left| \\frac{\\lambda_j - \\lambda ^{(k-1)}} {\\lambda_l - \\lambda ^{(k-1)}} \\right| \\right) \\\\\n",
    "&= \\left\\| \\vec{v}^{(k-1)}-\\vec{q}_j \\right\\|\\cdot O\\left( \\left| {\\lambda_j- \\lambda ^{(k-1)}} \\right| \\right) \\\\\n",
    "&= O\\left( \\left\\| \\vec{v}^{(k-1)}-\\vec{q}_j \\right\\|^3 \\right)\n",
    "\\end{align}$$\n",
    "\n",
    "$$\\left|\\lambda^{k}-\\lambda_j\\right| = O\\left( \\left\\| \\vec{v}^{(k)}-\\vec{q}_j \\right\\|^2 \\right) = O\\left( \\left\\| \\vec{v}^{(k-1)}-\\vec{q}_j \\right\\|^6\\right)$$\n",
    "\n",
    "And still it's more expensive because at each step you need to apply the $LU$ or Cholesky factorization to solve the linear system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational cost\n",
    "If $A$ is an $m \\times m$ dense matrix, then the computational cost of the [power iteration](#Power-iteration) for each step is $O(m^2)$ for multiplying a matrix with a vector. For the inverse iteration, one $LU$ factorization of $A-\\lambda I$ costs $O(m^3)$ ops, which only needs to be computed once in the inverse iteration, and then in each step of the iteration, the cost is $O(m^2)$ for the forward and backward substitutions. For the Rayleigh quotient iteration, the cost for each step is $O(m^3)$ for solving\n",
    "the system of linear equations.\n",
    "\n",
    "However, the computation cost can be greatly reduced, especially for the Rayleigh quotient iteration, if we first reduce the symmetric matrix $A$ to a tridiagonal for by a sequence of symmetric orthogonal transformations which does not change the eigenvalues of $A$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduction to Tridiagonal form\n",
    "\n",
    "We know that for a real symmetric matrix $A$, it has an eigenvalue decomposition $A = Q \\Lambda Q^{\\mathrm{T}}$, where $\\Lambda$ is a diagonal matrix which contains the eigenvalues of $A$, and $Q$ is an orthogonal matrix which contains the eigenvectors of $A$.\n",
    "\n",
    "However in general we can only multiply a sequence of orthogonal matrices to the left and their transpose to the right of $A$ such that the result is a tridiagonal matrix. And such orthogonal matrices can be determined by the\n",
    "using Householder reflector.\n",
    "\n",
    "$$\\underset{Q^{\\mathrm{T}}}{\\underbrace{Q_{m-2}\\cdots Q_{2}Q_{1}}} \\;\\; A \\;\\; \\underset{Q}{\\underbrace{Q_{1}^{\\mathrm{T}}Q_{2}^{\\mathrm{T}}\\cdots Q_{m-2}^{\\mathrm{T}}}} = T = \\left[ \\begin{array}{ccccc}\n",
    "\\times & \\times & & & \\\\\n",
    "\\times & \\times & \\times & & \\\\\n",
    "& \\times & \\times & \\times & \\\\\n",
    "& & \\times & \\times & \\times \\\\\n",
    "& & & \\times & \\times\n",
    "\\end{array}\n",
    "\\right]$$\n",
    "\n",
    "We can also see that $A$ is similar to $T$ and they have the same eigenvalues.\n",
    "\n",
    "$Algorithm$\n",
    "\n",
    "```\n",
    "for k = 1 to m - 2\n",
    "    x = A_{k+1:m,k}\n",
    "    v_{k} = x + sign( x_{1} ) \\| x \\|_{2} e_{1}\n",
    "    v_{k} = v_{k} / \\| v_{k} \\|_{2}\n",
    "    A_{k+1:m,k:m} = A_{k+1:m,k:m} - 2 v_{k} ( v_{k}' A_{k+1:m,k:m} )\n",
    "    A_{k:m,k+1:m} = A_{k:m,k+1:m}  2 ( A_{k:m,k+1:m} v_{k} ) v_{k}'\n",
    "end\n",
    "```\n",
    "\n",
    "In general, an eigenvalue problem solver first apply the above algorithm to reduce $A$ into a tridiagonal form $T$. Then an iteration after that. And for the reduction part the cost for each iteration is also $O\\left(m^{2}\\right)$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "314px",
    "width": "340px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
