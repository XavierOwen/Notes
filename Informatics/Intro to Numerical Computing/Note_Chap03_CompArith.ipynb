{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary and Decimal representation\n",
    "Usual expression:  \n",
    "\n",
    "$37.0 = 3 \\times 10^{1} + 7 \\times 10^{0}$\n",
    "\n",
    "$472.375 = 4 \\times 10^{2} + 7 \\times 10^{1} + 2 \\times 10^{0} + 3 \\times 10^{-1} + 7 \\times 10^{-2} + 5 \\times 10^{-3}$\n",
    "\n",
    "$0.625 = 6 \\times 10^{-1} + 2 \\times 10^{-2} + 5 \\times 10^{-3}$\n",
    "\n",
    "Binary expression:\n",
    "\n",
    "$\\begin{align*}\n",
    "37.0 &= 32 + 4 +1 = 1 \\times 2^{5} + 1 \\times 2^{2} + 1 \\times 2^{0} \\\\\n",
    "&= (100101)_2\n",
    "\\end{align*}$\n",
    "\n",
    "$\\begin{align*}\n",
    "472.375 &= 256 + 128 + 64 + 16 + 8 + \\frac{1} {4} + \\frac{1} {8} \\\\\n",
    "&= 1 \\times 2^{8} + 1 \\times 2^{7} + 1 \\times 2^{6} + 1 \\times 2^{4} + 1 \\times 2^{3} + 1 \\times 2^{-2} + 1 \\times 2^{-3}\\\\\n",
    "&= (111011000.011)_2\n",
    "\\end{align*}$\n",
    "\n",
    "$\\begin{align*}\n",
    "0.625 &= \\frac{1} {2} + \\frac{1} {8} = 1 \\times 2^{-1} + 1 \\times 2^{-3} \\\\\n",
    "&= (0.101)_2\n",
    "\\end{align*}$\n",
    "\n",
    "## Infinite Decimal\n",
    "$\\frac{1} {3} = 0.333333\\dots$, now we have:\n",
    "\n",
    "$\\begin{align*}\n",
    "\\frac{1} {3} &= \\frac{1} {2^{2}} + \\frac{1} {2^{4}} + \\frac{1} {2^{6}} + \\frac{1} {2^{8}} + \\dots \\\\\n",
    "&= (0.01010101\\dots)_2\n",
    "\\end{align*}$\n",
    "\n",
    "Others like $0.1$, now we have:\n",
    "\n",
    "$\\begin{align*}\n",
    "0.1 &= \\frac{1} {2^{4}} + \\frac{1} {2^{5}} + \\frac{1} {2^{8}} + \\frac{1} {2^{9}} + \\frac{1} {2^{12}} + \\frac{1} {2^{13}} + \\dots \\\\\n",
    "&= (0.0001100110011\\dots)_2\n",
    "\\end{align*}$\n",
    "\n",
    ">**e.g.** How to easily get the binary expression of a infinite decimal? Such as $0.175$.\n",
    ">\n",
    "$$ \\begin{align*}\n",
    "0.175 \\times 2 &= 0.35 < 1, & x_{-1} = 0 \\\\\n",
    "0.35  \\times 2 &= 0.7  < 1, & x_{-2} = 0 \\\\\n",
    "0.7   \\times 2 &= 1.4  > 1 \\Rightarrow 0.4, & x_{-3} = 1 \\\\\n",
    "0.4   \\times 2 &= 0.8  < 1, & x_{-4} = 0 \\\\\n",
    "0.8   \\times 2 &= 1.6  > 1 \\Rightarrow 0.6, & x_{-5} = 1 \\\\\n",
    "0.6   \\times 2 &= 1.2  > 1 \\Rightarrow 0.2, & x_{-6} = 1 \\\\\n",
    "0.2   \\times 2 &= 0.4  < 1, & x_{-7} = 0 \\\\\n",
    "& &\\text{and then falls into infinite loop} \\\\\n",
    "&0.175 = \\small( 0. \\, 001 \\underset{\\text{loop}}{\\underbrace{0110}} \\underset{\\text{loop}}{\\underbrace{0110}} \\underset{\\text{loop}}{\\underbrace{0110}} \\dots \\small)_{2} \n",
    "\\end{align*}$$\n",
    "\n",
    "***\n",
    "\n",
    ">**e.g.** How to easily get the binary expression of an integer? Such as $131$.\n",
    ">\n",
    "$$ \\begin{align*}\n",
    "131 \\div 2 &= 65 \\;\\cdots\\cdots\\; 1, & x_{1} = 1 \\\\\n",
    "65 \\div 2 &= 32 \\;\\cdots\\cdots\\; 1, & x_{2} = 1 \\\\\n",
    "32 \\div 2 &= 16, & x_{3} = 0 \\\\\n",
    "16 \\div 2 &= 8, & x_{4} = 0 \\\\\n",
    "8 \\div 2 &= 4, & x_{5} = 0 \\\\\n",
    "4 \\div 2 &= 2, & x_{6} = 0 \\\\\n",
    "2 \\div 2 &= 1, & x_{7} = 0 \\\\\n",
    "&\\;\\; \\; \\downarrow  \\\\\n",
    " x_{8} &= 1 \\\\\n",
    "&131 = \\small( 10000011 \\small)_{2} \n",
    "\\end{align*}$$\n",
    "\n",
    "\n",
    "## IEEE standard of floating point numbers\n",
    "The basic unit in the memory of a computer is **bit**. It is a unit set either *on* or *off*, corresponding to a value which is either $1$ or $0$.\n",
    "\n",
    "**Integer**: 32 bits, with the first bit as the sign. Therefore the range of integers in a 32 bit computer is\n",
    "about from $-2 \\times 10^{9} $ to $2 \\times 10^{9} $.\n",
    "\n",
    "**Real number**: floating point representation.\n",
    ">**e.g.**  \n",
    ">\n",
    ">$37.0 = (100101)_2 = (1.00101)_2 \\times 2^5$\n",
    ">\n",
    ">$472.375 = (111011000.011)_2 = (1.11011000011)_2 \\times 2^8$\n",
    ">\n",
    ">$0.625 = (0.101)_2 = (1.01)_2 \\times 2^{-1}$\n",
    ">\n",
    ">$\\frac{1} {3} = (0.0101010101\\dots)_2 = (1.01010101\\dots)_2 \\times 2^{-2}$\n",
    ">\n",
    ">$0.1 = (0.0001100110011\\dots)_2 = (1.100110011\\dots) \\times 2^{-4}$\n",
    "\n",
    "In general, a floating point number can be written as:\n",
    "$$x = \\pm \\left( d_0 + \\frac{d_1} {2} + \\frac{d_2} {2^2} + \\frac{d_3} {2^3} + \\cdots + \\frac{d_{p-1}} {2^{p-1}}\\right) \\times 2^{E}$$\n",
    "\n",
    "Here, $E$ is the *Exponent*; $d_1, d_2, \\dots, d_{p-1}$ is the *fraction*; and $d_0 = 1$ for *normalized number*, a number is normalized when it is written in scientific notation with one nonzero decimal digit before the decimal point.\n",
    "\n",
    "**IEEE Single Precision**: **32 bits** are used to store a single precision number, where the first bit is for the sign, $\\pm$, $0$ for positive real numbers and $1$ for negative real numbers. $\\left( \\overline{ a_1 a_2 a_3 \\dots a_8} \\right)_2$ is used to save the exponent part and its range is between $0$ and $255$. In order to cover real numbers less than $1$, the exponent should be able to compensate negative values. For this reason,\n",
    "\n",
    "$$\\boxed{E = \\left( \\overline{ a_1 \\, a_2 \\, a_3 \\, \\dots \\, a_8} \\right)_2 - 127}$$\n",
    "\n",
    "Then the range of $E$ will be $\\left\\{-127, -126, \\dots, 128\\right\\}, i.e.,$ from $( 0 \\, 0 \\, 0 \\, \\dots \\, 0 )_{2}$, to $( 1 \\, 1 \\, 1 \\, \\dots \\, 1 )_{2}$\n",
    "\n",
    "|   s   | exponent | fraction |\n",
    "|:-----:|:--------:|:--------:|\n",
    "| $\\pm$ | $\\overline{ a_1 \\, a_2 \\,a_3 \\, \\dots \\, a_8}$ | $\\overline{ d_1 \\, d_2 \\, d_3 \\, \\dots \\, d_{23}}$ |\n",
    "\n",
    "Normally, we donote the 32 bits as $b_i, i=0,1, \\dots, 32$, then the **normalized value** is:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{value} &= (-1)^{b_{31}} \\times \\left( 1. \\overline{ \\, b_{22} \\, b_{21} \\, \\dots \\, b_{0}} \\right)_{2} \\times 2^{\\left( \\overline{ b_{30} \\, b_{29} \\, \\dots \\,b_{23}} \\right)_{2}-127} \\\\\n",
    "&= \\boxed{ (-1)^{\\text{sign}}\\times \\left(1+\\sum _{i=1}^{23}b_{23-i} \\, 2^{-i}\\right)\\times 2^{(e-127)} }\n",
    "\\end{align*}$$\n",
    "\n",
    "At this time, exponent is encoded using an [offset-binary representation](https://en.wikipedia.org/wiki/Offset_binary), $i.e.$, an exponent bias, 127, has to be substructed.\n",
    "\n",
    "But things are different when $b_{30} \\, b_{29} \\, \\dots \\,b_{23}$, the exponent bits, are all $0$, or all $1$. \n",
    "\n",
    "1. When they are all $0$, we use the **denormalized value**, or called **subnormal numbers**.\n",
    "$$\\boxed{ \\text{value} = (-1)^{b_{31}} \\times \\left( 0. \\overline{ b_{22} \\, b_{21} \\, \\dots \\, b_{0}} \\right)_{2} \\times 2^{-126} = (-1)^{\\text{sign}}\\times \\left( \\sum _{i=1}^{23}b_{23-i} \\, 2^{-i}\\right)\\times 2^{-126} }$$\n",
    "This is used to fill the gap between $0$ and the smallest positive normalized number. The smallest subnormal number is $2^{-23} \\times 2^{-126} = 2^{-149} \\approx 1.4 \\times 10^{-45}$ and the smallest positive normalized number is $1 \\times 2^{−126} \\approx 1.18 \\times 10^{−38}$ \n",
    "2. When they are all $1$, it is not a number, $i.e.$, infinity ($\\pm \\inf$) if $b_{30} = b_{29} = \\cdots = b_{23} = 0$ , or NaN, if not $b_{30} = b_{29} = \\cdots = b_{23} = 0$. (In this situation it's the quiet or signalling NaN.)\n",
    "\n",
    "$Summary$  \n",
    "An hierarchy\n",
    "\n",
    "|                       Exponent digits                      |                                                               Value                                                              |\n",
    "|:----------------------------------------------------------:|:--------------------------------------------------------------------------------------------------------------------------------:|\n",
    "|  $( 0 \\, 0 \\, 0 \\, 0 \\, 0 \\, 0 \\,0 \\, 0 )_{2} = (0)_{10}$  |                        $\\pm \\left( 0.\\overline{ \\, d_{1} \\, d_{2} \\, d_{3} \\, \\dots \\, d_{23}} \\right)_{2} \\times 2^{-126}$                        |\n",
    "|  $( 0 \\, 0 \\, 0 \\, 0 \\, 0 \\, 0 \\,0 \\, 1 )_{2} = (1)_{10}$  |                        $\\pm \\left( 1.\\overline{ \\, d_{1} \\, d_{2} \\, d_{3} \\, \\dots \\, d_{23}} \\right)_{2} \\times 2^{1-127}$                        |\n",
    "|  $( 0 \\, 0 \\, 0 \\, 0 \\, 0 \\, 0 \\,1 \\, 0 )_{2} = (2)_{10}$  |                        $\\pm \\left( 1.\\overline{ \\, d_{1} \\, d_{2} \\, d_{3} \\, \\dots \\, d_{23}} \\right)_{2} \\times 2^{2-127}$                        |\n",
    "|  $( 0 \\, 0 \\, 0 \\, 0 \\, 0 \\, 0 \\,1 \\, 1 )_{2} = (3)_{10}$  |                        $\\pm \\left( 1.\\overline{ \\, d_{1} \\, d_{2} \\, d_{3} \\, \\dots \\, d_{23}} \\right)_{2} \\times 2^{3-127}$                        |\n",
    "|                              ↓                             |                                                                 ↓                                                                |\n",
    "| $( 0 \\, 1 \\, 1 \\, 1 \\, 1 \\, 1 \\,1 \\, 1 )_{2} = (127)_{10}$ |                       $\\pm \\left( 1.\\overline{ \\, d_{1} \\, d_{2} \\, d_{3} \\, \\dots \\, d_{23}} \\right)_{2} \\times 2^{127-127}$                       |\n",
    "| $( 1 \\, 0 \\, 0 \\, 0 \\, 0 \\, 0 \\,0 \\, 0 )_{2} = (128)_{10}$ |                       $\\pm \\left( 1.\\overline{ \\, d_{1} \\, d_{2} \\, d_{3} \\, \\dots \\, d_{23}} \\right)_{2} \\times 2^{128-127}$                       |\n",
    "|                              ↓                             |                                                                 ↓                                                                |\n",
    "| $( 1 \\, 1 \\, 1 \\, 1 \\, 1 \\, 1 \\,0 \\, 0 )_{2} = (252)_{10}$ |                       $\\pm \\left( 1.\\overline{ \\, d_{1} \\, d_{2} \\, d_{3} \\, \\dots \\, d_{23}} \\right)_{2} \\times 2^{252-127}$                       |\n",
    "| $( 1 \\, 1 \\, 1 \\, 1 \\, 1 \\, 1 \\,0 \\, 1 )_{2} = (253)_{10}$ |                       $\\pm \\left( 1.\\overline{ \\, d_{1} \\, d_{2} \\, d_{3} \\, \\dots \\, d_{23}} \\right)_{2} \\times 2^{253-127}$                       |\n",
    "| $( 1 \\, 1 \\, 1 \\, 1 \\, 1 \\, 1 \\,1 \\, 0 )_{2} = (254)_{10}$ |                       $\\pm \\left( 1.\\overline{ \\, d_{1} \\, d_{2} \\, d_{3} \\, \\dots \\, d_{23}} \\right)_{2} \\times 2^{254-127}$                       |\n",
    "| $( 1 \\, 1 \\, 1 \\, 1 \\, 1 \\, 1 \\,1 \\, 1 )_{2} = (255)_{10}$ | $\\begin{cases}\\pm \\inf & \\text{ if } d_{1} = d_{2} = d_{3} = \\cdots = d_{23} = 0 \\\\ \\text{NaN} & \\text{ otherwise } \\end{cases}$ |\n",
    "\n",
    "Other special cases:  \n",
    "\n",
    "- $0.0$ is stored as 32 digits of zeros.\n",
    "- The smallest normalized positive number is $(1.000 \\dots 0)_{2} \\times 2^{-126}$, which is approximately $1.18 \\times  10^{-38}$\n",
    "- The smallest normalized positive number is $(1.111 \\dots 1)_{2} \\times 2^{+127}$, which is approximately $3.4 \\times  10^{38}$\n",
    "- *machine epsilon*: from 1.0 to next real number that 32 bits computer can express:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\epsilon_{\\textrm{mach}} &= (1.00000000000000000000001)_{2} \\times 2^{(01111111)_{2}-127} - 1.0 \\\\\n",
    "&= (1.00000000000000000000001)_{2} \\times 2^{0} - (1.00000000000000000000000)_{2} \\times 2^{0} \\\\\n",
    "&= 2^{-23} \\approx 1.2 \\times 10^{-7}\n",
    "\\end{align*}$$\n",
    "(and for the machine epsilon for 64 bits computer, which has 52 digits of fraction, its $\\epsilon_{\\textrm{mach}}$ is $2^{-52} \\approx 2.0 \\times 10^{-16}$)  \n",
    "\n",
    "*And this is the reason why we say there are 8 digits precision for the single precision, and 16 digits for the double precision.*\n",
    "\n",
    "## Rounding Error\n",
    "$Def$  \n",
    "$$\\textbf{Absolute error} = \\big| \\text{approximate solution} - \\text{true solution} \\big|,\n",
    "\\textbf{Relative error} = \\frac{\\big| \\text{absolute error} \\big|}  {\\big| \\text{true solution} \\big|}$$\n",
    "\n",
    "$\\odot$The relative error makes more sense.$\\square$\n",
    "\n",
    "Notice that error is inevitable due to the infinite number of real numbers and only a finite number of units in the computer memory, and, the gaps between neighboring floating point numbers are finer when they are close zero and are much sparser when they are away from zero.\n",
    "\n",
    "A given real number $x$ can only be represented approximated as a 'nearby' floating-point number, denoted as $fl(x)$.\n",
    "$$x = \\pm \\left( 1.\\overline{\\, d_{1} \\, d_{2} \\, d_{3} \\dots d_{23} \\, d_{24} \\, d_{25} \\, \\dots} \\right)_{2} \\times 2^E, \\\\ fl(x) = \\pm \\left( 1.\\overline{\\, d_{1} \\, d_{2} \\, d_{3} \\dots d_{23}} \\right)_{2} \\times 2^E$$\n",
    "\n",
    "which only takes the first 23 digits of $x$ in a single precision.\n",
    "\n",
    "**Rounding error**: the diffrence, $\\left|\\, x - fl(x) \\right|$\n",
    "\n",
    "$Conclusion$\n",
    "\n",
    "$$\\boxed{ \\left|\\, x - fl(x) \\right| \\leq \\small( 0. \\, \\underset{23\\textrm{ digit}}{\\underbrace{000 \\dots 001}} \\: \\small)_{2} \\times 2^E = 2^{(E - 23)} }$$\n",
    "\n",
    "$$\\boxed{ \\frac{\\left|\\, x - fl(x) \\right|} {\\left| x\\right|} \\leq  \\frac{2^{(E - 23)}} {2^E} = 2^{-23} = \\epsilon_{\\text{mach}} }$$\n",
    "\n",
    "$i.e.$,\n",
    "\n",
    "Relative rounding error is always bounded by $\\epsilon_{\\text{mach}}$, and for any real number $x$, there exists a $\\epsilon$ with $|\\epsilon| < \\epsilon_{\\text{mach}}$ sach such that $fl(x) = x + x \\ast O \\left( \\epsilon_{\\text{mach}} \\right) = x + x \\ast \\epsilon$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cancelation and lost of precision\n",
    ">**e.g.** $x = 3.4159265358979, y = 3.14159265358568$. What's the subtraction in 32 bits computer\n",
    ">\n",
    ">Of course the true value is $ans = x - y = 4.1100000000000000\\times 10^{-12} $\n",
    ">\n",
    ">Then denote $h_{i} = d_{i} - g_{i}$, easily to see that\n",
    ">\n",
    ">$$\\begin{align*}\n",
    "x-y &= \\small(0.\\,\\underset{21 \\text{digits}}{\\underbrace{00\\dots 0}} \\overline{\\,h_{22}h_{23}h_{24}h_{25}\\dots} \\,\\small)_{2} \\,\\times\\, 2^{E} \\\\\n",
    "&= \\small(\\overline{h_{22}.h_{23}h_{24}h_{25}\\dots} \\,\\small)_{2} \\,\\times\\, 2^{E-22}\n",
    "\\end{align*}$$\n",
    ">\n",
    ">However in MATLAB, \n",
    ">\n",
    ">$$\\begin{align*}\n",
    "z &= x \\ominus y = fl(x) - fl(y) = \\small(0.\\,\\underset{21 \\text{digits}}{\\underbrace{00\\dots 0}} \\overline{\\,h_{22}h_{23}} \\,\\small)_{2} \\,\\times\\, 2^{E} \\\\\n",
    "&= \\small( \\overline{ h_{22}.h_{23}}00\\dots0  \\, \\small)_{2} \\,\\times\\, 2^{(E-22)}\n",
    "\\end{align*}$$\n",
    ">\n",
    "> so the difference between the true value of $x-y$ and the computed value $z$ is\n",
    ">\n",
    ">$$\\small(0.\\,0\\overline{h_{24}h_{25}\\dots} \\,\\small)_{2} \\,\\times\\, 2^{E-22}$$\n",
    "\n",
    "And another example is when we do addition\n",
    "\n",
    ">**e.g.**\n",
    ">\n",
    ">$$x = 1.\\, \\overline{d_1\\,d_2\\,\\dots\\,d_{21}d_{22}d_{23}} \\times 2^E, y = 1.\\, \\overline{ g_1 \\, g_2 \\, \\dots \\, g_{21} g_{22}g_{23}} \\times 2^{E - 24}$$\n",
    ">\n",
    "> The computer need first match their components and then it will add the following values in fact, so they changed into: \n",
    ">\n",
    ">$$\\tilde{x}=1.\\, \\overline{d_1\\,d_2\\,\\dots\\,d_{21}d_{22}d_{23}} \\times 2^E, \\tilde{y}=0.\\, \\underset{\\text{23 digits}}{\\underbrace{00\\dots000}} \\times 2^{E-24}$$\n",
    ">\n",
    ">where the meaningful digits, $g_1 \\, g_2 \\, \\dots \\, g_{21} \\, g_{22} \\, g_{23}$, in $y$ are lost in $\\tilde{y}$ while shifting the digit to match with the component of $x$\n",
    ">\n",
    ">Like a single precision computer with $8$ digits of precision, it holds that $1.0 \\oplus 10^{-9} = 1.0$\n",
    "\n",
    "## Floating point arithmetic\n",
    "In IEEE standard, it is always satisfied that, for any two floating point numbers $x$ and $y$, we have:\n",
    "\n",
    "$$\n",
    "x \\oplus y = \\left( x + y \\right) \\left( 1 + \\epsilon \\right) \\\\\n",
    "x \\ominus y = \\left( x - y \\right) \\left( 1 + \\epsilon \\right) \\\\\n",
    "x \\otimes y = \\left( x \\times y \\right) \\left( 1 + \\epsilon \\right) \\\\\n",
    "x \\oslash y = \\left( x \\; / \\; y \\right) \\left( 1 + \\epsilon \\right) \n",
    "$$\n",
    "\n",
    "Where $|\\epsilon| < \\epsilon_{\\text{mach}}$\n",
    "\n",
    "## Taylor's theorem and truncation error\n",
    "$Theorem$\n",
    "**Taylor's theorem**: given continuous real function $f(x)$, assume $f(x)$ is smooth in a neighborhood of $x$, then $f(x + h)$:\n",
    "\n",
    "$$f(x + h) = f(x) + hf'(x) + \\frac{h^2} {2}f''(x) + \\cdots + \\frac{h^n} {n!}f^{\\left( n \\right)}(x) + \\frac{h^{n+1}} {\\left( n + 1 \\right)!}f^{\\left( n+1 \\right)}(\\theta)$$\n",
    "\n",
    "where $\\theta \\in \\left[ x, x+h \\right]$. \n",
    "\n",
    "$Def$\n",
    "\n",
    "And if we approximate $f(x + h)$ by the first $n + 1$ terms on the right side, then we call\n",
    "\n",
    "$$\\frac{h^{n+1}} {\\left( n + 1 \\right)!}f^{\\left( n+1 \\right)}(\\theta)$$\n",
    "\n",
    "as the **truncation error** of this approximation.\n",
    "\n",
    ">**e.g.** take $n=1$,  \n",
    ">\n",
    ">$$f(x + h) = f(x) + hf'(x) + \\frac{h^2} {2}f''( \\theta )$$\n",
    ">\n",
    ">Then \n",
    ">$$f'(x) = \\frac{f(x + h) - f(x)} {h} - \\frac{f''( \\theta )} {2} h$$\n",
    ">\n",
    ">We call $\\displaystyle \\frac{f(x + h) - f(x)} {h}$ a finite difference approximatin of the derivative $f'(x)$. And the truncation error here is $\\displaystyle \\frac{f''( \\theta )} {2}$. Meaning that:\n",
    ">\n",
    ">$\\odot$\n",
    ">If $h$ is reduced to $0$, then the truncation error also decreases to $0$ and therefore an accurate approximation of $f'(x)$ is obtained from $\\displaystyle \\frac{f(x + h) - f(x)} {h}$.\n",
    "\n",
    "However, in real computer computation, due to the rounding error,  a very tiny $h$ does not give satisfactory result. When we compute $\\displaystyle \\frac{f(x + h) - f(x)} {h}$, it in fact computer:\n",
    "\n",
    "$$\\frac{fl \\left[ \\, f \\Big( fl\\small[ x + h \\small] \\big) \\, \\right] \\; - \\; fl\\left[\\, f \\Big( fl\\small[ x \\small] \\big) \\right]} {h} := f^{\\ddagger}(x)$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "f^{\\ddagger}(x) =&\\; \\frac{ f \\big( fl\\small[ x + h \\small] \\big) \\cdot \\small(1 + \\epsilon_1 \\small) \\; - \\;  f \\big( fl\\small[ x \\small] \\big) \\cdot \\small(1 + \\epsilon_2 \\small) } {h} \\\\\n",
    "=&\\; \\frac{ f \\left( \\left( x + h \\right) \\left( 1 + \\epsilon_3 \\right) \\right) \\cdot \\small(1 + \\epsilon_1 \\small) \\; - \\; f \\left( x \\left( 1 + \\epsilon_4 \\right) \\right) \\cdot \\small(1 + \\epsilon_2 \\small)} {h} \\\\\n",
    "=&\\; \\frac{ \\Big( f \\left( x + h \\right) + \\left( x + h \\right) \\epsilon_3 f'\\left( x + h \\right) + \\frac{\\left( x + h \\right)^2 \\, \\epsilon_3 ^2} {2} f''\\left( \\theta_1 \\right)  \\Big) \\small(1 + \\epsilon_1 \\small) } {h} \\\\ &\\;\\;- \\frac{  \\Big( f \\left( x \\right) + x\\,\\epsilon_4\\,f'(x) + \\frac{ x^2 \\epsilon_4^2 } {2} f''\\left( \\theta_2 \\right)  \\Big) \\small(1 + \\epsilon_2 \\small) } {h} \\\\\n",
    "\\approx &\\; \\frac{ \\Big( f \\left( x + h \\right) + \\left( x + h \\right) \\epsilon_3 \\, f'\\left( x + h \\right) + O\\left( \\epsilon_3^2 \\right)  \\Big) \\, \\small(1 + \\epsilon_1 \\small)} {h} \\\\ &\\;\\;- \\frac{\\Big( f \\left( x \\right) + x \\, \\epsilon_4 \\, f'(x) + O\\left( \\epsilon_4^2 \\right)  \\Big)  \\small(1 + \\epsilon_2 \\small)} {h} \\\\\n",
    "=&\\; \\frac{f \\left( x + h \\right) \\; - \\; f \\left( x \\right)} {h} + \\frac{ \\epsilon_1 f \\left( x + h \\right) \\; - \\; \\epsilon_2 f \\left( x \\right)} {h} \\\\ &\\;\\;+ \\frac{\\left( x + h \\right) \\epsilon_3 \\, f'\\left( x + h \\right) \\, \\small(1 + \\epsilon_1 \\small) \\; - \\; x \\, \\epsilon_4 \\, f'(x) \\, \\small(1 + \\epsilon_2 \\small)} {h} + \\frac{O\\left( \\epsilon^2_{\\text{mach}} \\right)} {h} \\\\\n",
    "=&\\; \\frac{f \\left( x + h \\right) \\; - \\; f \\left( x \\right)} {h} + \\frac{ \\epsilon_1 \\left( f(x) + h\\,f'(x) + o(h^2) \\right) \\; - \\; \\epsilon_2 f \\left( x \\right)} {h} \\\\ &\\;\\;+ \\frac{\\left( x + h \\right) \\epsilon_3 \\, \\left( f'(x) + h\\,f''(x) + o(h^2) \\right) \\, \\small(1 + \\epsilon_1 \\small) \\; - \\; x \\, \\epsilon_4 \\, f'(x) \\, \\small(1 + \\epsilon_2 \\small)} {h} + \\frac{O\\left( \\epsilon^2_{\\text{mach}} \\right)} {h} \\\\\n",
    "=& \\; \\frac{f \\left( x + h \\right) \\; - \\; f \\left( x \\right)} {h} + \\frac{ \\epsilon_1 \\, f(x) \\; - \\; \\epsilon_2 \\, f \\left( x \\right)} {h} + \\frac{x \\left( \\epsilon_3 - \\epsilon_4 \\right)f'(x)} {h} + \\frac{O\\left( \\epsilon^2_{\\text{mach}} \\right)} {h} \\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "After we discard the last part, move constant $h$ into the big $O$ notation, $i.e.$,\n",
    "\n",
    "$$\\boxed{ f^{\\ddagger}(x) = \\frac{f \\left( x + h \\right) \\; - \\; f \\left( x \\right)} {h} + \\max \\{ xf'(x), f(x) \\} O\\left( \\frac{\\epsilon_{\\text{mach}}} {h} \\right)}$$\n",
    "\n",
    "\n",
    "Therefore the true difference between $f′(x)$ and the computed $f^{\\ddagger}(x)$ is:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{Total Numerical Error} &= f'(x) - \\frac{fl \\left[ \\, f \\Big( fl\\small[ x + h \\small] \\big) \\, \\right] \\; - \\; fl\\left[\\, f \\Big( fl\\small[ x \\small] \\big) \\right]} {h}\\\\\n",
    "&= f'(x) - \\frac{f(x + h) - f(x)} {h} + \\max \\{ xf'(x), f(x) \\} O\\left( \\frac{\\epsilon_{\\text{mach}}} {h} \\right) \\\\\n",
    "&= -\\frac{f''(\\theta)} {2} h + \\max \\{ xf'(x), f(x) \\} O\\left( \\frac{\\epsilon_{\\text{mach}}} {h} \\right) \\\\\n",
    "&= \\text{Truncation Error} + \\text{Rounding Error}\n",
    "\\end{align*}$$\n",
    "\n",
    "This explains why the error will go up when $h$ is very close to zero, due to $\\text{Rounding Error}$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "109px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
