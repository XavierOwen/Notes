{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial interpolation and data-fitting\n",
    "\n",
    "Given a set of * **equally spaced** point* $t_1, t_2, \\dots, t_m$, and *data* $y_1, y_2, \\dots, y_m$, determine a polynomial of degree $m-1$: $p(t) = x_1 + x_2 t + x_3 t^2 + \\cdots + x_m t^{m-1}$, such that\n",
    "\n",
    "$$p(t_i) = y_i, i = 1, 2, \\dots , m$$\n",
    "\n",
    "This problem can be written in terms of solving the following system of linear equations\n",
    "\n",
    "$$\\left\\{ \n",
    "\\begin{array}{ccccccccccc}\n",
    "x_1 & + & x_2 t_1 & + & x_3 t_1^2 & + & \\cdots  & + & x_m t_1 ^{m-1} & = & y_1 \\\\\n",
    "x_1 & + & x_2 t_2 & + & x_3 t_2^2 & + & \\cdots  & + & x_m t_2 ^{m-1} & = & y_2 \\\\\n",
    "x_1 & + & x_2 t_3 & + & x_3 t_3^2 & + & \\cdots  & + & x_m t_3 ^{m-1} & = & y_3 \\\\\n",
    "&&&&\\vdots& &&&&=& \\vdots \\\\\n",
    "x_1 & + & x_2 t_m & + & x_3 t_m^2 & + & \\cdots  & + & x_m t_m ^{m-1} & = & y_m \\\\\n",
    "\\end{array}\n",
    "\\right .$$\n",
    "\n",
    "or in matrix form\n",
    "\n",
    "$$\\left[\n",
    "\\begin{array}{ccccc}\n",
    "1 & t_1 & t_1^2 & \\cdots & t_1^{m-1} \\\\\n",
    "1 & t_2 & t_2^2 & \\cdots & t_2^{m-1} \\\\\n",
    "1 & t_3 & t_3^2 & \\cdots & t_3^{m-1} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & t_m & t_m^2 & \\cdots & t_m^{m-1} \n",
    "\\end{array}\n",
    "\\right]\\left[\n",
    "\\begin{array}{c}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3 \\\\\n",
    "\\vdots \\\\\n",
    "x_m \n",
    "\\end{array}\n",
    "\\right]\n",
    "=\\left[\n",
    "\\begin{array}{c}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "y_3 \\\\\n",
    "\\vdots \\\\\n",
    "y_m \n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "Here $(t_i,y_i)$ are all given for $i = 1,2,\\dots,m$. Only the polynomial coefficients $x_1, x_2, \\dots, x_m$ are unknown. The matrix above is obviously a Vandermonde matrix, which is invertible as long as $t_i \\neq t_j$, for $i \\neq j$. It also implies the uniqueness of the solution of the system.\n",
    "\n",
    "However, when there're many points to fit, the interpolation polynomial $p(t)$ will have a high degree, and become a ill conditioned problems.\n",
    "\n",
    "And to fix this, we can seek a polynomial of degree much less than $m-1$, like $n-1$ with $n\\ll m$\n",
    "\n",
    "|$$m=20,n=20$$|$$m=20,n=10$$|\n",
    "|:-:|:-:|\n",
    "|![](./Raw/m20n20.png)|![](./Raw/m20n10.png)|\n",
    "\n",
    "But it is impossible for a polynomial of degree $n-1$ to satisfy $m$ interpolation conditinos, since to satisfy such equations we need to solve:\n",
    "\n",
    "$$\\left[\n",
    "\\begin{array}{ccccc}\n",
    "1 & t_1 & t_1^2 & \\cdots & t_1^{n-1} \\\\\n",
    "1 & t_2 & t_2^2 & \\cdots & t_2^{n-1} \\\\\n",
    "1 & t_3 & t_3^2 & \\cdots & t_3^{n-1} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & t_m & t_m^2 & \\cdots & t_m^{n-1} \n",
    "\\end{array}\n",
    "\\right]\\left[\n",
    "\\begin{array}{c}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3 \\\\\n",
    "\\vdots \\\\\n",
    "x_n \n",
    "\\end{array}\n",
    "\\right]\n",
    "=\\left[\n",
    "\\begin{array}{c}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "y_3 \\\\\n",
    "\\vdots \\\\\n",
    "y_m \n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "which may be without a solution. What to do now?$\\newcommand{\\ffrac}{\\displaystyle\\frac}$\n",
    "\n",
    "We don't require the $p(t)$ of degree $n-1$ to interpolate exactly given $m$ points, rather, we need a $p(t)$ such that the difference $\\displaystyle \\sum_{i=1}^{m} \\left|p(t_i) - y_i \\right|^2$ is minimized. This is the **linear least squares problem**. In other words, we need to find the polynomial with coefficients $x_1, x_2, \\dots, x_n$, that minimize the residual of the system of linear equations. After that $x_1, x_2, \\dots, x_n$ are called the **linear squares solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear least squares problem\n",
    "\n",
    "Given matrix $A_{m \\times n}$, $m \\geq n$, full rank, and a vector $\\vec{b} \\in \\mathbb{R}^{m}$, so generally there's no solution for equation $A\\vec{x} = \\vec{b}$. So instead we consider the following linear least squares problem:\n",
    "\n",
    "$$\\min_{\\vec{x} \\in \\mathbb{R}^n}\\|\\vec{b} - A\\vec{x}\\|_2^2$$\n",
    "\n",
    "the solution $\\vec{x}^*$ would minimize $\\|\\vec{b} - A\\vec{x}\\|_2$. Here's a picture to get the initial idea:\n",
    "\n",
    "![](./Raw/leastSquaresSolution.png)\n",
    "\n",
    "So the residual $\\vec{r} = \\vec{b} - A \\vec{x}^*$ must be perpendicular to the $\\DeclareMathOperator*{\\range}{range}\\DeclareMathOperator*{\\rank}{rank} \\range(A)$\n",
    "\n",
    "$Theorem$ 1\n",
    "\n",
    "Given an $m \\times n$ matrix $A$, with $m > n$ and $\\rank(A) = n$, and another vector $\\vec{b}\\in \\mathbb{R}^{m}$, then a vector $\\vec{x}^* \\in \\mathbb{R}^n$ minimizes $\\|\\vec{b} - A\\vec{x}\\|_2^2$, thereby solving the least squares problem, $iff$ one of these three equivalent conditions are satisfied\n",
    "1. $\\vec{b} - A \\vec{x}^* \\perp \\range(A)$, that is $A^{\\mathrm{T}}\\left( \\vec{b} - A \\vec{x}^* \\right) = \\vec{0} $\n",
    "2. $A^{\\mathrm{T}}A\\vec{x}^* = A^{\\mathrm{T}}\\vec{b}$\n",
    "3. $A \\vec{x}^* $ is the orthogonal projection of $\\vec{b}$ onto $\\range(A)$.\n",
    "\n",
    "$Proof$\n",
    "\n",
    "- First equivalence\n",
    "\n",
    ">First we prove: *condition 1* $\\Longrightarrow \\vec{x}^*$ minimizes $\\|\\vec{b} - A\\vec{x}\\|_2^2$\n",
    "\n",
    ">We choose one vector in $\\range(A)$, which is $A(\\vec{x}^* - \\vec{x})$, here $\\vec{x}$ is any vector in $\\mathbb{R}^n$. So that follow the *condition 1* we have $\\vec{b} - A \\vec{x}^* \\perp A(\\vec{x}^* - \\vec{x})$. So that we have\n",
    "\n",
    ">$$\\begin{align}\n",
    "&\\|\\vec{b} - A\\vec{x}\\|_2^2 = \\left\\|\\vec{b} - A\\vec{x}^* + A\\left( \\vec{x}^* - \\vec{x} \\right)\\right\\|_2^2 \\\\ \\stackrel{\\;\\;\\vec{b} - A \\vec{x}^* \\perp \\,A(\\vec{x}^* - \\vec{x})}{=} \\;\\;&\\|\\vec{b} - A\\vec{x}^*\\|_2^2 + \\left\\| A\\left( \\vec{x}^* - \\vec{x} \\right) \\right\\|_2^2 \\geq \\|\\vec{b} - A\\vec{x}^*\\|_2^2 \n",
    "\\end{align}$$\n",
    "\n",
    ">Therefore, $\\vec{x}^*$ minimizes $\\|\\vec{b} - A\\vec{x}\\|_2^2$\n",
    "\n",
    ">Then we prove: *condition 1* $\\Longleftarrow \\vec{x}^*$ minimizes $\\|\\vec{b} - A\\vec{x}\\|_2^2$\n",
    "\n",
    ">Assume not, then we can find a different vector $\\vec{y} \\in \\mathbb{R}^n$ such that $\\vec{b} - A \\vec{y}$ is orthogonal to $\\range(A)$, by solving the equation:\n",
    "\n",
    ">$$A\\vec{y} = P \\vec{b}$$\n",
    "\n",
    ">where $P\\vec{b}$ represents the orthogonal projection of $\\vec{b}$ to $\\range(A)$. So that \n",
    "\n",
    ">$$\\begin{array}{rl}\n",
    "& \\vec{b} - A \\vec{y} = \\vec{b} - P \\vec{b} \\perp \\range(A) \\\\[0.5em]\n",
    "\\Rightarrow & \\vec{b} - A \\vec{y} \\neq \\vec{b} - A \\vec{x}^*  \\\\[0.5em]\n",
    "\\Rightarrow & A\\left( \\vec{x}^* - \\vec{y} \\right) \\neq \\vec{0}  \\\\[0.6em]\n",
    "\\Rightarrow & \\begin{align*}\n",
    "\\left\\|\\vec{b} - A\\vec{x}^*\\right\\|_2^2 &= \\left\\|\\vec{b} - A\\vec{y} - A\\left( \\vec{x}^* - \\vec{y} \\right) \\right\\| _2^2 \\\\\n",
    "&= \\left\\|\\vec{b} - A\\vec{y}\\right\\|_2^2 + \\left\\|A\\left( \\vec{x}^* - \\vec{y} \\right)\\right\\|_2^2 \\\\\n",
    "&> \\left\\|\\vec{b} - A\\vec{y}\\right\\|_2^2\n",
    "\\end{align*}\n",
    "\\end{array}$$\n",
    "\n",
    ">This contradiction shows that $\\vec{b} - A \\vec{x}^*$ must be orthogonal $\\range(A)$.\n",
    "\n",
    "- First and Second Equivalence\n",
    "\n",
    ">It's trivial that they are equivalent:\n",
    "\n",
    ">$$A^{\\mathrm{T}}A\\vec{x}^* = A^{\\mathrm{T}}\\vec{b} \\Longleftrightarrow A^{\\mathrm{T}}\\left( \\vec{b} - A \\vec{x}^* \\right) = \\vec{0} \n",
    "$$\n",
    "\n",
    "- Second and Third Equivalence\n",
    "\n",
    "> Still we can prove that they are equivalent:\n",
    "\n",
    "> $$\\begin{array}{rrl}\n",
    "& A^{\\mathrm{T}}A\\vec{x}^* \\!\\!\\!\\!\\!&= A^{\\mathrm{T}}\\vec{b} \\\\[0.4em]\n",
    "\\Leftrightarrow \\!\\!\\!& \\vec{x}^* \\!\\!\\!\\!\\!&= \\left( A^{\\mathrm{T}}A \\right) A^{\\mathrm{T}}\\vec{b} \\\\[0.4em]\n",
    "\\Leftrightarrow \\!\\!\\!& A\\vec{x}^* \\!\\!\\!\\!\\!&= A\\left( A^{\\mathrm{T}}A \\right) A^{\\mathrm{T}}\\vec{b}\n",
    "\\end{array}$$\n",
    "\n",
    "> Since $A$ is full rank, so that the inverse exists. And we've already known that $A\\left( A^{\\mathrm{T}}A \\right) A^{\\mathrm{T}}\\vec{b}$ is just the orthogonal projection of $\\vec{b}$ onto $\\range(A)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditioning of linear least squares problems\n",
    "The condition number of solving a linear system $A\\vec{x} = \\vec{b}$ is $\\DeclareMathOperator*{\\Cond}{Cond} \\Cond(A) = \\big\\|A\\big\\|_2\\left\\|A^{-1}\\right\\|_2$, when $A$ is a square matrix.\n",
    "\n",
    "So what's the condition number of solving the least squares problem $\\displaystyle \\min_{\\vec{x}\\in\\mathbb{R}^n} \\left\\| \\vec{b} - A \\vec{x} \\right\\|_2$ for a given $m \\times n$ matrix $A$, with $m > n$ and $\\rank(A) = n$. From the **theorem 1**, we can see that the solution would be \n",
    "\n",
    "$$\\vec{x}^* = \\left( A^{\\mathrm{T}}A \\right)^{-1} A^{\\mathrm{T}} \\vec{b}$$\n",
    "\n",
    "where $A$ is column full rank matrix so that $A^{\\mathrm{T}}A$ is invertible. To complete the explanation, now we define the **pseudoinverse** of $A$ as $A^+:=\\left( A^{\\mathrm{T}}A \\right)^{-1}A^{\\mathrm{T}}$. So that now we can shorten the expression like\n",
    "\n",
    "$$\\vec{x}^* = \\left( A^{\\mathrm{T}}A \\right)^{-1} A^{\\mathrm{T}} \\vec{b} = A^+ \\vec{b}$$\n",
    "\n",
    "And in this situation the condition number will be\n",
    "\n",
    "$$\\Cond(A) = \\big\\|A\\big\\|_2\\left\\|A^{+}\\right\\|_2$$\n",
    "\n",
    "There's another point: the conditioning of projecting $\\vec{b}$ to $\\range(A)$.\n",
    "\n",
    "Define $\\theta  := \\small\\langle \\vec{b},\\range(A) \\small\\rangle$ as the angle between $\\vec{b}$ and its projection on $\\range(A)$, ranging from $-\\ffrac{\\pi} {2}$ to $\\ffrac{\\pi} {2}$.\n",
    "- If $\\theta \\approx 0$, then a small relative change in $\\vec{b}$ will just lead to a similarly small relative change in $P\\vec{b}$, well-conditioned!\n",
    "- If $\\theta \\approx \\ffrac{\\pi} {2}$, then a small relative change in $\\vec{b}$ will just lead to a relatively large change in $P\\vec{b}$, ill-conditioned!\n",
    "\n",
    "Based on the above obeservation, we have the following theorem\n",
    "\n",
    "$Thorem$ 2\n",
    "\n",
    "The condition number of solving the linear least squares problem is bounded by\n",
    "\n",
    "$$\\frac{\\Cond(A)} {\\cos \\theta}$$\n",
    "\n",
    "$Proof$\n",
    "\n",
    "$$\\begin{cases}\n",
    "\\;\\vec{x} = A^{+} \\vec{b} \\\\\n",
    "\\left( \\vec{x} + \\Delta \\vec{x} \\right) = A^{+} \\left( \\vec{b} + \\Delta \\vec{b} \\right) \\\\\n",
    "\\end{cases} \\Longrightarrow \\Delta \\vec{x} = A^{+} \\Delta \\vec{b}$$\n",
    "\n",
    "So that the condition number of solution $\\vec{x}$ with respect to the perturbations in $\\vec{b}$ is\n",
    "\n",
    "$$\\begin{align}\n",
    "\\kappa_{\\vec{b} \\to \\vec{x}} &= \\sup_{\\Delta \\vec{b} \\neq \\vec{0}} \\frac{\\left\\| \\Delta \\vec{x} \\right\\| / \\left\\| \\vec{x} \\right\\|} {\\left\\| \\Delta \\vec{b} \\right\\|\\big/\\left\\| \\vec{b} \\right\\|} \\\\\n",
    "&= \\sup_{\\Delta \\vec{b} \\neq \\vec{0}} \\frac{\\left\\| A^{+} \\cdot \\Delta \\vec{b} \\right\\| \\big/ \\left\\| \\vec{x} \\right\\|} {\\left\\| \\Delta \\vec{b} \\right\\|\\big/\\left\\| \\vec{b} \\right\\|} \\\\\n",
    "&= \\frac{\\left\\| A^{+} \\right\\| \\cdot \\left\\| \\vec{b} \\right\\|} {\\left\\| \\vec{x} \\right\\|} = \\left\\| A^{+} \\right\\| \\cdot \\frac{\\left\\| \\vec{b} \\right\\|} {\\left\\| P\\vec{b} \\right\\|} \\cdot \\frac{ \\left\\| P\\vec{b} \\right\\|} {\\left\\| \\vec{x} \\right\\|} \\\\\n",
    "&= \\left\\| A^{+} \\right\\| \\cdot \\frac{1} {\\cos \\theta} \\cdot \\frac{ \\left\\| A\\vec{x} \\right\\|} {\\left\\| \\vec{x} \\right\\|} \\\\\n",
    "&\\leq \\frac{\\left\\| A^{+} \\right\\| \\cdot \\left\\| A \\right\\|} {\\cos \\theta} = \\frac{\\Cond(A)} {\\cos \\theta}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms for solving linear least squares problems\n",
    "## Solve normal equation\n",
    "From $Theorem$ 1, we know that the solution to the least squares problem satisfies the normal equation\n",
    "\n",
    "$$A^{\\mathrm{T}}A \\vec{x} = A^{\\mathrm{T}}\\vec{b}$$\n",
    "\n",
    "And since $A$ is full rank, so that $A^{\\mathrm{T}}A$ is symmetric positive definite, and it has a Cholesky Factorization\n",
    "\n",
    "$$A^{\\mathrm{T}}A = LL^{\\mathrm{T}}$$\n",
    "\n",
    "1. Form $A^{\\mathrm{T}}A$ and $A^{\\mathrm{T}}\\vec{b}$\n",
    "2. Factorize $A^{\\mathrm{T}}A = LL^{\\mathrm{T}}$, using the build-in function `chol`\n",
    "3. Solve $LL^{\\mathrm{T}} \\vec{x} = A^{\\mathrm{T}} \\vec{b}$, by backward/forward substitution\n",
    "\n",
    "## By QR factorization\n",
    "Denote the reduced $QR$ factorization of $A$ by $A = \\hat{Q} \\hat{R}$, where $\\hat{Q}$ is a $m \\times n$ matrix with orthonormal columns, and $\\hat{R}$ is a $n \\times n$ upper triangular matrix. Then, from $A^{\\mathrm{T}}A \\vec{x} = A^{\\mathrm{T}}\\vec{b}$, so that\n",
    "\n",
    "$$\\hat{R}^{\\mathrm{T}} \\hat{Q}^{\\mathrm{T}} \\hat{Q} \\hat{R} \\vec{x} = \\hat{R}^{\\mathrm{T}} \\hat{Q}^{\\mathrm{T}} \\vec{b}$$\n",
    "\n",
    "which implies\n",
    "\n",
    "$$\\hat{R} \\vec{x} = \\hat{Q}^{\\mathrm{T}} \\vec{b}$$\n",
    "\n",
    "1. Compute the $QR$ factorization $A = \\hat{Q} \\hat{R}$, by Housholder/Modified Gram-Schmidt\n",
    "2. Computer $\\hat{Q}^{\\mathrm{T}} \\vec{b}$\n",
    "3. Solve $\\hat{R} \\vec{x} = \\hat{Q}^{\\mathrm{T}} \\vec{b}$, by backward substitution\n",
    "***\n",
    "\n",
    "Another way to derive the equation: Since $A\\vec{x}$ is the orthogonal projection of $\\vec{b}$ to the $\\range(A)$:\n",
    "\n",
    "$$A\\vec{x} = \\hat{Q}\\hat{Q}^{\\mathrm{T}} \\vec{b} \\Longrightarrow \\hat{R} \\vec{x} = \\hat{Q}^{\\mathrm{T}} \\vec{b}$$\n",
    "\n",
    "\n",
    "## By singular value decomposition\n",
    "The reduced singular value decomposition of a full rank $m \\times n$ matrix $A$ can be written as\n",
    "\n",
    "$$A = \\hat{U} \\hat{\\Sigma} V^{\\mathrm{T}}$$\n",
    "\n",
    "where $\\hat{U}$ is an $m \\times n$ matrix with orthonormal columns, $\\hat{\\Sigma}$ is an $n \\times n$ diagonal matrix with singular values of $A$ on its diagonal, and $V$ is an $n \\times n$ orthogonal matrix. So that\n",
    "\n",
    "$$\n",
    "A^{\\mathrm{T}}A\\vec{x} = V\\Sigma^{\\mathrm{T}}\\hat{U}^{\\mathrm{T}}\\hat{U} \\hat{\\Sigma} V^{\\mathrm{T}}\\vec{x} = V \\Sigma\\hat{U}^{\\mathrm{T}} \\vec{b} = A^{\\mathrm{T}}\\vec{b}\n",
    "\\\\[0.8em]\n",
    "\\vec{x} = {V^{\\mathrm{T}}}^{-1} \\hat{\\Sigma}^{-1} \\hat{U}^{-1} \\vec{b} = V \\hat{\\Sigma}^{-1} \\hat{U}^{\\mathrm{T}} \\vec{b}\n",
    "$$\n",
    "\n",
    "1. Compute reduced SVD $A = \\hat{U} \\hat{\\Sigma} V^{\\mathrm{T}}$\n",
    "2. $\\vec{x} = V \\hat{\\Sigma}^{-1} \\hat{U}^{\\mathrm{T}} \\vec{b}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stability of solving least squares problems\n",
    "Here using $QR$ factorization is more stable than directly solve the linear equation. Let's see from an example.\n",
    "\n",
    ">**e.g.***\n",
    ">\n",
    ">Fix $e^{\\sin(4t)}$ from $0$ to $1$; we'll discretize the interval by $100$ points and use a polynomial with degree $14$. The function now looks like\n",
    ">\n",
    ">$$\\left[\n",
    "\\begin{array}{ccccc}\n",
    "1 & x_1 & x_1^2 & \\cdots & x_1^{14} \\\\\n",
    "1 & x_2 & x_2^2 & \\cdots & x_2^{14} \\\\\n",
    "1 & x_3 & x_3^2 & \\cdots & x_3^{14} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & x_{100} & x_{100}^2 & \\cdots & x_{100}^{14} \n",
    "\\end{array}\n",
    "\\right]\\left[\n",
    "\\begin{array}{c}\n",
    "a_1 \\\\\n",
    "a_2 \\\\\n",
    "a_3 \\\\\n",
    "\\vdots \\\\\n",
    "a_{15} \n",
    "\\end{array}\n",
    "\\right]\n",
    "=c \\cdot \\left[\n",
    "\\begin{array}{c}\n",
    "e^{\\sin(4x_1)} \\\\\n",
    "e^{\\sin(4x_2)} \\\\\n",
    "e^{\\sin(4x_3)} \\\\\n",
    "\\vdots \\\\\n",
    "e^{\\sin(4x_{100})} \n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    ">Here, $c$ is used to scale $a_{15}$ to $1$, as a checking solution, $c = 1/2006.787678808116$. In the last case, where the normal equation is solved, $a_{15} = 0.24664$, completely different from $1$.\n",
    "\n",
    "$Conclusion$\n",
    "\n",
    "\n",
    "\n",
    "For [method 1](#Solve-normal-equation), $\\Cond(A^{\\mathrm{T}}A) = \\big\\|A^{\\mathrm{T}}A\\big\\|_2\\left\\|\\left(A^{\\mathrm{T}}A\\right)^{-1}\\right\\|_2$; then\n",
    "\n",
    "$$\\begin{align}\n",
    "\\big\\|A^{\\mathrm{T}}A\\big\\|_2 &= \\sup_{\\vec{x} \\neq \\vec{0}}\\frac{\\big\\|A^{\\mathrm{T}}A\\vec{x}\\big\\|_2} {\\left\\|\\vec{x}\\right\\|_2} = \\lambda_{\\text{max}}^2 \\\\\n",
    "\\big\\|\\left(A^{\\mathrm{T}}A\\right)^{-1}\\big\\|_2 &= \\frac{1} {\\lambda_{\\text{min}}^2}\\\\\n",
    "\\Longrightarrow \\Cond(A^{\\mathrm{T}}A) &= \\frac{\\lambda_{\\text{max}}^2} {\\lambda_{\\text{min}}^2}\n",
    "\\end{align}$$\n",
    "\n",
    "So that actually $\\Cond(A^{\\mathrm{T}}A) = \\left( \\Cond(A) \\right)^2$\n",
    "\n",
    "For [method 2](#By-QR-factorization), $\\Cond(A) = \\Cond(\\hat{R})$.\n",
    "\n",
    "$$\\begin{align}\n",
    "\\Cond(A) &= \\Cond(\\hat{Q}\\hat{R}) \\\\\n",
    "&= \\left\\| \\hat{Q}\\hat{R} \\right\\| \\cdot \\left\\| \\left( \\hat{Q}\\hat{R} \\right)^{+} \\right\\| \\\\\n",
    "&= \\sup_{\\vec{x} \\neq \\vec{0}} \\frac{\\left\\| \\hat{Q}\\hat{R}\\vec{x} \\right\\|} {\\left\\| \\vec{x} \\right\\|} \\cdot \\sup_{\\vec{x} \\neq \\vec{0}} \\frac{\\left\\| \\left( \\hat{Q}\\hat{R} \\right)^{+}\\vec{x} \\right\\|} {\\left\\| \\vec{x} \\right\\|} \\\\\n",
    "&= \\left\\| \\hat{R} \\right\\| \\cdot \\left\\| \\left( \\hat{R} \\right)^{+} \\right\\| \\\\\n",
    "&= \\Cond(\\hat{R})\n",
    "\\end{align}$$\n",
    "\n",
    "$Theorem$ 3\n",
    "\n",
    "Let the full rank least squares problem $\\min_{\\vec{x} \\in \\mathbb{R}^n}\\|\\vec{b} - A\\vec{x}\\|_2^2$ be solved by $QR$ factorization with Householder triangularization. This algorithm is stable in the sense that the computed solution $\\tilde{\\vec{x}}$ is the minimizer of a nearby least squares problem, $i.e.$, $\\tilde{\\vec{x}}$ minimizes $\\left\\|\\vec{b} - \\left(A + \\delta A \\right)\\vec{x}\\right\\|_2^2$ , where $\\left\\| \\delta A \\right\\|_2 = \\| A \\|_2 O(\\epsilon_{\\text{machine}} )$.\n",
    "\n",
    "$Theorem$ 4\n",
    "\n",
    "The solution of the full rank least squares problem $\\min_{\\vec{x} \\in \\mathbb{R}^n}\\|\\vec{b} - A\\vec{x}\\|_2^2$ via solving the normal equation $A^{\\mathrm{T}}A \\vec{x} = A^{\\mathrm{T}}\\vec{b}$ is unstable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "89px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
