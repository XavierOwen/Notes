{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigenvalue and Eigenvector\n",
    "Given an $m \\times m$ matrix $A$, we say a ***nonzero vector*** $\\vec{x} \\in \\mathbb{C}^m$ is an **eigenvector** of $A$, and a number $\\lambda \\in \\mathbb{C}$ is its **eigenvalue**, if\n",
    "\n",
    "$$A\\vec{x} = \\lambda \\vec{x}$$\n",
    "\n",
    "How to compute them?\n",
    "\n",
    "$$A\\vec{x} = \\lambda \\vec{x} \\Longleftrightarrow \\left( \\lambda I - A \\right)\\vec{x} = \\vec{0}$$\n",
    "\n",
    "\n",
    "which has a ***nonzero solution*** $\\vec{x}$, which requires that the matrix $\\lambda I - A$ is ***singular***, thus\n",
    "\n",
    "$$\\DeclareMathOperator*{\\det}{det} A\\vec{x} = \\lambda \\vec{x} \\Longleftrightarrow \\left( \\lambda I - A \\right)\\vec{x} = \\vec{0} \\Longleftrightarrow \\det\\left( \\lambda I - A \\right) = 0$$\n",
    "\n",
    "Here $\\det\\left( \\lambda I - A \\right)$ is actually a polynomial of degree $m$ in $\\lambda$, called the **characteristic polynomial** of matrix $A$.\n",
    "1. Solve the **characteristic polynomial** to find possible $\\lambda$s, the engenvalues\n",
    "2. For each **engenvalue**, solve the linear system $\\left( \\lambda I - A \\right)\\vec{x} = \\vec{0}$ to find the corresponding eigenvectors of $A$.\n",
    "\n",
    ">**e.g.** Compute the eigenvalue of the matrix $A = \\left[\\begin{array}{cc}\n",
    "2 & 1 \\\\\n",
    "0 & 1\n",
    "\\end{array}\\right]$\n",
    ">\n",
    ">>$$\\lambda I - A = \\lambda \\cdot \\left[\\begin{array}{cc}\n",
    "1 & 0 \\\\\n",
    "0 & 1\n",
    "\\end{array}\\right] - \\left[\\begin{array}{cc}\n",
    "2 & 1 \\\\\n",
    "0 & 1\n",
    "\\end{array}\\right] = \\left[\\begin{array}{cc}\n",
    "\\lambda - 2 & 1 \\\\\n",
    "0 & \\lambda - 1\n",
    "\\end{array}\\right]$$\n",
    "\n",
    ">> So that the chracteristic polynomial of $A$ is $\\left( \\lambda - 2 \\right)\\left( \\lambda - 1 \\right)$. Therefore the eigenvalues of $A$ are $\\lambda_1 = 2$ and $\\lambda_2 = 1$. Corresponding to the eigenvalue $2$, the solution of the linear system $(\\lambda_1 I - A) \\vec{x} = \\vec{0}$ would be\n",
    "\n",
    ">>$$\\left[\\begin{array}{cc}\n",
    "0 & -1 \\\\\n",
    "0 & 1\n",
    "\\end{array}\\right] \\vec{x} = \\left[\\begin{array}{c}\n",
    "0 \\\\\n",
    "0 \n",
    "\\end{array}\\right] \\Longrightarrow \\vec{x} = k_1 \\cdot \\left[\\begin{array}{c}\n",
    "1 \\\\\n",
    "0 \n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    ">> where $k_1$ is an arbitrary constant. And the other one is $k_2 \\cdot \\left[\\begin{array}{c}\n",
    "1 \\\\\n",
    "-1 \n",
    "\\end{array}\\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigenspace and multiplicity of eigenvalues\n",
    "$\\odot$ If a vector $\\vec{x}$  is an eigenvector of $A$ corresponding to an eigenvalue $\\lambda$, then any scalar multiple of $\\vec{x}$ is also an eigenvector of $A$ corresponding to $\\lambda$.$\\Join$\n",
    "\n",
    "**eigenspace** of $A$ corresponding to the eigenvalue $\\lambda$: the solution space of $\\left( A - \\lambda I\\right)\\vec{x} = \\vec{0}$, denoted a $E_{\\lambda}$. And the dimension of $E_{\\lambda}$ is also the **geometric multiplicity** of $\\lambda$, which represents the maximum number of linearly linearly independent eigenvectors corresponding to the eigenvalue $\\lambda$.\n",
    "\n",
    "**algebraic multiplicity**, the multiplicity of each root $\\lambda$.\n",
    "\n",
    "Now denote all the distinct eigenvalues of a matrix $A$ be $\\lambda_1, \\lambda_2, \\dots, \\lambda_k$, so that the characteristic polynomial $\\det(\\lambda I - A)$ would be then expressed as\n",
    "\n",
    "$$\\det(\\lambda I - A) = (\\lambda - \\lambda_1)^{l_1}(\\lambda - \\lambda_2)^{l_2 }\\cdots(\\lambda - \\lambda_k)^{l_k}$$\n",
    "\n",
    "Here $l_1, l_2, \\dots, l_k$ are the algebraic multiplicities of $\\lambda_1, \\lambda_2, \\dots, \\lambda_k$ respectively.\n",
    "\n",
    "$Conclusion$\n",
    "\n",
    "For any eigenvalue $\\lambda^*$ of a matrix $A$, its geometric multiplicity is always less than or equal to its algebric multiplicity.\n",
    "\n",
    "$Proof$\n",
    "\n",
    "Suppose a $m \\times m$ matrix $A$ with a eigenvalue $\\lambda^*$ whose geomatrix multiplicity is $l^*$. Then let the orthonormal basis of its eigenspace $E_{\\lambda^*}$ be $\\vec{v}_1, \\vec{v}_2, \\dots, \\vec{v}_{l^*}$. And let the complementary orthonormal vector be $\\vec{v}_{l^*+1},\\vec{v}_{l^*+2},\\dots,\\vec{v}_{m}$, so that they can form the orthogonormal basis of the $m\\text{-dimensional}$ vector space.\n",
    "\n",
    "Let $V$ be the orthogonal matrix with $\\vec{v}_{1},\\vec{v}_{2},\\dots,\\vec{v}_{m}$ as its columns, then we have\n",
    "\n",
    "$$V^{\\mathrm{T}}AV = \\left[\\begin{array}{cc}\n",
    "\\lambda I & C \\\\\n",
    "\\mathbf{0} & D\n",
    "\\end{array}\\right]$$\n",
    "\n",
    "where $I$ is the $l^* \\times l^*$ identity matrix, $C$ is an $l^*\\times (m-l^*)$ matrix, and $D$ is an $(m-l^*) \\times (m-l^*)$ matrix. Note that $\\det(V^{\\mathrm{T}})\\det(V) = 1$. So we have\n",
    "\n",
    "$$\\det(\\lambda I - A) = \\det(\\lambda I - V^{\\mathrm{T}}AV) = \\det( (\\lambda - \\lambda^*)I ) \\cdot \\det(\\lambda I - D) = (\\lambda - \\lambda^*)^{l^*} \\det(\\lambda I - D)$$\n",
    "\n",
    "which implies that the algebraic multiplicity of $\\lambda^*$ is at least $l^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity and Diagonalizability\n",
    "$Def$\n",
    "\n",
    "A matrix $A$ is said to be **similar** to another $B$ $iff$ there exists a ***nonsigular matrix*** $X$ such that\n",
    "\n",
    "$$A = XBX^{-1}$$\n",
    "\n",
    "$Conclusion$\n",
    "\n",
    "$$\\det(A - \\lambda I) = \\det(XBX^{-1} - \\lambda XIX^{-1}) = \\det(B - \\lambda I)$$\n",
    "\n",
    "And therefore they have the same eigenvalues.\n",
    "\n",
    "Here $m \\times m$ matrix $A$ is called **diagonalizable** if it is similar to a diagonal matrix $\\Lambda $.\n",
    "\n",
    "$$A = X\\Lambda X^{-1} \\Longrightarrow AX = X\\Lambda $$\n",
    "\n",
    "We can also see that the columns of $X$ are eigenvectors of the matrix $A$ corresponding to eigenvalues on the diagonal of $\\Lambda $: $A \\vec{x}^c_i = \\lambda_i \\vec{x}^c_i$.\n",
    "\n",
    "**defective**: An eigenvalue whose algebraic mutiplicity exceeds its geometric multiplicity is a **defective eigenvalue**; A matrix with defective eigenvalues is a **defective matrix**. This promises its eigenvalue decomposition. And of course any diagonal matrix is nondefective.\n",
    "\n",
    "***\n",
    "For simplicity, now we only consider real symmetric matrix $A$ ($A^{\\mathrm{T}} = A$). So that the eigenvalues are all real numbers, and eigenvectors can form a orthonormal basis of the $m\\text{-dimensional}$ vector space. \n",
    "\n",
    "$Theorem$ 1 \n",
    "\n",
    "For any real symmetric $m \\times m$ matrix $A$, there exist a real orthogonal matrix $Q$, and a real diagonal matrix $\\Lambda$, such that \n",
    "\n",
    "$$A = Q \\Lambda Q^{\\mathrm{T}} \\Longrightarrow A\\left[\n",
    "\\begin{array}{cccc}\n",
    "\\vec{q}_1^c & \\vec{q}_2^c & \\cdots & \\vec{q}_m^c \\\\\n",
    "\\end{array}\n",
    "\\right] = \\left[\n",
    "\\begin{array}{cccc}\n",
    "\\vec{q}_1^c & \\vec{q}_2^c & \\cdots & \\vec{q}_m^c \\\\\n",
    "\\end{array}\n",
    "\\right] \\left[\n",
    "\\begin{array}{cccc}\n",
    "\\lambda_1 & & & \\\\\n",
    "& \\lambda_2 & & \\\\\n",
    "& & \\ddots & \\\\\n",
    "& & & \\lambda_m \\\\\n",
    "\\end{array}\n",
    "\\right]$$\n",
    "\n",
    "This is the **eigenvalue decomposition** of matrix $A$. We can also give them an order so that $\\left| \\lambda_1 \\right| \\geq \\left| \\lambda_2 \\right| \\geq \\cdots \\geq \\left| \\lambda_1 \\right| \\geq\\left| \\lambda_m \\right| \\geq 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditioning of Eigenvalue Problems\n",
    "Given an $m \\times m$ matrix $A$ which is diagonalizable with eigenvalues $\\lambda_1, \\lambda_2, \\dots \\lambda_m$, and a set of eigenvectors $\\vec{x}^c_1,\\vec{x}^c_2, \\dots, \\vec{x}^c_m$ so that we can write $A = X\\Lambda X^{-1}$ or equivalently $\\Lambda = X^{-1}AX$.\n",
    "\n",
    "Assume there is a small perturbation $\\Delta A$ on the matrix $A$. Let us consider eigenvalues of the perturbed matrix $A + \\Delta A$. We denote\n",
    "\n",
    "$$X^{-1}\\Delta A X = F$$\n",
    "\n",
    "So that $X^{-1}\\left( A + \\Delta A \\right)X = \\Lambda + F$ and let $\\mu$ be the eigenvalue of $\\left( A + \\Delta A \\right)$ and $D+F$ corresponding to the eigenvector $\\vec{v}$.\n",
    "\n",
    "$$\\left( D + F \\right) \\vec{v} = \\mu \\vec{v} \\Longrightarrow \\mu \\vec{v} - D \\vec{v} = F\\vec{v}$$\n",
    "\n",
    "Therefore we have\n",
    "\n",
    "$$\\begin{align}\n",
    "\\left\\| \\vec{v} \\right\\|_2 &= \\left\\| \\left( \\mu I - D \\right)^{-1} F \\vec{v} \\right\\|_2 \\\\\n",
    "&\\leq \\left\\| \\left( \\mu I - D \\right)^{-1} \\right\\|_2 \\cdot \\left\\| F \\right\\|_2 \\cdot \\left\\| \\vec{v} \\right\\|_2 \\\\\n",
    "&= \\frac{\\left\\| F \\right\\|_2 \\cdot \\left\\| \\vec{v} \\right\\|_2} {\\left| \\mu - \\lambda_k \\right|}\n",
    "\\end{align}$$\n",
    "\n",
    "So that \n",
    "\n",
    "$$\\DeclareMathOperator*{\\Cond}{Cond}\n",
    "\\begin{align}\n",
    "\\left| \\mu - \\lambda_k \\right| &\\leq \\left\\| F \\right\\|_2 \\\\\n",
    "&\\leq \\left\\| X \\right\\|_2 \\cdot \\left\\| \\Delta A \\right\\|_2 \\cdot \\left\\| X^{-1} \\right\\|_2 \\\\\n",
    "&= \\Cond(X) \\left\\|\\Delta A \\right\\|_2 \n",
    "\\end{align}$$\n",
    "\n",
    "where $\\lambda_k$ is the closest to $\\mu$ among the eigenvalues of the matrix $A$. \n",
    "\n",
    "If the matrix $A$ is symmetric, then we know From $Theorem$ 1 that the matrix $Q$ is orthogonal. Therefore solving the eigenvalues of a symmetric matrix is always well-conditioned, since $\\Cond(Q) = 1$ for orthogonal matrix Q."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms for solving eigenvalue problems\n",
    "To find the eigenvalue is to solve the corresponding characteristic polynomial. But that's an ill-conditioned problem. At the same time, there is no exact formula to express the roots of a general high order ($> 5$) polynomials. This means that in general finding the eigenvalues of a matrix cannot be achieved within a finite number of steps of operations. Therefore an eigensolver has to be *iterative*.\n",
    "\n",
    "Similar to the idea from $QR$ and $LU$, we wish to multiply a sequence of orthogonal matrices and their transposes to the both sides of $A$, such that\n",
    "\n",
    "$$Q_m \\cdots Q_2 Q_1 A Q^{\\mathrm{T}}_1 Q^{\\mathrm{T}}_2 \\cdots Q^{\\mathrm{T}}_m = \\Lambda$$\n",
    "\n",
    "And then\n",
    "\n",
    "$$A= Q^{\\mathrm{T}}_1Q^{\\mathrm{T}}_2\\cdots Q^{\\mathrm{T}}_m \\Lambda Q_1Q_2 \\cdots Q_m = Q^{\\mathrm{T}}\\Lambda Q$$\n",
    "\n",
    "However such an approach is just an imagination. An eigenvalue problem cannot be solved in a finite number of steps in general. The approach to solve eigenvalue problem is in general to produce a sequence\n",
    "\n",
    "$$Q_j \\cdots Q_2 Q_1 A Q^{\\mathrm{T}}_1 Q^{\\mathrm{T}}_2 \\cdots Q^{\\mathrm{T}}_j$$\n",
    "\n",
    "such that it converges to an diagonal matrix $\\Lambda$ as $ j \\to 1$. In this sense we say\n",
    "\n",
    "**Any eigenvalue solver must be iterative.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "69px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
