{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Simple Regression Model\n",
    "\n",
    "## Definition of the Simple Regression Model\n",
    "\n",
    "$Def\\newcommand{\\using}[1]{\\stackrel{\\mathrm{#1}}{=}}\n",
    "\\newcommand{\\ffrac}{\\displaystyle \\frac}\n",
    "\\newcommand{\\space}{\\text{ }}\n",
    "\\newcommand{\\bspace}{\\;\\;\\;\\;}\n",
    "\\newcommand{\\QQQ}{\\boxed{?\\:}}\n",
    "\\newcommand{\\CB}[1]{\\left\\{ #1 \\right\\}}\n",
    "\\newcommand{\\SB}[1]{\\left[ #1 \\right]}\n",
    "\\newcommand{\\P}[1]{\\left( #1 \\right)}\n",
    "\\newcommand{\\dd}{\\mathrm{d}}\n",
    "\\newcommand{\\Tran}[1]{{#1}^{\\mathrm{T}}}\n",
    "\\newcommand{\\d}[1]{\\displaystyle{#1}}\n",
    "\\newcommand{\\EE}[2][\\,\\!]{\\mathbb{E}_{#1}\\left[#2\\right]}\n",
    "\\newcommand{\\Var}[2][\\,\\!]{\\mathrm{Var}_{#1}\\left[#2\\right]}\n",
    "\\newcommand{\\Cov}[2][\\,\\!]{\\mathrm{Cov}_{#1}\\left(#2\\right)}\n",
    "\\newcommand{\\Corr}[2][\\,\\!]{\\mathrm{Corr}_{#1}\\left(#2\\right)}\n",
    "\\newcommand{\\I}[1]{\\mathrm{I}\\left( #1 \\right)}\n",
    "\\newcommand{\\N}[1]{\\mathrm{N} \\left( #1 \\right)}\n",
    "\\newcommand{\\ow}{\\text{otherwise}}$\n",
    "\n",
    "***Simple Linear Regression Model***: Explains variable $y$ in terms of variable $x$ like the following \n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 x + u$$\n",
    "\n",
    "Here, $y$ is called the ***dependent variable***, or ***explained variable***, or ***response variable***, or ***regressand***; $x$ is the ***independent variable***, or ***explanatory variable***, or ***regressor***. Also, $\\beta_0$ is the ***Intercept parameter***, $\\beta_1$ is the ***slope parameter***, $u$ is the ***error term***, or ***disturbance***, or ***unobservables***.\n",
    "\n",
    "Since we already have a intercept, so we can also assume that $\\boxed{\\EE{u} = 0}$.\n",
    "\n",
    "**Interpretation**\n",
    "\n",
    "$\\ffrac{\\partial y} {\\partial x} = \\beta_1$ as long as $\\ffrac{\\partial u} {\\partial x} = 0$\n",
    "\n",
    "- The first term is to say by how much does the dependent variable change if the independent variable is increased by one unit\n",
    "- remember let all other things remain equal\n",
    "- rarely applicable though.\n",
    "\n",
    "***\n",
    "\n",
    "Then, how $u$ and $x$ are related? The natural way is to consider their *correlation coefficient*, while that's not enough cause it only measures the linear dependence between $u$ and $x$.\n",
    "\n",
    "$Remark$\n",
    "\n",
    ">It's possible for $u$ to be uncorrelatd with $x$ while being correlated with a function of $x$, say $x^2$.\n",
    "\n",
    "A better choice is consider the expected value of $u$ given $x$, which is that *the average value of $u$ does not depend on the value of* $x$, or $u$* is **mean independent** of $x$*. Mathematically:\n",
    "\n",
    "$$\\boxed{\\EE{u\\mid x} = \\EE{u}}$$\n",
    "\n",
    "Thus we obtain the ***zero conditional mean assumption***: $\\EE{u \\mid x} = 0$ and then the ***population regression function (PRF)***: $\\boxed{ \\EE{y \\mid x} = \\beta_0 + \\beta_1 x }$ which is a linear function of $X$.\n",
    "\n",
    "![](./figs/simpleRegressionModel.png)\n",
    "\n",
    "We call $\\beta_0 + \\beta_1 x$ the ***systematic part*** of $y$ and $u$, ***unsystematic***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deriving the Ordinary Least Squares Estimates\n",
    "\n",
    "1. Random sample of $n$ observations: $\\CB{\\P{x_i,y_i}: i=1,2,\\dots, n}$\n",
    "    - obviously $y_i = \\beta_0 + \\beta_1 x_i + u_i$ for all $i$\n",
    "    - $u_i$ is the ***error (disturbance) term*** for observation $i$\n",
    "2. Fit as good as possible a regression line through the data points\n",
    "    \n",
    "Before that, one significant waypoint: $\\EE{u \\mid x} = 0 \\Longrightarrow \\boxed{\\Cov{x,u} = 0}\\Longrightarrow \\boxed{\\EE{xu} = 0}$\n",
    "\n",
    "$Proof$\n",
    ">$\\begin{align}\n",
    "\\EE{u\\mid x} &= \\int_{-\\infty}^{\\infty} u\\cdot f_{u \\mid x} \\P{u\\mid x} \\;\\dd{u} \\\\\n",
    "&= \\int_{-\\infty}^{\\infty} u\\ffrac{f\\P{u,x}} {f_{X}\\P{x}} \\;\\dd{u} \\\\\n",
    "&= \\ffrac{\\d{\\int_{-\\infty}^{\\infty} u \\cdot f\\P{u,x}\\;\\dd{u}}} {f_X\\P{x}} = 0\n",
    "\\end{align}$\n",
    ">\n",
    ">$\\begin{align}\n",
    "\\Cov{x,u} &= \\EE{XU} \\\\\n",
    "&= \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty} xu\\cdot f\\P{x,u}\\;\\dd{x}\\dd{u}\\\\\n",
    "&= \\int_{-\\infty}^{\\infty}x \\cdot{ \\int_{-\\infty}^{\\infty} u\\cdot f\\P{x,u}\\;\\dd{u} }\\dd{x}\n",
    "\\end{align}$\n",
    "\n",
    "Alternatively we can write them as $\\boxed{\\EE{y - \\beta_0 - \\beta_1 x} = 0}$ and $\\boxed{\\EE{x\\P{y - \\beta_0 - \\beta_1 x}} = 0}$. And these two *MIGHT* be the restriction on two unknown parameters thus to obtain $\\hat\\beta_0$ and $\\hat\\beta_1$, the estimators of $\\beta_0$ and $\\beta_1$ respectively! The *possible* method is (we can regard the following as moment estimation çŸ©ä¼°è®¡):\n",
    "\n",
    "$$\\begin{cases}\n",
    "n^{-1} \\sum \\P{y_i - \\hat\\beta_0 - \\hat\\beta_1 x_i} = 0 \\\\\n",
    "n^{-1} \\sum x_i\\P{y_i - \\hat\\beta_0 - \\hat\\beta_1 x_i} = 0\n",
    "\\end{cases}$$\n",
    "\n",
    "the solution is quite easy to obtain ðŸ˜‰ðŸ˜‰ðŸ˜‰:\n",
    "\n",
    "$$\\hat{\\beta}_1 = \\ffrac{\\d{\\sum_{i=1}^{n} \\P{x_i - \\bar{x}}\\P{y_i - \\bar{y}}}} {\\d{\\sum_{i=1}^{n}\\P{x_i - \\bar{x}}^2}}, \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}$$\n",
    "\n",
    "$Remark$\n",
    "\n",
    "- $\\bar y = n^{-1}\\sum_{n} y_i$, the sample average of the $y_i$.\n",
    "- $\\hat\\beta_1$ can be interpreted as the sample covariance between $x$ and $y$ divided by the sample variance of $x$.\n",
    "- Implication: if $x$ and $y$ are positively correlated in the sample, then $\\hat\\beta_1$ is positive\n",
    "\n",
    "***\n",
    "This method is called the ***Ordinary Least Square***. Kinda different from what we've learned before...\n",
    "\n",
    "$$\\begin{cases}\n",
    "\\ffrac{\\partial \\P{\\sum\\limits_{i=1}^{n} \\P{y_i - \\beta_0 - \\beta_1 x_i}^2}} {\\partial \\beta_0} = 0 \\Longrightarrow \\sum\\limits_{i=1}^{n} -2\\P{y_i - \\hat\\beta_0 - \\hat\\beta_1 x_i} = 0  \\\\\n",
    "\\ffrac{\\partial \\P{\\sum\\limits_{i=1}^{n} \\P{y_i - \\beta_0 - \\beta_1 x_i}^2}} {\\partial \\beta_1} = 0 \\Longrightarrow \\sum\\limits_{i=1}^{n} -2x_i\\P{y_i - \\hat\\beta_0 - \\hat\\beta_1 x_i} = 0\n",
    "\\end{cases}$$\n",
    "\n",
    "- ***fitted value***: $\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i$\n",
    "- ***residual***: $\\hat{u}_i = y_i - \\hat{y}_i = y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i$\n",
    "\n",
    "![](./figs/Residual_fittedValue.png)\n",
    "\n",
    "$Remark$\n",
    "\n",
    ">The result can be obtained by differenting on $\\beta_1$ and $\\beta_0$. And the regression line will go through point $\\P{\\bar{x},\\bar{y}}$ and different sample will result in different estimates and thus a different regression line. Also, the **residual** $\\hat u_i$ is quite different from the **error term** $u_i$.\n",
    "\n",
    "- ***sum of squared residuals***: $\\text{SSR} = \\sum\\limits_{i=1}^n \\hat u_i^2 = \\sum\\limits_{i=1}^n\\P{y_i - \\hat\\beta_0 - \\hat\\beta_1 x_i}^2$. And actually the method we've learned before is exactly the way to find the value of $\\P{\\hat\\beta_0, \\hat\\beta_1}$ to minimize the $\\text{SSR}$.\n",
    "- We also called the first equation set without $n^{-1}$ the ***first order conditions*** for the OLS estimate.\n",
    "- ***OLS Regression Line***: $\\hat y = \\hat\\beta_0 + \\hat\\beta_1 x$, also known as ***sample regression function (SRF)*** cause it's the estimated version of *PRF*, $\\EE{y \\mid x} = \\beta_0 + \\beta_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of OLS on Any Sample of Data\n",
    "\n",
    "### algebraic properties of olS Statistics\n",
    "\n",
    "For any sample data, we have the following\n",
    "\n",
    "- $\\sum\\limits_{i=1}^{n} \\hat{u}_i = 0$, followed directly from the **first order conditions**\n",
    "- $\\sum\\limits_{i=1}^{n} x_i\\hat{u}_i = 0$\n",
    "- $\\bar{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\bar{x}$\n",
    "\n",
    "from the first one, we can take the average of both sides of equation $y_i = \\hat y_i + \\hat u_i$ and obtain: $\\bar{y} = \\bar{\\hat y}$; and follow the first and the second one, the sample covariance between $\\hat y_i$ and $\\hat u_i$ is $0$.\n",
    "\n",
    "Also we define several measures:\n",
    "\n",
    "- ***Total sum of squares***: $\\text{SST} \\equiv \\sum\\limits_{i=1}^{n} \\P{y_i - \\bar{y}}^2$\n",
    "- ***Explained sum of squares***: $\\text{SSE} \\equiv \\sum\\limits_{i=1}^{n} \\P{\\hat{y}_i - \\bar{y}}^2$\n",
    "- ***Residual sum of squares***: $\\text{SSR} \\equiv \\sum\\limits_{i=1}^{n} \\hat{u}_i^2 = \\text{SST} - \\text{SSE}$\n",
    "\n",
    "$Proof$\n",
    "\n",
    "$\\bspace \\begin{align}\n",
    "\\text{SST} &= \\sum_{i=1}^{n} \\P{y_i - \\bar y}^2 \\\\\n",
    "&= \\sum_{i=1}^{n} \\SB{\\P{y_i - \\hat y_i}+ \\P{\\hat y_i - \\bar y}}^2 \\\\\n",
    "&= \\sum_{i=1}^{n} \\SB{\\hat u_i + \\hat y_i - \\bar y}^2 \\\\\n",
    "&= \\sum_{i=1}^{n} \\hat u_i^2 + 2\\sum_{i=1}^{n}\\hat u_i \\P{\\hat y_i -\\bar y}+\\sum_{i=1}^{n}\\P{\\hat y_i -\\bar y}^2\\\\\n",
    "&= \\text{SSR} + 2\\sum_{i=1}^{n}\\hat u_i \\P{\\hat y_i -\\bar y} + \\text{SSE}\n",
    "\\end{align}$\n",
    "\n",
    "There $2\\sum_{i=1}^{n}\\hat u_i \\P{\\hat y_i -\\bar y} = 0$ cause $\\ffrac{\\sum_{i=1}^{n}\\hat u_i \\P{\\hat y_i -\\bar y}} {n-1} = \\ffrac{\\sum \\hat u_i \\hat y_i - \\sum \\hat u_i \\bar y} {n-1} = \\Cov{\\hat u_i, \\hat y_i} = \\hat\\beta_1\\Cov{\\hat u_i,x_i} = 0$. Thus, proved. Or use other methods if you like.\n",
    "\n",
    "### Goodness-of-Fit\n",
    "\n",
    "We first assume that $\\text{SST} \\neq 0$ which will happen only when all the $y_i$ equal the same value. Then:\n",
    "\n",
    "$Def$\n",
    "\n",
    "***Goodness of fit measure*** (***R-squared***), $R^2 = \\ffrac{\\text{SSE}} {\\text{SST}} = 1 - \\ffrac{\\text{SSR}}{\\text{SST}}$.\n",
    "\n",
    "$Remark$\n",
    "\n",
    "> R-squared measures the *fraction* of the total variation that is explained by the regression.\n",
    ">\n",
    "> And we can also prove that $R^2$ is equal to the square of the sample correlation coefficient between $y_i$ and $\\hat y_i$, $\\rho^2\\P{y_i, \\hat y_i}$.\n",
    ">\n",
    "> A seemingly low $R^2$ does not necessarily mean that an OLS regression equation is useless. And a high $R^2$ does NOT necessarily mean that the regression has a causal interpretation! (There may be other factors that affect the election outcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Units of Measurement and Functional Form\n",
    "### The Effects of Changing Units of Measurement on OLS Statistics\n",
    "\n",
    "The slope parameter and the intercept parameter expand or shrink with the same pace as the measure change of dependent variable, rather, generally only the slope parameter change with the measure change of independent variable.\n",
    "\n",
    "As for $R^2$, never gonna change with any of $x$ or $y$.\n",
    "\n",
    "### Incorporating Nonlinearities in Simple Regression\n",
    "\n",
    "***Semi-logarithmic form***\n",
    "\n",
    "$\\log\\P{y} = \\beta_0 + \\beta_1 x + u$\n",
    "\n",
    "and here $\\beta_1$ is the change rate of \"the percentage change of $y$ over $x$\"\n",
    "\n",
    "***Log-logarithmic form***\n",
    "\n",
    "$\\log\\P{y} = \\beta_0 + \\beta_1 \\log\\P{x} + u$\n",
    "\n",
    "and here $\\beta_1$ is the change rate of \"*the percentage change of $y$ over the percentage change of $x$*\". The log-log form postulates a constant elasticity model,    whereas the semi-log form assumes a semi-elasticity model\n",
    "\n",
    "### The Meaning of \"linear\" Regression\n",
    "\n",
    "The model (equation: $y = \\beta_0 + \\beta_1 x + u$) is linear in the *parameters* $\\beta_0$ and $\\beta_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Values and Variances of the OLS Estimators\n",
    "\n",
    "Mainly the statistical properties of OLS estimation. We now view $\\hat\\beta_0$ and $\\hat\\beta_1$ as estimators for the parameters $\\beta_0$ and $\\beta_1$. In other words, the distributions of $\\hat\\beta_0$ and $\\hat\\beta_1$ over different random samples from the population.\n",
    "\n",
    "### Unbiasedness of OLS\n",
    "\n",
    "$Assumption$ $\\text{SLR}.1$ to $\\text{SLR}.4$\n",
    "\n",
    "-  Linearity of the parameters: we define the population model, $y = \\beta_0 + \\beta_1 x + u$\n",
    "-  Random sampling: $\\CB{\\P{x_i,y_i}: i = 1,2,\\dots,n}$; and define: $u_i = y_i - \\beta_0 - \\beta_1 x_i$\n",
    "-  Sample variation in explanatory variable: $\\sum\\limits_{i=1}^{n} \\P{x_i - \\bar{x}}^2 > 0$, or equivalently, sample variance exists, $\\CB{x_i, i = 1,2,\\dots,n}$ are not all the same value\n",
    "    - otherwise it would be impossible to study how different values of the explanatory variable lead to different values of the dependent variable\n",
    "- Zero conditional mean: $\\EE{u_i \\mid x_i} = 0$\n",
    "\n",
    "$Theroem.1$ Unbisedness of OLS\n",
    "\n",
    "Using assumption $\\text{SLR}.1$ through $\\text{SLR}.4$, we have $\\EE{\\hat{\\beta}_0} = \\beta_0 $ and $\\EE{\\hat{\\beta}_1} = \\beta_1$.\n",
    "\n",
    "To prove this, we first mention a missed property for the estimators, linearity.\n",
    "\n",
    "$$\\hat{\\beta}_1 = \\ffrac{\\d{\\sum_{i=1}^{n} \\P{x_i - \\bar{x}}\\P{y_i - \\bar{y}}}} {\\d{\\sum_{i=1}^{n}\\P{x_i - \\bar{x}}^2}} = \\ffrac{\\d{\\sum_{i=1}^{n} \\P{x_i - \\bar{x}}y_i}} {\\d{\\sum_{i=1}^{n}\\P{x_i - \\bar{x}}^2}} \\hat= \\sum k_i y_i$$\n",
    "\n",
    "$$k_i = \\ffrac{x_i - \\bar{x}} {\\d{\\sum_{i=1}^{n}\\P{x_i - \\bar{x}}^2}} \\Rightarrow\\sum k_i = 0,\\bspace\\sum k_i x_i = \\ffrac{\\d{\\sum_{i=1}^{n}x_i^2 - x_i\\bar{x}}} {\\d{\\sum_{i=1}^{n}\\P{x_i - \\bar{x}}^2}} = 1$$\n",
    "\n",
    "$$\\hat\\beta_1 = \\sum k_i y_i = \\beta_0 \\sum k_i + \\beta_1 \\sum k_i x_i + \\sum k_i u_i = \\beta_1 + \\sum k_i u_i$$\n",
    "\n",
    "$$\\EE{\\hat\\beta_1} = \\EE{\\beta_1 + \\sum k_i u_i} = \\beta_1 + \\sum k_i \\EE{u_i} = \\beta_1$$\n",
    "\n",
    "As for $\\beta_0$, it's rather easy now. \n",
    "\n",
    "$$\\hat\\beta_0 = \\bar y - \\hat\\beta_1 \\bar x = \\beta_0 + \\beta_1 \\bar x + \\bar{u} -\\hat\\beta_1 \\bar x \\Longrightarrow \\hat\\beta_0 = \\beta_0 + \\P{\\beta_1 - \\hat\\beta_1} \\bar x + \\bar{u}$$\n",
    "\n",
    "And here $\\bar u = n^{-1}\\sum u_i$.\n",
    "\n",
    "Also we'd like to mention: $\\hat u_i = y_i - \\hat\\beta_0 -\\hat\\beta_1 x_i = \\P{\\beta_0 + \\beta_1 x_i + u_i} - \\hat\\beta_0 -\\hat\\beta_1 x_i$ so that\n",
    "\n",
    "$$\\hat u_i = u_i - \\P{\\hat\\beta_0 - \\beta_0} - \\P{\\hat\\beta_1 - \\beta_1}x_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variances of the OLS Estimators\n",
    "\n",
    "$Assumption.5$\n",
    "\n",
    "Homoskedasticity: $\\Var{u_i \\mid x_i} = \\sigma^2$, called the ***error variance***. This is to say that the value of the explanatory variable must contain no information about the variability of the unobserved factors; and with this we can measure the sample variability, $\\Var{\\hat\\beta_0}$ and $\\Var{\\hat\\beta_1}$\n",
    "\n",
    "$Remark$\n",
    "\n",
    "$\\text{SLR}.5$ would make the calculation of variance so much easier.\n",
    "\n",
    "Under that, we first can know that $\\EE{u^2 \\mid x} = \\sigma^2$ and then $\\sigma^2 = \\EE{u^2 \\mid x} \\using{\\text{assumption } 4}{} \\EE{u^2} = \\Var{u}$. Then\n",
    "\n",
    "$Theorem.2$ Sampling Variances of the OLS Estimator\n",
    "\n",
    "Using $\\text{SLR}.1$ to $\\text{SLR}.5$,\n",
    "\n",
    "$\\begin{align}\n",
    "\\Var{\\hat\\beta_1} &= \\Var{\\beta_1 + \\ffrac{\\d{\\sum_{i=1}^{n} \\P{x_i - \\bar{x}}u_i}} {\\d{\\sum_{i=1}^{n}\\P{x_i - \\bar{x}}^2}}} = \\Var{\\ffrac{\\d{\\sum_{i=1}^{n} \\P{x_i - \\bar{x}}u_i}} {\\d{\\sum_{i=1}^{n}\\P{x_i - \\bar{x}}^2}}} \\\\\n",
    "&= \\P{\\ffrac{1} {\\sum_{i=1}^{n}\\P{x_i - \\bar{x}}^2}}^2 \\P{\\sum_{i=1}^{n} \\P{x_i - \\bar{x}}^2 \\Var{u_i}}\\\\\n",
    "&= \\ffrac{1} {\\SB{{\\sum_{i=1}^{n}\\P{x_i - \\bar{x}}^2}}^2} \\sigma^2 \\cdot \\text{SST}_x = \\ffrac{\\sigma^2} {\\text{SST}_x}\n",
    "\\end{align}$\n",
    "\n",
    "or even quicker:\n",
    "\n",
    "$\\Var{\\hat\\beta_1} = \\sigma^2 \\sum k_i^2 = \\ffrac{ \\sigma^2} {\\d{\\sum_{i=1}^{n}\\P{x_i - \\bar{x}}^2}}$\n",
    "\n",
    "And since we've already had $\\hat\\beta_0 = \\beta_0 + \\P{\\beta_1 - \\hat\\beta_1} \\bar x + \\bar{u}$, then we have\n",
    "\n",
    "$\\Var{\\hat\\beta_0} = 0 + \\ffrac{\\sum\\Var{u}} {n^2} + \\P{\\bar{x}}^2 \\cdot \\ffrac{\\sigma^2} {\\text{SST}_x} = \\ffrac{ \\text{SST}_x} {n\\cdot \\text{SST}_x}\\sigma^2 + \\ffrac{ n\\P{\\bar{x}}^2} {n\\cdot \\text{SST}_x}\\sigma^2 = \\ffrac{\\sum x_i^2} {n\\cdot \\text{SST}_x}\\sigma^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Remark$\n",
    "\n",
    "Assuption $4$ and $5$ can be also writen as $\\EE{y \\mid x} = \\beta_0 + \\beta_1 x$ and $\\Var{y \\mid x} = \\sigma^2 = \\Var{u \\mid x} = \\Var{u}$ respectively.\n",
    "\n",
    "![](./figs/SLRHomoskedasticity.png)\n",
    "\n",
    "In other words, the conditional expectation of $y$ given $x$ is linear in $x$, but the variance of $y$ given $x$ is constant.\n",
    "\n",
    "$Remark$\n",
    "\n",
    "When $\\Var{u \\mid x}$ depends on $x$, the error term is said to exhibit ***heteroskedasticity*** (or just because of nonconstant variance). And since we always have $\\Var{u \\mid x} = \\Var{y \\mid x}$, heteroskedasticity is present whenever $\\Var{y \\mid x}$ is a function of $x$.\n",
    "\n",
    "$Remark$\n",
    "\n",
    "1. Larger **error variance** $\\sigma^2$, larger $\\Var{\\hat\\beta_1}$\n",
    "2. More variability in the independent variable $x$ is preferred\n",
    "***\n",
    "\n",
    "Also, in case that confidence intervals needed, their ***standard deviation*** are $\\text{sd}\\P{\\hat\\beta_i} = \\sqrt{\\Var{\\hat\\beta_i}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the Error Variance\n",
    "\n",
    "$\\bspace \\Var{u_i \\mid x_i} \\using{independency} \\sigma^2 = \\Var{u_i}$\n",
    "\n",
    "But Error Variance $\\sigma^2$ is not always given thus we have to estimate that sometimes.\n",
    "\n",
    "An intuitive estimation would be like $\\tilde\\sigma^2 = \\ffrac{1} {n} \\sum\\limits_{i=1}^{n} \\P{\\hat{u}_i - \\bar{\\hat{u}}_i}^2 = \\ffrac{1} {n} \\sum\\limits_{i=1}^{n} \\hat{u}_i^2$ however, this is an biased estimation. Here's the unbiased one:\n",
    "\n",
    "$$\\hat\\sigma^2 = \\ffrac{1} {n-2} \\sum_{i=1}^{n} \\hat u_i^2$$\n",
    "\n",
    "$Remark$\n",
    "\n",
    ">Here $2$ is the number of estimated regression coefficients, or you can call it a degree of freedom...\n",
    "\n",
    "$Theorem.3$\n",
    "\n",
    "$\\bspace \\EE{\\hat{\\sigma}^2} = \\sigma^2$\n",
    "\n",
    "$Proof$\n",
    "\n",
    ">We need to use the assumption $\\text{SSR}.1$ to $\\text{SSR}.5$, and the preceding results: $\\sum \\hat u_i = 0$ and $\\hat u_i = u_i - \\P{\\hat\\beta_0 - \\beta_0} - \\P{\\hat\\beta_1 - \\beta_1} x_i$. We go across all $i$ for the second equation and will get that: $0 = \\bar{u} - \\P{\\hat\\beta_0 - \\beta_0} - \\P{\\hat\\beta_1 - \\beta_1} \\bar x$; substracting it from that again leads to:  $\\hat u_i = \\P{u_i - \\bar u} - \\P{\\hat\\beta_1 - \\beta_1} \\P{x_i - \\bar x}$. Therefore,\n",
    ">\n",
    ">$$\\bspace \\begin{align}\n",
    "\\sum_{i=1}^{n} \\hat u_i^2 &= \\sum_{i=1}^{n} \\P{\\P{u_i - \\bar u}^2 + \\P{\\hat\\beta_1 - \\beta_1}^2\\P{x_i - \\bar x}^2 - 2\\P{u_i - \\bar{u}}\\P{\\hat\\beta_1 - \\beta_1}\\P{x_i - \\bar x}} \\\\\n",
    "&= \\P{\\sum_{i=1}^{n} \\P{u_i - \\bar u}^2} + \\P{\\hat\\beta_1 - \\beta_1}^2 \\sum_{i=1}^{n} \\P{x_i - \\bar x}^2 - 2\\P{\\hat\\beta_1 - \\beta_1} \\sum_{i=1}^{n} u_i\\P{x_i - \\bar{x}}\n",
    "\\end{align}$$\n",
    ">\n",
    ">Taking its expectation, the first term got a expectation $\\P{n-1}\\sigma^2$; the second term got a expectation of $\\EE{\\P{\\hat\\beta_1 - \\beta_1}^2} = \\Var{\\hat\\beta_1} = \\sigma^2/SST_x$. And for the third term, we first rewrite it as $2\\P{\\hat\\beta_1 - \\beta_1}\\QQQ$; taking expectation gives $2\\sigma^2$. Basically that's it.\n",
    "\n",
    "Then it's the ***standard error of regression (SER)***: $\\hat\\sigma = \\sqrt{\\hat\\sigma^2}$. We can also plug the value back to get the estimate value of $\\Var{\\hat\\beta_i}$, standard deviation and standard error of $\\hat\\beta_i$\n",
    "\n",
    "$\\DeclareMathOperator*{\\sd}{sd} \\sd\\P{{\\hat\\beta_1}} = \\sqrt{{\\Var{\\hat\\beta_1}}} = \\sigma / \\sqrt{\\text{SST}_x}$\n",
    "\n",
    "$\\sd\\P{\\hat\\beta_0} = \\sqrt{{\\Var{\\hat\\beta_0}}} = \\sqrt{\\ffrac{\\sum x_i^2} {n\\cdot \\text{SST}_x}} \\sigma$\n",
    "\n",
    "$\\DeclareMathOperator*{\\se}{se} \\se\\P{{\\hat\\beta_1}} = \\sqrt{\\widehat{\\Var{\\hat\\beta_1}}} = \\hat\\sigma / \\sqrt{\\text{SST}_x}$\n",
    "\n",
    "$\\se\\P{\\hat\\beta_0} = \\sqrt{\\widehat{\\Var{\\hat\\beta_0}}} = \\sqrt{\\ffrac{\\sum x_i^2} {n\\cdot \\text{SST}_x}}\\hat \\sigma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "229px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
