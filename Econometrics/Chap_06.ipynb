{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Regression Analysis: Further Issues\n",
    "## Effects of Data Scaling on OLS Statistics\n",
    "\n",
    "By analysing an example, we have when the data for **dependent variable** are scaled to $k$ times as before,\n",
    "\n",
    "- the OLS coefficient estimates are scaled to $\\DeclareMathOperator*{\\argmin}{argmin}\n",
    "\\DeclareMathOperator*{\\argmax}{argmax}\n",
    "\\DeclareMathOperator*{\\plim}{plim}\n",
    "\\newcommand{\\using}[1]{\\stackrel{\\mathrm{#1}}{=}}\n",
    "\\newcommand{\\ffrac}{\\displaystyle \\frac}\n",
    "\\newcommand{\\asim}{\\overset{\\text{a}}{\\sim}}\n",
    "\\newcommand{\\space}{\\text{ }}\n",
    "\\newcommand{\\bspace}{\\;\\;\\;\\;}\n",
    "\\newcommand{\\QQQ}{\\boxed{?\\:}}\n",
    "\\newcommand{\\void}{\\left.\\right.}\n",
    "\\newcommand{\\Tran}[1]{{#1}^{\\mathrm{T}}}\n",
    "\\newcommand{\\d}[1]{\\displaystyle{#1}}\n",
    "\\newcommand{\\CB}[1]{\\left\\{ #1 \\right\\}}\n",
    "\\newcommand{\\SB}[1]{\\left[ #1 \\right]}\n",
    "\\newcommand{\\P}[1]{\\left( #1 \\right)}\n",
    "\\newcommand{\\abs}[1]{\\left| #1 \\right|}\n",
    "\\newcommand{\\norm}[1]{\\left\\| #1 \\right\\|}\n",
    "\\newcommand{\\dd}{\\mathrm{d}}\n",
    "\\newcommand{\\Exp}{\\mathrm{E}}\n",
    "\\newcommand{\\RR}{\\mathbb{R}}\n",
    "\\newcommand{\\EE}{\\mathbb{E}}\n",
    "\\newcommand{\\NN}{\\mathbb{N}}\n",
    "\\newcommand{\\ZZ}{\\mathbb{Z}}\n",
    "\\newcommand{\\QQ}{\\mathbb{Q}}\n",
    "\\newcommand{\\AcA}{\\mathscr{A}}\n",
    "\\newcommand{\\FcF}{\\mathscr{F}}\n",
    "\\newcommand{\\Var}[2][\\,\\!]{\\mathrm{Var}_{#1}\\left[#2\\right]}\n",
    "\\newcommand{\\Avar}[2][\\,\\!]{\\mathrm{Avar}_{#1}\\left[#2\\right]}\n",
    "\\newcommand{\\Cov}[2][\\,\\!]{\\mathrm{Cov}_{#1}\\left(#2\\right)}\n",
    "\\newcommand{\\Corr}[2][\\,\\!]{\\mathrm{Corr}_{#1}\\left(#2\\right)}\n",
    "\\newcommand{\\I}[1]{\\mathrm{I}\\left( #1 \\right)}\n",
    "\\newcommand{\\N}[1]{\\mathcal{N} \\left( #1 \\right)}\n",
    "\\newcommand{\\ow}{\\text{otherwise}}k$ times, intercept included\n",
    "- statistical significance won't change\n",
    "- the standard error are scaled to $k$ times\n",
    "- $t$ statistics won't change\n",
    "- the endpoints for the CI are scaled to $k$ times\n",
    "- $R$-squared won't change\n",
    "- sum of squared residuals, $\\text{SSR}$ are scaled to $k^2$ times\n",
    "- standard error of the regression, $\\hat\\sigma = \\sqrt{\\ffrac{\\text{SSR}} {n-k-1}}$, are scaled to $k$ times\n",
    "\n",
    "Now we scale on of the **independent variable** to its $k$ times, say $x_1$\n",
    "\n",
    "- only $\\hat\\beta_1$ is scaled to its $1/k$ times\n",
    "- also its standard error is scaled to its $1/k$ times\n",
    "- all other things keep the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beta Coefficients\n",
    "\n",
    "Standard deviation and mean are key features of the data, so now we introduce regression using $z$-scores, where the variable is ***standardized*** in the sample by *subtracting off its mean* and *dividing by its standard deviation*.\n",
    "\n",
    "$$\\begin{cases}\n",
    "y_i = \\hat\\beta_0 + \\hat\\beta_1 x_{i1} + \\hat\\beta_2 x_{i2} + \\cdots + \\hat\\beta_k x_{ik} + \\hat u_i \\\\\n",
    "\\bar y = \\hat\\beta_0 + \\hat\\beta_1 \\bar x_1 + \\hat\\beta_2 \\bar x_{2} + \\cdots + \\hat\\beta_k \\bar x_k + 0\n",
    "\\end{cases} \\\\[1.5em]\n",
    "\\Rightarrow\\begin{align}\n",
    "\\ffrac{y_i-\\bar y} {\\hat\\sigma_y} &= \\P{\\ffrac{\\hat\\sigma_1} {\\hat\\sigma_y}} \\hat\\beta_1 \\P{\\ffrac{x_{i1} - \\bar x_1} {\\hat\\sigma_1}} + \\P{\\ffrac{\\hat\\sigma_2} {\\hat\\sigma_y}} \\hat\\beta_2 \\P{\\ffrac{x_{i2} - \\bar x_2} {\\hat\\sigma_2}} + \\cdots + \\P{\\ffrac{\\hat\\sigma_k} {\\hat\\sigma_y}} \\hat\\beta_k \\P{\\ffrac{x_{ik} - \\bar x_k} {\\hat\\sigma_k}} + \\ffrac{\\hat u_i} {\\hat\\sigma_y} \\\\\n",
    "z_y &\\equiv \\hat b_1 z_1 + \\hat b_2 z_2 + \\cdots + \\hat b_k z_k + \\text{error}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $z_y$ denotes the $z$-score of $y$, $z_i$ for $i=1,2,\\dots,k$ is the $z$-score of $x_i$. And the new coefficients are $\\hat b_j = \\ffrac{\\hat\\sigma_j} {\\hat\\sigma_y} \\hat\\beta_j$, called the ***standardized coefficients*** or ***beta coefficients***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More on Functional Form\n",
    "### More on Using Logarithmic Functional Forms\n",
    "\n",
    "$Review$\n",
    "\n",
    ">For model $\\log\\P{y} = \\beta_0 + \\beta_1\\log\\P{x_1} + \\beta_2 x_2 + u$, $\\beta_1 = \\ffrac{\\partial \\log\\P{y}} {\\partial \\log\\P{x_1}}$, the elasticity, presenting the percentage change of $y$ if $x_1$ increases by $1\\%$.\n",
    "\n",
    "Benefits, blahblahblah; Drawbacks, blahblahblah...\n",
    "***\n",
    "\n",
    "About rescaling: for variables appearing in logarithmic form, their slope coefficients are invariant to rescalings.\n",
    "\n",
    "$$ \\log\\P{y_i} = \\beta_0 + \\beta_1 x_i + u_i \\Rightarrow \\log\\P{c_iy_i} = \\P{\\log\\P{c_1} +\\beta_0} + \\beta_1 x_i + u_i \\\\\n",
    "y_i = \\beta_0 + \\beta_1 \\log\\P{x_i} + u_i \\Rightarrow y_i = \\P{\\beta_1\\log\\P{c_1} + \\beta_0} + \\beta_1 \\log\\P{x_i} + u_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models with Quadratics\n",
    "\n",
    "A simple example, the estimated equation: $\\hat y = \\hat\\beta_0 +\\hat\\beta_1 x + \\hat\\beta_2 x^2$. So that the approximation:\n",
    "\n",
    "$$\\Delta \\hat y \\approx \\P{\\hat\\beta_1 + 2\\hat\\beta_2 x}\\Delta x\\Rightarrow \\ffrac{\\Delta \\hat y} {\\Delta x} = \\hat\\beta_1 + 2\\hat\\beta_2 x$$\n",
    "\n",
    "Some times there's marginal effects, when $\\hat\\beta_1\\cdot\\hat\\beta_2 < 0$. The turning point is at \n",
    "\n",
    "$$x^* = \\abs{\\ffrac{\\hat\\beta_1} {2\\hat\\beta_2}}$$\n",
    "\n",
    "$Remark$\n",
    "\n",
    ">More possible model: $\\log\\P{y} = \\beta_0 + \\beta_1\\log\\P{x} +\\beta_2\\P{\\log\\P{x}}^2$. It's elasticity is $\\beta_1$ when $\\beta_2 = 0$ otherwise\n",
    ">\n",
    ">$$\\ffrac{\\partial\\log\\P{y}} {\\partial\\log\\P{x}} = \\beta_1 + 2\\beta_2 \\log\\P{x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models with Interaction Terms\n",
    "\n",
    "Sometimes we have the model like:\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 \\cdot x_2 + u$$\n",
    "\n",
    "then the partial effect of $x_2$ on $y$ is $\\ffrac{\\Delta y} {\\Delta x_2} = \\beta_2 + \\beta_3 x_1$ meaning that there's a ***interaction effect*** between $x_1$ and $x_2$. And this can only complicate out explanation to the model. Not recommended.\n",
    "\n",
    "One thing to do for a better explanation is to ***reparameterize*** the model, from the original one to\n",
    "\n",
    "$$y = \\alpha_0 + \\delta_1 x_1 + \\delta_2 x_2 + \\beta_3 \\P{x_1 - \\mu_1}\\P{x_2 - \\mu_2} +u$$\n",
    "\n",
    "- Easy interpretation of all parameters\n",
    "- Standard errors for partial effects at the mean values available\n",
    "- If necessary, interaction may be centered at other interesting values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More on Goodness-of-Fit and Selection of Regressors\n",
    "\n",
    "$R^2$ is simply an estimate of how much variation in $y$ is explained by $x_1, x_2, \\dots,x_k$ in the population. Two forgettable points:\n",
    "\n",
    "- A high $R^2$ does not imply that there is a causal interpretation因果关系\n",
    "- A low $R^2$ does not preclude排除 precise estimation of partial effects\n",
    "\n",
    "### Adjusted $R$-Squared\n",
    "\n",
    "$Review$\n",
    "\n",
    "The ordinary $R$-squared: $R^2 = 1-\\ffrac{\\text{SSR}} {\\text{SST}}$, as am estimate for $1-\\ffrac{\\sigma_{u}^2} {\\sigma_{y}^2}$, the ***population ***$R$***-squared***. Here's the adjusted one:\n",
    "\n",
    "$$\\bar R^2 = 1-\\ffrac{\\ffrac{\\text{SSR}} {n-k-1}} {\\ffrac{\\text{SST}} {n-1}} = 1-\\ffrac{\\hat \\sigma^2} {\\ffrac{ \\text{SST}} {n-1}}$$\n",
    "\n",
    "- it imposes a penalty for adding additional independent variables to a model\n",
    "- Not like $R^2$ a non-decreasing one, $\\bar R^2$ will increase $iff$ the $t$ statistic on the new variable is greater than one in absolute value\n",
    "- could be Negative\n",
    "\n",
    "$$\\bar R^2 = 1 - \\P{1-R^2} \\ffrac{n-1} {n-k-1}$$\n",
    "\n",
    "### using Adjusted $R$-Squared to Choose between Nonnested Models\n",
    "\n",
    "***nonnested models***: Two models are **nonnested models** because neither is a special case of the other.\n",
    "\n",
    "If we're comparing two nonnested models to decide which one is better, \n",
    "\n",
    "- if they have different number of parameters, $R^2$ can't reveal a thing. We should take the degree of freedom into account thus use $\\bar R^2$\n",
    "- also not comparable when they differ in their definition of the dependent variable\n",
    "\n",
    "### Controlling for Too Many Factors in Regression Analysis\n",
    "\n",
    "Certain variables should not be held fixed, if so, we're ***over controlling***. And it really partially depend on what the problem we are focusing on is.\n",
    "\n",
    "### Adding Regressors to Reduce the Error Variance\n",
    "\n",
    "With more regressors\n",
    "\n",
    "1. more multicollinearity problems\n",
    "2. reduces the error variance\n",
    "3. but it's hard to find those variables that are uncorrelated with other regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction and Residual Analysis\n",
    "### Confidence Intervals for Predictions\n",
    "\n",
    "The estimated model: $\\hat y = \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 x_2 + \\cdots + \\hat\\beta_k x_k$. And the estimated parameter is:\n",
    "\n",
    "$$\\theta_0 = \\Exp\\SB{y\\mid x_1 = c_1, x_2 = c_2,\\dots,x_k = c_k} = \\beta_0 + \\beta_1 c_1 + \\cdots + \\beta_k c_k$$\n",
    "\n",
    "Its estimator is $\\hat\\theta_0 = \\hat\\beta_0 + \\hat\\beta_1 c_1 + \\cdots + \\hat\\beta_k c_k$\n",
    "\n",
    "And the standard error for the confident interval: $\\hat\\theta_0 \\pm z^*\\cdot \\text{se}\\P{\\hat\\theta_0}$. How to find that? We can write\n",
    "\n",
    "$$\\begin{align}\n",
    "y &= \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_k x_k + u \\\\\n",
    "&= \\P{\\theta_0 -\\beta_1 c_1 - \\cdots - \\beta_k c_k} + \\beta_1 x_1 + \\cdots + \\beta_k x_k + u\\\\\n",
    "&= \\theta_0 + \\beta_1\\P{x_1 - c_1} + \\beta_2 \\P{x_2 - c_2} + \\cdots + \\beta_k \\P{x_k - c_k} + u\n",
    "\\end{align}$$\n",
    "\n",
    "Then run regression for $y$ on $x_i-c_i$. The intercept, that's the $\\hat\\theta_0$. Then we put the *variance in the unobserved error* into consideration.\n",
    "\n",
    "$x_1^0,x_2^0,\\dots,x_k^0$ are the new values of the independent variables, $u^0$ is the unobserved error. Then we have\n",
    "\n",
    "$$y^0 = \\beta_0 + \\beta_1 x_1^0 + \\cdots + \\beta_k x_k^0 +u^0$$\n",
    "\n",
    "As before, our best prediction of $y^0$ is the expected value of $y^0$ given the explanatory variables. So there's actually a ***prediction error***:\n",
    "\n",
    "$$\\hat e^0 = y^0 - \\hat y^0 = \\P{\\beta_0 + \\beta_1 x_1^0 + \\cdots + \\beta_k x_k^0 } + u^0 - \\hat y^0$$\n",
    "\n",
    "and by the **zero mean** of the unobserved error and the unbiasedness of the parameters we have\n",
    "\n",
    "$$\\Exp\\SB{\\hat e^0} = \\P{\\beta_0 + \\beta_1 x_1^0 + \\cdots + \\beta_k x_k^0 } + 0 - \\P{\\beta_0 + \\beta_1 x_1^0 + \\cdots + \\beta_k x_k^0 } = 0$$\n",
    "\n",
    "However, its variance is not $0$. The ***variance of the prediction error*** (conditional on all in-sample values of the independent variables) is\n",
    "\n",
    "$$\\Var{\\hat e^0} = \\Var{\\hat y^0} + \\Var{u^0} = \\Var{\\hat y^0} + \\sigma^2$$\n",
    "\n",
    "$$\\text{se}\\P{\\hat e^0} = \\CB{\\SB{\\text{se}\\P{\\hat y^0}}^2+\\hat\\sigma^2}^{0.5}$$\n",
    "\n",
    "Using the same reasoning for the $t$ statistics of the $\\hat\\beta_j$, $\\ffrac{\\hat e^0} {\\text{se}\\P{\\hat e^0}}$ has a $t$ distribution with $df = n-\\P{k+1}$. Therefore,\n",
    "\n",
    "$$P\\CB{-t_{0.025} \\leq \\ffrac{\\hat e^0} {\\text{se}\\P{\\hat e^0}} \\leq t_{0.025}} = 0.95$$\n",
    "\n",
    "and the ***prediction interval*** is $\\hat y^0 \\pm t_{0.025} \\cdot \\text{se} \\P{\\hat e^0}$, which is *wider* than the confidence interval for $\\hat y^0$ because of there's an extra $\\hat\\sigma^2$ when calculating $\\text{se}\\P{\\hat e^0}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Analysis\n",
    "### Predicting $y$ When $\\log\\P{y}$ Is the Dependent Variable\n",
    "\n",
    "The model is: $\\log\\P{y} = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_k x_k +u$. Then, the predict $\\log\\P{y}$ is $\\widehat{\\log\\P{y}} = \\hat\\beta_0 +\\hat\\beta_1 x_1 + \\hat\\beta_2 x_2 + \\cdots + \\hat\\beta_k x_k$. However, we CANNOT predict $y$ as $\\hat y =\\exp\\P{\\widehat{\\log\\P{y}}}$. With the assumption $\\text{MLR}.1$ through $\\text{MLR}.6$, we have\n",
    "\n",
    "$$\\Exp\\SB{y \\mid \\mathbf{x}} = \\exp\\P{\\ffrac{\\sigma^2} {2}} \\cdot\\exp\\P{\\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_k x_k}$$\n",
    "\n",
    ">$Remark$\n",
    ">\n",
    ">Here the term $\\exp\\P{\\ffrac{\\sigma^2} {2}}$ can be obtained by the moment generating function for normal distribution $r.v.$\n",
    "\n",
    "Then the actual prediction should be $\\hat y = \\exp\\P{\\ffrac{\\hat\\sigma^2} {2}} \\cdot\\exp\\P{\\widehat{\\log\\P{y}}}$. And the truth is that this prediction is not unbiased, but consistent.\n",
    "\n",
    "And while this method depend on the normality of the error term, $u$. Here's an alternative way to avoid that:\n",
    "\n",
    "If we just assume that $u$ is independent of the explanatory variables, then we have\n",
    "\n",
    "$$\\Exp\\SB{y\\mid \\mathbf{x}} = \\alpha_0 \\exp\\P{\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_k x_k},\\bspace \\alpha_0 = \\Exp\\SB{\\exp\\P{u}}>1$$\n",
    "\n",
    "If we know what's $\\hat\\alpha_0$ then we can directly say that $\\hat y = \\hat\\alpha_0\\exp\\P{\\widehat{\\log\\P{y}}}$. So how to estimate $\\alpha_0$ without the normality assumption?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smearing Estimate\n",
    "\n",
    "The first method is based on $\\alpha_0 = \\Exp\\SB{\\exp\\P{u}}$. We replace the unobserved errors $u_i$ to the corresponding OLS residuals: $\\hat u_i = \\log\\P{y_i} - \\hat\\beta_0 - \\hat\\beta_1x_{i1} - \\hat\\beta_2 x_{i2} - \\cdots - \\hat\\beta_k x_{ik}$, so that\n",
    "\n",
    "$$\\hat\\alpha_0 = \\ffrac{1} {n} \\sum_{i=1}^{n} \\exp\\P{\\hat u_i}$$\n",
    "\n",
    "A **moments estimator**! And it's consistent but not unbiased, because we have replaced $u_i$  with $\\hat u_i$ inside a nonlinear function. This version of $\\hat\\alpha_0$ is called the ***smearing estimate***.\n",
    "\n",
    "### Regression Estimate \n",
    "\n",
    "A different estimate of $\\alpha_0$ is based on a *simple regression through the origin*. We first define:\n",
    "\n",
    "$$m_i = \\exp\\P{\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_k x_{ik}}$$\n",
    "\n",
    "so that $\\Exp\\SB{y_i\\mid m_i} = \\alpha_0 m_i$ and if we could really observe what $m_i$ is, we could obtain an *unbiased* estimator of $\\alpha_0$ from the regression $y_i$ on $m_i$ without an intercept.\n",
    "\n",
    "Well, not possible. So we replace the $\\beta_j$ with their OLS estimates and obtain $\\hat m_i = \\exp\\P{\\widehat{\\log\\P{y_i}}}$ where, again, $\\log\\P{y_i}$ are the fitted values from the regression $\\log\\P{y_i}$ on $x_{i1}, x_{i2}, \\dots, x_{ik}$ (here is a different regression with an intercept, just to obtain $\\log\\P{y_i}$). Then the OLS slope estimate from the simple regression $y_i$ on $m_i$ (without an intercept): \n",
    "\n",
    "$$\\check{\\alpha}_0 = \\ffrac{\\d{\\sum_{i=1}^{n} \\hat m_i y_i}} {\\d{\\sum_{i=1}^{n} \\hat m_i^2}}$$\n",
    "\n",
    "And this is the ***regression estimate*** of $\\alpha_0$, still, consistent but not unbiased.\n",
    "\n",
    "$Remark$\n",
    "\n",
    "> **smearing estimate** can gurantee that the estimates are greater than $1$ while **regression estimate** cna't. **regression estimate** will be much less than $1$ when the assumption of independence between $u$ and $x_j$ is violated.\n",
    "\n",
    "$Steps$\n",
    "\n",
    "1. Obtain the fitted values, $\\widehat{\\log\\P{y_i}}$, and residuals, $\\hat u_i$, from the regression $\\log\\P{y}$ on $x_{1}, x_{2}, \\dots, x_{k}$, with intercept.\n",
    "2. Calculate $\\hat\\alpha_0$ or $\\check\\alpha_0$\n",
    "3. For given values of $x_{1}, x_{2}, \\dots, x_{k}$, obtain $\\widehat{\\log\\P{y}}$ from $\\hat y = \\hat\\alpha_0\\exp\\P{\\widehat{\\log\\P{y}}}$\n",
    "4. Obtain the prediction $\\hat y$ also from the preceding equation\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further, what's the goodness-of-fit for our $\\log\\P{y}$. One that is easy to implement, and have the same value whether we estimate $\\alpha_0$ as $\\hat\\alpha_0 = \\Exp\\SB{\\exp\\P{u}} = \\exp\\P{\\ffrac{\\hat\\sigma^2} {2}} $, or $\\hat\\alpha_0 = \\ffrac{1} {n} \\sum_{i=1}^{n} \\exp\\P{\\hat u_i}$, or $\\check{\\alpha}_0 = \\ffrac{\\d{\\sum\\nolimits_{i=1}^{n} \\hat m_i y_i}} {\\d{\\sum\\nolimits_{i=1}^{n} \\hat m_i^2}}$. The $R$-squared is defined as \n",
    "\n",
    "$$R^2 = 1 - \\ffrac{\\text{SSR}} {\\text{SST}}$$\n",
    "\n",
    "or, just the square of the correlation between $y_i$ and $\\hat y_i$, $\\rho^2\\P{y_i,\\hat y_i}$. So when the independent variable is $\\log\\P{y}$ we would define\n",
    "\n",
    "$$R^2 = \\rho^2\\P{y_i,\\hat y_i} = \\rho^2\\P{y_i,\\hat \\alpha_0 m_i} = \\rho^2\\P{y_i,\\hat m_i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "176px",
    "width": "287px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
