{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple regression analysis: Inference\n",
    "## Sampling Distributions of the OLS Estimators\n",
    "\n",
    "To make statistical inference on $\\DeclareMathOperator*{\\argmin}{argmin}\n",
    "\\DeclareMathOperator*{\\argmax}{argmax}\n",
    "\\newcommand{\\using}[1]{\\stackrel{\\mathrm{#1}}{=}}\n",
    "\\newcommand{\\ffrac}{\\displaystyle \\frac}\n",
    "\\newcommand{\\space}{\\text{ }}\n",
    "\\newcommand{\\bspace}{\\;\\;\\;\\;}\n",
    "\\newcommand{\\QQQ}{\\boxed{?\\:}}\n",
    "\\newcommand{\\void}{\\left.\\right.}\n",
    "\\newcommand{\\CB}[1]{\\left\\{ #1 \\right\\}}\n",
    "\\newcommand{\\SB}[1]{\\left[ #1 \\right]}\n",
    "\\newcommand{\\P}[1]{\\left( #1 \\right)}\n",
    "\\newcommand{\\dd}{\\mathrm{d}}\n",
    "\\newcommand{\\Tran}[1]{{#1}^{\\mathrm{T}}}\n",
    "\\newcommand{\\d}[1]{\\displaystyle{#1}}\n",
    "\\newcommand{\\EE}[2][\\,\\!]{\\mathbb{E}_{#1}\\left[#2\\right]}\n",
    "\\newcommand{\\Var}[2][\\,\\!]{\\mathrm{Var}_{#1}\\left[#2\\right]}\n",
    "\\newcommand{\\Cov}[2][\\,\\!]{\\mathrm{Cov}_{#1}\\left(#2\\right)}\n",
    "\\newcommand{\\Corr}[2][\\,\\!]{\\mathrm{Corr}_{#1}\\left(#2\\right)}\n",
    "\\newcommand{\\I}[1]{\\mathrm{I}\\left( #1 \\right)}\n",
    "\\newcommand{\\N}[1]{\\mathrm{N} \\left( #1 \\right)}\n",
    "\\newcommand{\\ow}{\\text{otherwise}}\\hat\\beta_j$, more than just first two moment of it is needed; we need to know the full sampling distribution of the $\\hat\\beta_j$, which cannot be obtained just under **Gauss-Markov assumptions**.\n",
    "\n",
    "Remember that the OLS estimators depend also on the underlying distribution of the errors. Thus to track $\\hat\\beta_j$, another assumption is needed.\n",
    "\n",
    "$Assumption$ $\\text{MLR}.6$ (Normality)\n",
    "\n",
    "The **population error** $u$ is independent of the explanatory variables $x_1, x_2, \\dots, x_k$ and is normally distributed with mean $0$ and variance $\\sigma^2: u \\sim \\N{0,\\sigma^2}$\n",
    "\n",
    "$Remark$\n",
    "\n",
    "$\\text{MLR}.6$ is much stronger than any of our previous assumptions. It can be derived that $\\EE{u \\mid x_1,x_2, \\dots, x_k} = \\EE{u} = 0$ and $\\Var{u \\mid x_1,x_2, \\dots, x_k} = \\Var{u} = \\sigma^2$.\n",
    "\n",
    "And for cross-sectional regression applications, $\\text{MLR}.1$ to $\\text{MLR}.6$ are called the ***classical linear model (CLM)*** assumptions. And thus we call the model under these six assumptions the ***classical linear model***.\n",
    "\n",
    "***\n",
    "Under the **CLM** assumptions, the OLS estimators $\\hat\\beta_0, \\hat\\beta_1, \\dots, \\hat\\beta_k$ have a stronger  efficiency property than they would under the Gauss-Markov assumptions: they are the ***minimum variance unbiased estimators***.\n",
    "\n",
    "$Remark$\n",
    "\n",
    "Summary for the population assumptions of the **CLM**: $y\\mid \\mathbf{x} \\sim \\N{\\beta_0 + \\beta_1x_1 + \\cdots + \\beta_kx_k,\\sigma^2}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Theorem.1$\n",
    "\n",
    "Under the CLM assumptions $\\text{MLR}.1$ through $\\text{MLR}.6$, conditional on the sample values of the independent variables, we have\n",
    "\n",
    "$$\\hat\\beta_j \\sim \\N{\\beta_j, \\Var{\\hat\\beta_j}} \\\\\n",
    "\\ffrac{\\hat\\beta_j - \\beta_j} {\\sqrt{\\Var{\\hat\\beta_j}}} = \\ffrac{\\hat\\beta_j - \\beta_j} {\\text{sd} \\P{ \\hat\\beta_j}} \\sim \\N{0,1}$$\n",
    "\n",
    "$Proof$\n",
    "\n",
    ">Remember we've write $\\hat\\beta_j = \\beta_j + \\sum_{i=1}^{n} w_{ij} u_{i}$ where $w_{ij} = \\ffrac{\\hat r_{ij}} {\\text{SSR}_j}$, the $\\texttt{i-}$th residual from the regression of the $x_j$ on all the other variables, over the sum of squared residuals from that regression. Since $w_{ij}$ can be treated as nonrandom, we can find the linearity of $\\hat\\beta_j$, it's a linear combination of the errors in the sampel, $\\CB{u_i: i = 1,2,\\dots, n}$. Under $\\text{MLR}.6$ and $\\text{MLR}.2$, the errors are $i.i.d.$ $\\N{0,\\sigma^2}$ $r.v.$. Basically that's it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Hypotheses about a Single Population Parameter: The $t$ Test \n",
    "\n",
    "Still we assume the model as $y = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_k x_k + u$, and the **CLM** assumptions.\n",
    "\n",
    "$Theorem.2$ $t$ DIStrIbutIoN for the StaNDarDIzeD eStIMatorS\n",
    "\n",
    "Under the **CLM** assumptions $\\text{MLR}.1$ through $\\text{MLR}.6$,\n",
    "\n",
    "$$\\ffrac{\\hat\\beta_j - \\beta_j} {\\text{se}\\P{\\hat\\beta_j}} \\sim t_{n-k-1} = t_{\\text{df}}$$\n",
    "\n",
    "$Proof$\n",
    "\n",
    "> $$\\ffrac{\\hat\\beta_j - \\beta_j} {\\text{se}\\P{\\hat\\beta_j}} = \\ffrac{\\ffrac{\\hat\\beta_j - \\beta_j} {\\text{sd}\\P{\\hat\\beta_j}}} {\\sqrt{\\ffrac{\\hat\\sigma^2} {\\sigma^2}}}, \\P{n-k-1} \\ffrac{\\hat\\sigma^2} {\\sigma^2} \\sim \\chi^2_{n-k-1}$$\n",
    "\n",
    "$Remark$\n",
    "\n",
    ">A $t$ statistic can be obtained by a normal ditributed $r.v.$ divided by the square root of a Chi squared $r.v.$ over its $\\text{df}$:\n",
    ">\n",
    ">$$t = \\ffrac{Z} {\\sqrt{X/n}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we set the *null hypothesis* as $H_0: \\beta_j = 0$. And the test statistic we are gonna use is \"the\" $t$ statistic, defined as\n",
    "\n",
    "$$t_{\\hat\\beta_j} = \\ffrac{\\hat\\beta_j}{\\text{se} \\P{\\hat\\beta_j}}$$\n",
    "\n",
    "- $t_{{\\hat\\beta_j}}$ has the same sign with $\\hat\\beta_j$ since $\\text{se} \\P{\\hat\\beta_j} > 0$\n",
    "- A larger $\\hat\\beta_j$ leads to a larger value of $t_{\\hat\\beta_j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Against One-Sided Alternatives\n",
    "\n",
    "*one-sided* ***alternative hypothesis***: $H_1: \\beta_j >0$. Then to reject null or alternative hypothesis, we need a ***significance level***, *the probability of rejecting* $H_0$ *when in fact it's true*. Thus if we really want to reject $H_0$, we need a \"sufficient large base line\" thus when $t_{\\hat\\beta_j}$ is big enough we can draw the conclusion that we'll mistakenly reject $H_0$ when it's true only $5\\%$ of the time if $t_{\\hat\\beta_j}$ is \"sufficient large\".\n",
    "\n",
    "That base line is called ***critical value***: calculated from the $95$th percentile in a $t$ distribution with $n-k-1$ degrees of freedom; denoted by $c$, also can be found in statistic table.\n",
    "\n",
    "All above is called the ***one-tailed test***, and the conclusion could be that (if reject $H_0$) rejection of $H_0$ will occur for $5\\%$ of all random samples when $H_0$ is true.\n",
    "\n",
    "$Remark$\n",
    "\n",
    "When the alternative is $H_1: \\beta_j <0$, we can just change the rejection rule to $t_{\\hat\\beta_j} < -c$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two-Sided Alternatives\n",
    "\n",
    "***Two-Sided Alternatives***: $H_1: \\beta_j \\neq 0$ while $H_0$ keep unchanged. \n",
    "\n",
    "Also there's a slight change on the rejection rule: $\\left|t_{\\hat\\beta_j}\\right| > c$. The difference here is, say, when the critical value is still $5\\%$, since it's now two-sided, now $c$ is $95$th percentile in a $t$ distribution with $n-k-1$ degrees of freedom.\n",
    "\n",
    "| Left-Sided Alternative | Right-Sided Alternative | Two-Sided Alternative |\n",
    "| :--------------------: | :----------------------:| :-------------------: |\n",
    "| ![](./figs/leftSideAlternative.png)      | ![](./figs/rightSideAlternative.png)  | ![](./figs/twoSideAlternative.png) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Other Hypotheses About $\\beta_j$\n",
    "\n",
    "Now $H_0: \\beta_j = a_j$ where $a_j$ is our hypothesized value of $\\beta_j$. Then the appropriate $t$ statistic is $t = \\ffrac{\\hat\\beta_j - a_j}{\\text{se} \\P{\\hat\\beta_j}}$\n",
    "\n",
    "### Computing $p$-Values for $t$ Tests\n",
    "\n",
    "Some scientists don't like the **significant level** as pre-given. They prefer use the statistic to find another quantity: ***$p$-value***, *the smallest significance level at which the null hypothesis would be rejected*, thus it's a *probability* as well. It can be found in the table, or, from your hand:\n",
    "\n",
    "$$\\begin{cases}\n",
    "P\\CB{\\left|T\\right| > \\left|t\\right|}, & \\text{two-sided alternative} \\\\\n",
    "P\\CB{T < t} = P\\CB{T > \\left|t\\right|}, & \\text{left-sided alternative} \\\\\n",
    "P\\CB{T > t}, & \\text{right-sided alternative} \\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "where, $T$ denote a $t$ distributed $r.v.$ with $n - k - 1$ degrees of freedom and $t$ denote the numerical value of the test statistic (this one is obtained from data). When it's small, we reject the null hypothesis and when it's large, we favor the null hypothesis.\n",
    "\n",
    "$Remark$\n",
    "\n",
    ">For the same $T$ and $t$, those $p$-value of one-sided alternative is the half of those of two-sided alternative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Reminder on the Language of Classical Hypothesis Testing\n",
    "\n",
    "When $H_0$ is not rejected, we prefer to use the language \"we fail to reject $H_0$ at the $x\\%$ level\". \n",
    "\n",
    "### Economic, or Practical, versus Statistical Significance\n",
    "\n",
    "In practise, $t_{\\hat\\beta_j} = \\ffrac{\\hat\\beta_j} {\\text{se}\\P{\\hat\\beta_j}}$ can be small either because of a small $\\text{se}\\P{\\hat\\beta_j}$ or a big $\\hat\\beta_j$. The **statistical significance** of a variable $x_j$ is determined entirely by the size of $t_{\\hat\\beta_j}$ whereas the ***economic significance (practical)*** of a variable is related to the size and sign of $\\hat\\beta_j$.\n",
    "\n",
    "$Remark$\n",
    "\n",
    "If a variable is statistically and economically important but has the \"wrong\" sign (those can't make any sense 😂😂😂), the regression model might be misspecified!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence Intervals\n",
    "\n",
    "For a population parameter $\\beta_j$ we can construct a ***confidence interval (CI)*** which provide a range of likely values for the population parameter.\n",
    "\n",
    "Since $\\ffrac{\\hat\\beta_j - \\beta_j} {\\text{se} \\P{\\hat\\beta_j}}$ has a $t$ distribution with $n-k-1$ degrees of freedom. A $95\\%$ confidence interval is given by $\\hat\\beta_j \\pm c \\cdot \\text{se} \\P{\\hat\\beta_j}$ where the constant $c$ is the $97.5^{\\text{th}}$ percentile in a $t_{n-k-1}$ distribution.\n",
    "\n",
    "$Remark$\n",
    "\n",
    ">The meaning of **CI**: if random samples are obtained again and again, with the upper and lower bond computed each time, then the (unknown) population value $\\hat\\beta_j$ would lie in the interval $\\hat\\beta_j \\pm c \\cdot \\text{se} \\P{\\hat\\beta_j}$ for $95\\%$ of the samples. \n",
    ">\n",
    ">And when $df$ is greater than $120$, we can use normal distribution to replace the $t$ distribution to find the percentile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Hypotheses about a Single Linear Combination of the Parameters\n",
    "\n",
    "Now the test hypothesis is $H_0 : \\beta_1 = \\beta_2$ and $H_1 : \\beta_1 < \\beta_2$ or $\\neq$ whatever. And the corresponding test statistisc is $t = \\ffrac{\\hat\\beta_1 - \\hat\\beta_2} {\\text{se} \\P{\\hat\\beta_1 - \\hat\\beta_2}}$ while the test procedures are the same.\n",
    "\n",
    "Even thought both the two can be both economically and statistically significant, their difference may not. First we can use the regression line to find the value of the numerator $\\hat\\beta_j - \\beta_j$; then calculate the denominator, since $\\SB{\\text{se}\\P{\\hat\\beta_1}}^2$ is an unbiased estimator of $\\Var{\\hat\\beta_1}$, and similarly for $\\SB{\\text{se}\\P{\\hat\\beta_2}}^2$, we have\n",
    "\n",
    "$$\\text{se}\\P{\\hat\\beta_1 - \\hat\\beta_2} = \\sqrt{\\SB{\\text{se}\\P{\\hat\\beta_1}}^2 + \\SB{\\text{se}\\P{\\hat\\beta_2}}^2 - 2s_{12}}$$\n",
    "\n",
    "here $s_{12}$ is an *estimate* of $\\Cov{\\hat\\beta_1,\\hat\\beta_2}$ but that's not some easy work. We now present an alternative approach. Let $\\theta_1 = \\beta_1 - \\beta_2$. Then the test hypothesis can be written as: $H_0: \\theta_1 = 0$ against $H_1: \\theta_1 < 0$.\n",
    "\n",
    "And still $t = \\ffrac{\\hat\\theta_1} {\\text{se}\\P{\\hat\\theta_1}}$ but the key point is still to find the denominator. To do so, we rewrite the model: $\\P{\\theta_1 + \\beta_2}x_1 + \\beta_2 x_2 \\to \\theta_1 x_1 + \\beta_2\\P{x_1 + x_2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Multiple Linear Restrictions: The $F$ Test\n",
    "\n",
    "### Testing Exclusion Restrictions\n",
    "\n",
    "Now the null hypothesis is that a set of variables has no effect on $y$, once another set of variables has been controlled. And an introductory example could be $H_0: \\beta_3 = 0, \\beta_4 = 0, \\beta_5 = 0$, with three ***exclusion restrictions*** and $H_1: H_0$ is not true. And this kind of test is called a ***multiple hypotheses test*** or a ***joint hypotheses test***.\n",
    "\n",
    "If we do $t$ test on these three parameters separately, we may find that none of them is to be rejected and thus we cannot reject $H_0$. However that's not true. And before revealing the truth, we have two more definitions: the ***unrestricted model*** is the original model with full parameters in, while the ***restricted model*** has fewer parameters. OK, now see the general case, and first we write the **unrestricted model**.\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_k x_k + u$$\n",
    "\n",
    "with $k+1$ number of parameters. Suppose that we have $q$ exclusion restrictions to test, assumed to be last $q$ parameters. Then the null hypothesis is: $H_0: \\beta_{k-q+1} = \\beta_{k-q+2} = \\cdots = \\beta_k = 0$ and the alternative hypothesis is that $H_1: H_0$ is not true. Then when we impose the restrictions under $H_0$, we are left with the **restricted** model:\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_{k-q}x_{k-q} + 0 + u$$\n",
    "\n",
    "Then here's the test statistic: $F$ statistic (of $F$ ratio).\n",
    "\n",
    "$$F \\equiv \\ffrac{\\ffrac{\\text{SSR}_r - \\text{SSR}_{ur}} {q}} {\\ffrac{\\text{SSR}_{ur}} {n-k-1}} \\begin{array}{c}\n",
    "\\\\\n",
    "\\longleftarrow \\textbf{numerator degrees of freedom}\\\\\n",
    "\\\\\n",
    "\\longleftarrow \\textbf{denominator degrees of freedom} = df_{ur}\n",
    "\\end{array}$$\n",
    "\n",
    "where $\\text{SSR}_r$ is the sum of squared residuals from the **restricted model** and $\\text{SSR}_{ur}$ is the sum of squared residuals from the **unrestricted model**. And $q$ is the **numerator degrees of freedom** which is $q = df_r - df_{ur} = \\P{n-\\P{k-q}-1} - \\P{n-k-1}$\n",
    "\n",
    "$Remark$\n",
    "\n",
    ">$\\text{SSR}_r \\geq \\text{SSR}_{ur}$ thus $F$ statistic is *nonnegative*.\n",
    ">\n",
    ">Also, the denominator of $F$ is just the unbiased estimator of $\\sigma^2 = \\Var{u}$ in the unrestricted model.\n",
    "\n",
    "And under $H_0$, $F$ is distributed as an $F$ $r.v.$ with $\\P{q,n-k-1}$ degrees of freedom, written as $F \\sim F_{q,n-k-1}$. The definition of $F$ distribution can be found elsewhere, omitted. The test process is first, to look up the statistical table and find critical value $c$ given $q$ and $n-k-1$. then we reject $H_0$ in favor of $H_1$ at the chosen significance level, if $F > c$.\n",
    "\n",
    "Then when $H_0$ is rejected, we say that $x_{k-q+1},x_{k-q+2},\\dots,x_k$ are ***jointly statistically significant*** at that significance level.                                                                         \n",
    "\n",
    "### Relationship Between $F$ and $t$ Statistics\n",
    "\n",
    "$F$ statistic can be used to test whether a group of variables should be included in a model while $t$ statistic is designed for one parameter. What if we try to apply $F$ test on one parameter? The result will be slightly different from the result from $t$ test, since $t^2_{n-k-1}$ is equivalent to $F_{1,n-k-1}$, so even their outcomes or the final conclusions are the same, the computed test statistics are different.\n",
    "\n",
    "Another difference is that it's way easier to do $t$ test on one-sided alternatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The $R$-Squared Form of The $F$ Statistic\n",
    "\n",
    "The $\\text{SSR}$ can be huge while $R$-Squared is limited from $0$ to $1$. We now present $F$ statistic in another form. Notice that $\\text{SSR}_r = \\text{SST}\\cdot\\P{1-R_r^2}$ and $\\text{SSR}_{ur} = \\text{SST}\\cdot\\P{1-R_{ur}^2}$, then\n",
    "\n",
    "$$F = \\ffrac{\\ffrac{\\text{SSR}_r - \\text{SSR}_{ur}} {q}} {\\ffrac{\\text{SSR}_{ur}} {n-k-1}} = \\ffrac{\\ffrac{R^2_{ur} - R^2_{r}} {q}} {\\ffrac{1-R^2_{ur}} {n-k-1}} $$\n",
    "\n",
    "### Computing $p$-Values for $F$ Tests\n",
    "\n",
    "In the $F$ testing context, the $p$-value is defined as $P\\CB{\\mathcal{F} > F}$. Here $\\mathcal{F}$ is the $F$ $r.v.$ with $\\P{q,n-k-1}$ degrees of freedom and $F$ on the right hand of the inequity is the actual value of the test statistic.\n",
    "\n",
    "**(V)**\n",
    "\n",
    ">The meaning of $p$-value here is still the probability of observing a value of $F$ at least as large as we did, given that the null hypothesis is true. A small $p$-value is evidence against $H_0$. For example, $p\\text{-value} = 0.16$ means that the chance of observing a value of $F$ as large as we  did when the null hypothesis was true is only $1.6\\%$ and thus we usually reject $H_0$ in such cases. However if $p\\text{-value} = 3.14$, then the chance of observing a value of the $F$ statistic as large as we did under the null hypothesis is $31.4\\%$. Most would find this to be pretty weak evidence against $H_0$.\n",
    "\n",
    "### The $F$ Statistic for Overall Significance of a Regression\n",
    "\n",
    "$H_0:x_1,x_2,\\dots,x_k$ can't help to explain $y \\iff H_0: \\beta_1 = \\beta_2 = \\cdots = \\beta_k = 0 \\iff H_0: \\EE{y \\mid x_1,x_2,\\dots,x_k} = \\EE{y}$ \n",
    "\n",
    "And if so, the restricted model would be $y = \\beta_0 + u$, then $R^2_{r} = 0, q = k$ and thus we have:\n",
    "\n",
    "$$F = \\ffrac{\\ffrac{R^2} {k}} {\\ffrac{1-R^2} {n-k-1}}$$\n",
    "\n",
    "where $R^2$ is just the $R$-squared from the regression of $y$ on $x_1,x_2,\\dots,x_k$. And this can be only used for testing joint exclusion of *all* independent variables, called the ***overall significance of the regression***.\n",
    "\n",
    "### testing general linear restrictions\n",
    "\n",
    "What if $H_0: \\beta_1 = 1, \\beta_2 = \\beta_3 = \\cdots = \\beta_k = 0$? We can just change the restricted model to $y - x_1 = \\beta_0 + u$.\n",
    "\n",
    "$$F = \\ffrac{\\ffrac{\\text{SSR}_r - \\text{SSR}_{ur}} {k}} {\\ffrac{\\text{SSR}_{ur}} {n-k-1}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Under the classical linear model assumptions $\\text{MLR}.1$ through $\\text{MLR}.6$, the OLS estimators are normally distributed.\n",
    "- Under the CLM assumptions ($\\text{MLR}.1$ through $\\text{MLR}.6$), the $t$ statistics have $t$ distributions under the null hypothesis.\n",
    "- We use $t$ statistics to test hypotheses about a single parameter against one- or two-sided alternatives, using one- or two-tailed tests, respectively. The most common null hypothesis is $H_0: \\beta_j = 0$, but we sometimes want to test other values of $\\beta_j$ under $H_0$.\n",
    "-  In classical hypothesis testing, we first choose a significance level, which, along with the $df$ and alternative hypothesis, determines the critical value against which we compare the $t$ statistic. It is more informative to compute the $p$-value for a $t$ test—the smallest significance level for which the null hypothesis is rejected—so that the hypothesis can be tested at any significance level.\n",
    "- Not finished yet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "69px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
