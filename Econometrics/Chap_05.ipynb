{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Regression Analysis: OLS Asymptotics\n",
    "\n",
    "So other than the finite sample properties in the previous chapters, we also need to know the ***asymptotic  properties*** or ***large sample properties*** of estimators and test statistics. And fortunately, under the assumptions we have made, OLS has satisfactory large sample  properties.\n",
    "\n",
    "$Review\\DeclareMathOperator*{\\argmin}{argmin}\n",
    "\\DeclareMathOperator*{\\argmax}{argmax}\n",
    "\\DeclareMathOperator*{\\plim}{plim}\n",
    "\\newcommand{\\using}[1]{\\stackrel{\\mathrm{#1}}{=}}\n",
    "\\newcommand{\\ffrac}{\\displaystyle \\frac}\n",
    "\\newcommand{\\space}{\\text{ }}\n",
    "\\newcommand{\\bspace}{\\;\\;\\;\\;}\n",
    "\\newcommand{\\QQQ}{\\boxed{?\\:}}\n",
    "\\newcommand{\\void}{\\left.\\right.}\n",
    "\\newcommand{\\CB}[1]{\\left\\{ #1 \\right\\}}\n",
    "\\newcommand{\\SB}[1]{\\left[ #1 \\right]}\n",
    "\\newcommand{\\P}[1]{\\left( #1 \\right)}\n",
    "\\newcommand{\\abs}[1]{\\left| #1 \\right|}\n",
    "\\newcommand{\\norm}[1]{\\left\\| #1 \\right\\|}\n",
    "\\newcommand{\\dd}{\\mathrm{d}}\n",
    "\\newcommand{\\Tran}[1]{{#1}^{\\mathrm{T}}}\n",
    "\\newcommand{\\d}[1]{\\displaystyle{#1}}\n",
    "\\newcommand{\\RR}{\\mathbb{R}}\n",
    "\\newcommand{\\EE}{\\mathbb{E}}\n",
    "\\newcommand{\\NN}{\\mathbb{N}}\n",
    "\\newcommand{\\ZZ}{\\mathbb{Z}}\n",
    "\\newcommand{\\QQ}{\\mathbb{Q}}\n",
    "\\newcommand{\\AcA}{\\mathscr{A}}\n",
    "\\newcommand{\\FcF}{\\mathscr{F}}\n",
    "\\newcommand{\\Exp}{\\mathrm{E}}\n",
    "\\newcommand{\\Var}[2][\\,\\!]{\\mathrm{Var}_{#1}\\left[#2\\right]}\n",
    "\\newcommand{\\Cov}[2][\\,\\!]{\\mathrm{Cov}_{#1}\\left(#2\\right)}\n",
    "\\newcommand{\\Corr}[2][\\,\\!]{\\mathrm{Corr}_{#1}\\left(#2\\right)}\n",
    "\\newcommand{\\I}[1]{\\mathrm{I}\\left( #1 \\right)}\n",
    "\\newcommand{\\N}[1]{\\mathrm{N} \\left( #1 \\right)}\n",
    "\\newcommand{\\ow}{\\text{otherwise}}$\n",
    "\n",
    "1. Expected values unbiasedness: $\\text{MLR}.1 \\sim \\text{MLR}.4$\n",
    "2. Variance formulas: $\\text{MLR}.1 \\sim \\text{MLR}.5$\n",
    "3. Gauss-Markov Theorem: $\\text{MLR}.1 \\sim \\text{MLR}.5$\n",
    "4. Exact sampling distributions/tests: $\\text{MLR}.1 \\sim \\text{MLR}.6$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consistency\n",
    "\n",
    "In practise, time series data regressions will fail the unbiasedness, only **consistency** remains.\n",
    "\n",
    "$Def$\n",
    "\n",
    ">Let $W_n$ be an estimator of $\\theta$ based on a sample $Y_1,Y_2,\\dots,Y_n$ of size $n$. Then, $W_n$ is a consistent estimator of $u$ if for every $\\varepsilon > 0$,\n",
    ">\n",
    ">$$P\\CB{\\abs{W_n - \\theta} > \\varepsilon} \\to 0 \\text{ as } n \\to \\infty  $$\n",
    ">\n",
    ">Or alternatively, for arbitrary $\\epsilon > 0$ and $n \\to \\infty$, we have $P\\CB{\\abs{W_n - \\theta}< \\epsilon} \\to 1$\n",
    "\n",
    "We can also write this as $\\text{plim}\\P{W_n} = \\theta$\n",
    "\n",
    "$Remark$\n",
    "\n",
    ">In our real life we don't have infinite samples thus this property involves a thought experiment about what would happen as the sample size gets *large*.\n",
    "\n",
    "$Theorem.1$\n",
    "\n",
    "Under assumptions $MLR.1$ through $MLR.4$, the OLS estimator $\\hat\\beta_j$ is consistent for $\\beta_j$, for all $j = 0,1,\\dots,k$ meaning that $\\plim\\P{\\hat\\beta_j} = \\beta_j$.\n",
    "\n",
    "$Proof$\n",
    "\n",
    ">$$\n",
    "\\hat\\beta_1 = \\ffrac{\\d{\\sum_{i=1}^{n} \\P{x_{i1} - \\bar{x}_1} y_i}} {\\d{\\sum_{i=1}^{n} \\P{x_{i1} - \\bar{x}_1}^2}}\n",
    "\\\\[0.6em]$$\n",
    ">\n",
    "><center>since $y_i = \\beta_0 + \\beta_1 x_1 + u_i$</center>\n",
    "\n",
    ">$$\\begin{align}\n",
    "\\hat\\beta_1&= \\beta_1 + \\ffrac{\\d{\\sum_{i=1}^{n} \\P{x_{i1} - \\bar{x}_1} u_i}} {\\d{\\sum_{i=1}^{n} \\P{x_{i1} - \\bar{x}_1}^2}}\\\\\n",
    "&= \\beta_1 + \\ffrac{\\d{\\ffrac{1} {n}\\sum_{i=1}^{n} \\P{x_{i1} - \\bar{x}_1} u_i}} {\\d{\\ffrac{1} {n}\\sum_{i=1}^{n} \\P{x_{i1} - \\bar{x}_1}^2}} \\\\[0.5em]\n",
    "\\end{align}\\\\[0.6em]$$\n",
    ">\n",
    "><center> by **law of large number**</center>\n",
    ">\n",
    ">$$\\begin{align}\n",
    "\\plim\\P{\\hat\\beta_1}&= \\beta_1 + \\ffrac{\\Cov{x_1,u}} {\\Var{x_1}}\\\\\n",
    "&= \\beta_1 + \\ffrac{0} {\\Var{x_1}} = \\beta_1\n",
    "\\end{align}$$\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Assumption.4'$ $MLR.4'$ **Zero Mean and Zero Correlation**\n",
    "\n",
    "$\\Exp\\SB{u} = 0$ and $\\Cov{x_j, u} = 0$, for $j = 1,2,\\dots,k$\n",
    "\n",
    "$Remark$\n",
    "\n",
    ">The original one is the assumption of **Zero conditional mean** that is $\\Exp\\SB{u \\mid x_1,x_2,\\dots,x_k} = 0$. $MLR.4$ is stronger than $MLR.4'$.\n",
    ">\n",
    ">Also $MLR.4'$ cannot guarantee the unbiasdness but consistency only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deriving the inconsistency in OLS\n",
    "\n",
    "> If the error $u$ is correlated with any of the independent variables $x_j$, then OLS is biased and inconsistent.\n",
    "\n",
    "In the simple regression case, the ***inconsistency*** in $\\hat\\beta_1$ (or loosely called the ***asymptotic bias***) is $\\plim\\P{\\hat\\beta_1} - \\hat\\beta_1 = \\ffrac{\\Cov{x_1,u}} {\\Var{x_1}}$. And it's positive if $x_1$ and $u$ are positively correlated, negative otherwise.\n",
    "\n",
    "And this formula will help us find the asymptotic analog of the omitted variable bias (ref **Chap_03.3**). Suppose the true model is $y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + v$ and the OLS estimators with **first four Gauss-Markov assumptions** is $\\hat\\beta_0$, $\\hat\\beta_1$, and $\\hat\\beta_2$, and obviously these three are **consistent**. Then omit $x_2$ and do the simple regression of $y$ on $x_1$ with model $y = \\beta_0 + \\beta_1 x_1 + u$, then we have $u = \\beta_2 x_2 + v$. Let $\\tilde\\beta_1$ denote the simple regression slope estimator. Then\n",
    "\n",
    "$$\\plim \\tilde\\beta_1 = \\beta_1 + \\beta_2 \\ffrac{\\Cov{x_1,x_2}} {\\Var{x_1}} = \\beta_1 + \\beta_2 \\delta_1$$\n",
    "\n",
    "If $x_1$ and $x_2$ are *uncorrelated* (in the population), then $\\delta_1 = 0$, and $\\tilde\\beta_1$ is a consistent estimator of $\\beta_1$ (although not necessarily unbiased). However, if $x_2$ has a positive partial effect on $y$, so that $\\beta_2 > 0$ and $\\Cov{x_1,x_2}>0$, $\\delta_1> 0$. Then the inconsistency in $\\tilde\\beta_1$ is positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asymptotic Normality and Large Sample Inference\n",
    "\n",
    ">$\\text{MLR}.6 \\iff $ the distribution of $y$ given $x_1,x_2,\\dots,x_k$, which is just $u$ then, is normal. Normality has nothing to do with the unbiasedness however to do statistical inference, we need that. And fortunately, by **central limit theorem**, even though the $y_i$ are not from a normal distribution, the OLS estimator still satisfy ***asymptotic normality***, which means they are approximately normally distributed in \n",
    "large enough sample sizes.\n",
    "\n",
    "$Theorem.2$ Asymptotic Normality of OLS\n",
    "\n",
    "Under the Gauss-Markov Assumptions, $\\text{MLR}.1$ through $\\text{MLR}.5$, \n",
    "\n",
    "- $\\sqrt{n}\\P{\\hat\\beta_j - \\beta_j} \\newcommand{\\asim}{\\overset{\\text{a}}{\\sim}}\\asim \\N{0, \\ffrac{\\sigma^2} {a_j^2}}$, where $\\ffrac{\\sigma^2} {a_j^2}$ is the ***asymptotic variance*** of $\\sqrt{n}\\P{\\hat\\beta_j - \\beta_j} $; and for the slope coefficients, $a_j^2 = \\plim \\P{\\ffrac{1} {n} \\sum_{i=1}^{n} \\hat r_{ij}^{2}}$ where the $r_{ij}$ are the residuals from regressing $x_j$ on the other independent variables. We say that $\\hat\\beta_j$ is *asymptotically normally distributed*;\n",
    "- $\\hat\\sigma^2$ is a consistent estimator of $\\sigma^2 = \\Var{u}$;\n",
    "- For each $j$, $\\ffrac{\\hat\\beta_j - \\beta_j} {\\text{sd}\\P{\\hat\\beta_j}}\\asim \\N{0,1}$; $\\ffrac{\\hat\\beta_j - \\beta_j} {\\text{se}\\P{\\hat\\beta_j}}\\asim \\N{0,1}$ where $\\text{se}\\P{\\hat\\beta_j} = \\sqrt{\\widehat{\\Var{\\hat\\beta_j}}} = \\sqrt{\\ffrac{\\hat\\sigma^2} {\\text{SST}_j \\P{1-R^2_j}}}$ is the usual OLS standard error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Remark$\n",
    "\n",
    ">Here we dropped the assumption $\\text{MLR}.6$ and the only one restriction remained is that the error has finite variance.\n",
    ">\n",
    ">Also note that the population distribution of the error term, $u$, is immutable and has nothing to do with the sample size. Thie theorem only says that regardless of the population distribution of $u$, the OLS estimators, when properly standardized, have approximate standard normal distributions. \n",
    ">\n",
    ">$\\text{sd}\\P{\\hat\\beta_j}$ depends on $\\sigma$ and is not observable, while $\\text{se}\\P{\\hat\\beta_j}$ depends on $\\hat\\sigma$ and can be computed. In the previous chapter we've already seen that: under **CLM**, $\\text{MLR}.1$ through $\\text{MLR}.6$, we have $\\ffrac{\\hat\\beta_j - \\beta_j} {\\text{sd}\\P{\\hat\\beta_j}}\\sim \\N{0,1}$ and $\\ffrac{\\hat\\beta_j - \\beta_j} {\\text{se}\\P{\\hat\\beta_j}}\\sim t_{n-k-1} = t_{df}$.\n",
    ">\n",
    ">In large samples, the $t$-distribution is close to the $\\N{0,1}$ distribution and thus $t$ test are valid in large samples *without* $\\text{MLR}.6$. But still we need $\\text{MLR}.1$ to $\\text{MLR}.5$.\n",
    "\n",
    "Now from $\\hat\\sigma^2$ is a consistent estimator of $\\sigma^2$, let's have a closer look of ${\\widehat{\\Var{\\hat\\beta_j}}} = {\\ffrac{\\hat\\sigma^2} {\\text{SST}_j \\P{1-R^2_j}}}$, where $\\text{SST}_j$ is the total sum of squares of $x_j$ in the sample, $R^2_j$ is the $R$-squared from regressing $x_j$ on all of the other independent variables. As the **sample size** *grows*, $\\hat\\sigma^2$ converges in probability to the constant $\\sigma^2$. Further, $R^2_j$ approaches a number strictly between $0$ and $1$. Then about the rate, the sample variance of $x_j$ is $\\ffrac{\\text{SST}_j} {n}$, so that it converges to $\\Var{x_j}$ as the sample size grows, meaning that we have: $\\text{SST}_j \\approx n\\sigma_j^2$, where $\\sigma_j^2$ is the population variance of $x_j$. Combining all these facts:\n",
    "\n",
    "$\\bspace \\widehat{\\Var{\\hat\\beta_j}}$ shrinks to zero at the rate of $1/n$, $\\text{se}\\P{\\hat\\beta_j}$ also shrinks to zero at the rate of $\\sqrt{1/n}$ . And the larger sample, the better.\n",
    "\n",
    "When $u$ is not normally distributed, $\\sqrt{\\widehat{\\Var{\\hat\\beta_j}}} = \\sqrt{\\ffrac{\\hat\\sigma^2} {\\text{SST}_j \\P{1-R^2_j}}}$ is called the **asymptotic standard error** and $t$ statistics are called **asymptotic *$\\textbf{t}$* statistics**. We also have **asymptotic confidence interval**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Large Sample Tests: The Lagrange Multiplier Statistic\n",
    "\n",
    "The ***Lagrange multiplier (LM) statistic***. We first consider the model: $y = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_k x_k + u$. The null hypothesis is $H_0:\\beta_{k-q+1} = \\beta_{k-q+2} = \\cdots = \\beta_k = 0$, the last $q$ parameters, putting $q$ exclusion restrictions on the model. The $LM$ statistic requires estimation of the restricted model only. Thus, assume that we have run the regression: $y = \\tilde\\beta_0 + \\tilde\\beta_1 x_1 + \\cdots + \\tilde\\beta_{k-q} x_{k-q} + \\tilde u$, where $\\tilde\\void$ indicates that the estimates are from the restricted model.\n",
    "\n",
    "However it turns out that to get a usable test statistic, we must include *all* of the independent variables in the regression so that we run the regression of $\\tilde u$ on $x_1, x_2,\\dots,x_k$, that we call an ***auxiliary regression***, a regression that is used to compute a test statistic but whose coefficients are not of direct interest.\n",
    "\n",
    "Then under the null hypothesis, the sample size $n$, multiplied by the usual $R$-squared from the auxiliary regression is distributed asymptotically as a $\\chi^2$ $r.v.$ with $q$ degrees of freedom. Here's the overall procedure for testing the joint significance of a set of $q$ independent variables using this method.\n",
    "\n",
    "***\n",
    "\n",
    "<center>Lagrange Multiplier Statistic for $q$ exclusion restrictions</center>\n",
    "\n",
    "1. Regress $y$ on the *restricted set* of independent variables, $x_1,\\dots, x_{k-q}$, and save the residuals, $\\tilde u$\n",
    "2. Regress $\\tilde u$ on *all* of the independent variables and obtain the $R$-squared, $R^2_u$. Just to distinguich from regress $y$ on them\n",
    "3.  Compute **Lagrange multiplier statistic**: $LM = nR_u^2$\n",
    "4.  Compare $LM$ to the appropriate critical value $c$, in a $\\chi_q^2$ distribution; if $LM > c$ then the null hypothesis is *rejected*. Even better, obtain the $p$-value as the probability that a $\\chi_q^2$ $r.v.$ exceeds the value of the test statistic. If the $p$-value is less than the desired significance level, then $H_0$ is rejected. If not, we fail to reject $H_0$. The rejection rule is essentially the same as for $F$ testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asymptotic Efficiency of OLS\n",
    "\n",
    "In general, the $k$ regressor case, the class of consistent estimators is obtained by generalizing the OLS first order conditions:\n",
    "\n",
    "$$\\sum_{i=1}^n g_j\\P{\\mathbf{x}_i} \\P{y_i - \\tilde\\beta_0 - \\tilde\\beta_1 x_{i1} - \\cdots - \\tilde\\beta_k x_{ik}} = 0, \\bspace j = 0,1,\\dots,k$$\n",
    "\n",
    "where $g_j\\P{\\mathbf{x}_i}$ denotes any function of all explanatory variables for observation $i$. And obviously $g_0\\P{\\mathbf{x}_i} = 1$ and $g_j\\P{\\mathbf{x}_j} = x_{ij}$ for $j=1,2,\\dots,k$ are the conditions to obtain the OLS estimators.\n",
    "\n",
    "Here's the theorem:\n",
    "\n",
    "$Theorem.3$ Asymptotic Efficiency of OLS\n",
    "\n",
    "Under the Gauss-Markov assumptions, let $\\tilde\\beta_j$ denote estimators that solve equations of the equation:\n",
    "\n",
    "$$\\sum_{i=1}^n g_j\\P{\\mathbf{x}_i} \\P{y_i - \\tilde\\beta_0 - \\tilde\\beta_1 x_{i1} - \\cdots - \\tilde\\beta_k x_{ik}} = 0, \\bspace j = 0,1,\\dots,k$$\n",
    "\n",
    "and let $\\hat\\beta_j\\newcommand{\\Avar}[2][\\,\\!]{\\mathrm{Avar}_{#1}\\left[#2\\right]}$ denote the OLS estimators. Then for $j=0,1,\\dots,k$, the OLS estimators have the smallest asymptotic variances: $\\Avar{\\sqrt{n} \\P{\\hat\\beta_j - \\beta_j}} \\leq \\Avar{\\sqrt{n} \\P{\\tilde\\beta_j - \\beta_j}}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "129px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
