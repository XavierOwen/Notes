{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Variables\n",
    "\n",
    "$\\newcommand{\\ffrac}{\\displaystyle \\frac}\n",
    "\\newcommand{\\Tran}[1]{{#1}^{\\mathrm{T}}}\n",
    "\\newcommand{\\d}[1]{\\displaystyle{#1}}\n",
    "\\newcommand{\\EE}[2][\\,\\!]{\\mathbb{E}_{#1}\\left[#2\\right]}\n",
    "\\newcommand{\\dd}{\\mathrm{d}}\n",
    "\\newcommand{\\Var}[2][\\,\\!]{\\mathrm{Var}_{#1}\\left[#2\\right]}\n",
    "\\newcommand{\\Cov}[2][\\,\\!]{\\mathrm{Cov}_{#1}\\left(#2\\right)}\n",
    "\\newcommand{\\Corr}[2][\\,\\!]{\\mathrm{Corr}_{#1}\\left(#2\\right)}\n",
    "\\newcommand{\\using}[1]{\\stackrel{\\mathrm{#1}}{=}}\n",
    "\\newcommand{\\I}[1]{\\mathrm{I}\\left( #1 \\right)}\n",
    "\\newcommand{\\N}[1]{\\mathrm{N} \\left( #1 \\right)}\n",
    "\\newcommand{\\space}{\\text{ }}\n",
    "\\newcommand{\\QQQ}{\\boxed{?\\:}}\n",
    "\\newcommand{\\SB}[1]{\\left[ #1 \\right]}\n",
    "\\newcommand{\\P}[1]{\\left( #1 \\right)}\n",
    "\\newcommand{\\abs}[1]{\\left| #1 \\right|}\n",
    "\\newcommand{\\norm}[1]{\\left\\| #1 \\right\\|}\n",
    "\\newcommand{\\CB}[1]{\\left\\{ #1 \\right\\}}$During an experiment, the quantities of interest, or the real-valued functions defined on the sample space are known as the ***Random Variables***, shortened as $r.v.$. And the value of the outcome variable is determined by the outcome of the experiment, we shall assign probabilities to the possible values of the $r.v.$.\n",
    "\n",
    "A special case for this is the ***Indicator Variable***, denoted as $I$ for an event $E$.\n",
    "\n",
    "$$I = \\begin{cases}\n",
    "1 & \\text{if } E \\text{ occurs} \\\\\n",
    "0 & \\text{if } E \\text{ doesn't occur}\n",
    "\\end{cases}$$\n",
    "\n",
    "**e.g.**  \n",
    "Suppose that independent trials, each of which results in any of $m$ possible outcomes with respectively probabilities $p_1, \\dots, p_m$, $\\sum p_i = 1$, are continually performed. Let $X$ denote the number of trials needed until each outcome has occurred at least once. What's $P\\CB{X = n}$?\n",
    "\n",
    ">Instead solve that directly, we first calculate $P\\CB{X = n}$. Let $A_i$ denote the event that outcome $i$ has not yet occured after the first $n$ trials, $i = 1, \\dots, m$.\n",
    ">\n",
    ">$\\begin{align}\n",
    "P\\left\\{X > n\\right\\} &= P\\left( \\bigcup_{i=1}^{m} A_i \\right) \\\\\n",
    "&= \\sum_{i=1}^{m} P(A_i) - \\underset{i<j}{\\sum\\sum} P(A_iA_j) \\\\\n",
    "& \\;\\;\\; + \\underset{i<j<k}{\\sum\\sum\\sum} P(A_iA_jA_k) - \\cdots + (-1)^{m+1}P(A_1 A_2 \\cdots A_m)\n",
    "\\end{align}$\n",
    ">\n",
    ">Now, $P(A_i)$ is the probability that each of the first $n$ trials results in a $\\text{non-}i$ outcome, and so by independence\n",
    ">\n",
    ">$P(A_i) = (1 - p_i)^{n}$\n",
    ">\n",
    ">And similarly, $P(A_iA_j)$ is the probability that the first $n$ trials all result in a $\\text{non-}i$ and $\\text{non-}j$ outcome, and so\n",
    ">\n",
    ">$P(A_iA_j) = (1-p_i - p_j)^{n}$\n",
    ">\n",
    ">As all of the other possibilities are similar, we see that\n",
    ">\n",
    ">$\\begin{align}\n",
    "P\\left\\{X>n\\right\\} = \\sum_{i=1}^{n}(1-p_i)^n - \\underset{i<j}{\\sum\\sum} (1-p_i - p_j)^n + \\underset{i<j<k}{\\sum\\sum\\sum} (1 - p_i - p_j - p_k)^n - \\cdots\n",
    "\\end{align}$\n",
    ">\n",
    ">Since $P\\left\\{X=n\\right\\} = P\\left\\{X>n-1\\right\\} -P\\left\\{X>n\\right\\}$, and $(1-a)^{n-1} - (1-a)^ n = a(1-a)^{n-1}$ that \n",
    ">\n",
    ">$\\begin{align}\n",
    "P\\left\\{X=n\\right\\} &= \\sum_{i=1}^{m} p_i (1-p_i)^{n-1} - \\underset{i<j}{\\sum\\sum} (p_i + p_j) (1-p_i - p_j)^{n-1} \\\\ \n",
    "&\\;\\;\\;+ \\underset{i<j<k}{\\sum\\sum\\sum} (p_i+p_j+p_k)(1 - p_i - p_j - p_k)^n - \\cdots\n",
    "\\end{align}$\n",
    "\n",
    "***\n",
    "Other than this **discrete** $r.v.$, we still have the **continuous** $r.v.$, like the lifetime of the car. \n",
    "\n",
    "We also define the ***culmulative distribution function***, $F(\\cdot)$, of the $r.v.$ $X$, on any real number $b$, $-\\infty < b < \\infty$, by $F(b) = P\\left\\{X \\leq b\\right\\}$. And some properties of the cdf $F$ are:\n",
    "\n",
    "- $F(b)$ is nondecreasing function of $b$.$\\\\[0.7em]$\n",
    "- $\\lim\\limits_{b \\to \\infty} F(b) = F(\\infty) = 1\\\\[0.7em]$\n",
    "- $\\lim\\limits_{b \\to -\\infty} F(b) = F(-\\infty) = 0$\n",
    "\n",
    "Also, we have: $P\\left\\{a < X \\leq b\\right\\} = F(b) - F(a)$ for all $a < b$. And for $P\\left\\{X<b\\right\\}$, we need a new strategy:\n",
    "\n",
    "$$\\begin{align}\n",
    "P\\left\\{X<b\\right\\}&= \\lim_{h \\to 0^+} P\\left\\{X \\leq b-h\\right\\}\\\\\n",
    "&= \\lim_{h \\to 0^+} F(b-h)\n",
    "\\end{align}$$\n",
    "\n",
    "just keep in mind that $P\\left\\{X<b\\right\\}$ *may not* equal to $F(b)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discrete $r.v.$\n",
    "\n",
    "A $r.v.$ that can take on at most a **countable** number of possible values is said to be ***discrete***, say $X$. We can define its ***probability mass function*** $p(a)$ as: $p(a) = P\\left\\{X = a\\right\\}$.\n",
    "\n",
    "Easy to find that $p(a)$ is **positive** for at most a countable number of values of $a$. So if $X$ must assume one of the values $x_1, x_2, \\dots$, then $p(x_i) > 0$ for $i = 1, 2, \\dots$ and $p(x_i)=0$ for all other values of $x$.\n",
    "\n",
    "Direct conclusions would be $\\sum_{i=1}^{\\infty}p(x_i) = 1$ and $F(a) = \\sum_{x_i \\leq a}x_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bernoulli $r.v.$\n",
    "\n",
    "For those $r.v.$ with the probability mass function defined as\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "p(0) = P\\left\\{X=0\\right\\} = 1-p\\\\[0.5em]\n",
    "p(1) = P\\left\\{X=1\\right\\} = p\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where $0 < p < 1$, namely, the probability of successful trial.\n",
    "\n",
    "## The Binomial $r.v.$\n",
    "\n",
    "Suppose that $n$ independent trials, each of which results in a success with probability $p$ and a failure with probability $1-p$. Let $X$ denote the **number of successes** that occur in the $n$ trials, then $X$ is said to be a ***Binomial*** $r.v.$ with **parameters** $(n,p)$.\n",
    "\n",
    "It's probability mass function is given by $p(i) = \\d{\\binom{n} {i}}p^i(1-p)^{n-i}$ for $i=0,1,\\dots,n$, and it's easy to verify that this holds: \n",
    "\n",
    "$$\\sum_{i=0}^{\\infty}p(i) = \\sum_{i=0}^{n} \\binom{n} {i}p^i(1-p)^{n-i} = (p+(1-p))^{n} = 1$$\n",
    "\n",
    "## The Geometric $r.v.$\n",
    "\n",
    "Suppose that independent trials, each having probability $p$ of being a success and denote $X$ as the number of trials required until the first success. Then this $X$ is said to be a ***geometric*** $r.v.$ with parameter $p$. It's probability mass function is given by $p(n) = P\\left\\{X=n\\right\\} = (1-p)^{n-1}p$ for $n = 1,2,\\dots$\n",
    "\n",
    "And it's easy to verify that $\\sum\\limits_{n=1}^{\\infty} p(n) = p\\sum\\limits_{n=1}^{\\infty} (1-p)^{n-1} = 1$\n",
    "\n",
    "## The Poisson Random Variable\n",
    "\n",
    "For $r.v.$ $X$, taking on one of the values $i = 0,1,\\dots$ with probability mass function given by\n",
    "\n",
    "$$p(i) = P\\left\\{X=i\\right\\} = e^{-\\lambda} \\frac{\\lambda^i} {i!}$$\n",
    "\n",
    "And it's easy to verify that $\\sum\\limits_{i=0}^{\\infty} p(i) = e^{-\\lambda} \\sum\\limits_{i=0}^{\\infty} \\ffrac{\\lambda^i} {i!} = e^{-\\lambda}e^{\\lambda} = 1$\n",
    "\n",
    "One important application is to **approximate** a *binomial* $r.v.$, with large $n$ and small $p$.\n",
    "\n",
    "$$\\begin{align}\n",
    "P_{\\text{binom}}\\left\\{X=i\\right\\} &= \\binom{n} {i} p^i (1-p)^{n-i} \\\\\n",
    "&= \\frac{n!} {(n-i)!i!} \\left( \\frac{\\lambda} {n} \\right)^i \\left( 1 - \\frac{\\lambda} {n} \\right)^{n-i} \\\\[0.6em]\n",
    "&= \\frac{n(n-1)(n-2) \\cdots (n-i+1)} {n!} \\frac{\\lambda^i} {i!} \\frac{(1-\\lambda/n)^n} {(1-\\lambda/n)^i} \\\\\n",
    "& \\approx 1 \\cdot \\frac{\\lambda^i} {i!} \\cdot \\frac{e^{-\\lambda}} {1} = P_{\\text{poisson}}\\left\\{X=i\\right\\}\n",
    "\\end{align}$$\n",
    "\n",
    "$Remark$\n",
    "\n",
    "- $0!=1$\n",
    "- For poisson distributed $X$, $\\EE{X} = \\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous $r.v.$\n",
    "\n",
    "Now the $r.v.$ can take on a uncountable set of values, also say $X$. We say $X$ is a ***continuous*** $r.v.$ if there exists a **nonnegative** function $f(x)$, defined for all real $x \\in (-\\infty, \\infty)$, having the property that for any set $B$ of real numbers, \n",
    "\n",
    "$$P\\left\\{X \\in B\\right\\} = \\int_{B} f(x) \\;\\dd{x}$$\n",
    "\n",
    "here $f(x)$ is the ***probability density function*** of $X$. It must sastify $P\\CB{X \\in \\left( -\\infty, \\infty\\right)} = \\d{\\int_{-\\infty}^{\\infty} f(x)\\;\\dd{x} = 1}$. And one funny thing about this is for any *particular value* assumed to $X$ like $a$, $P\\CB{X = a} = \\d{\\int_{a}^{a}f(x)\\;\\dd{x}}=0$.\n",
    "\n",
    "Also, we can use this to define the cumulative distribution $F(\\cdot)$: $F(a) = \\d{\\int_{-\\infty}^{a} f(x) \\;\\dd{x}}$, then we can differentiate both sides and it yields: $\\ffrac{\\dd{}} {\\dd{a}}F(a) = f(a)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Uniform Random Variable\n",
    "\n",
    "A $r.v.$ is said to be ***uniformly distributed*** over the interval $(0,1)$ if its pdf is given by\n",
    "\n",
    "$$f(x) = \\begin{cases}\n",
    "1 & \\text{if } 0 < x < 1 \\\\[0.6em]\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "And in general, we say that $X$ is a uniform random variable on the interval $(\\alpha, \\beta)$ if its pdf is given by\n",
    "\n",
    "$$f(x) = \\begin{cases}\n",
    "\\ffrac{1} {\\beta - \\alpha} & \\text{if } \\alpha < x < \\beta \\\\[0.6em]\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "## Exponential Random Variables\n",
    "\n",
    "A continuous $r.v.$ whose pdf is given, for some $\\lambda > 0$, by, \n",
    "\n",
    "$$f(x) = \\begin{cases}\n",
    "\\lambda e ^{-\\lambda x} & \\text{if }x\\geq 0 \\\\[0.6em]\n",
    "0 & \\text{if } x<0\n",
    "\\end{cases}$$\n",
    "\n",
    "is said to be an ***exponential*** $r.v.$ with parameter $\\lambda$. And for its cdf, we have\n",
    "\n",
    "$$F(a) = \\begin{cases}\n",
    "\\d{\\int_{0}^{a} \\lambda e ^{-\\lambda x} \\;\\dd{x}} = 1 - e^{-\\lambda a} & \\text{if } a\\geq 0 \\\\[0.6em]\n",
    "0 & \\text{if } a<0\n",
    "\\end{cases}$$\n",
    "\n",
    "And also it's easy to verify that $F(\\infty) = \\d{\\int_{0}^{\\infty} \\lambda e ^{-\\lambda x} \\;\\dd{x} = 1}$\n",
    "\n",
    "## Gamma Random Variables\n",
    "\n",
    "A continuous $r.v.$ whose pdf is given, for some $\\lambda > 0$ and $\\alpha > 0$, by \n",
    "\n",
    "$$f(x) = \\begin{cases}\n",
    "\\ffrac{\\lambda e ^{-\\lambda x} (\\lambda x)^{\\alpha-1}} {\\Gamma(\\alpha)} & \\text{if } x\\geq 0 \\\\[0.6em]\n",
    "0 & \\text{if } x<0\n",
    "\\end{cases}$$\n",
    "\n",
    "is said to be a ***gamma*** $r.v.$ with parameter $\\alpha$, $\\lambda$, and ***gamma function***, $\\Gamma(\\alpha) = \\d{\\int_{0}^{\\infty} e^{-x} x^{\\alpha - 1} \\; \\dd{x}}$.\n",
    " \n",
    "$Remark$\n",
    "\n",
    "By induction we can show that $\\Gamma(n) = (n-1)!$ for integral $n$.\n",
    "\n",
    ">$$\\begin{align}\n",
    "\\Gamma(n) &= \\int_{0}^{\\infty} e^{-x} x^{n-1} \\;\\dd{x} = (n-1)! \\\\\n",
    "&= \\int_{0}^{\\infty} e^{-x}\\; \\ffrac{\\dd{x^{n}}} {n}\\\\\n",
    "&= \\left.\\ffrac{e^{-x}x^n} {n}\\right|_{0}^{\\infty} - \\int_{0}^{\\infty} -e^{-x} \\ffrac{x^{n}} {n} \\;\\dd{x}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Random Variables\n",
    "\n",
    "We say that $X$ is a ***normal*** $r.v.$ with parameters $\\mu$ and $\\sigma^2$ if its pdf is given by\n",
    "\n",
    "$$f(x) = \\frac{1} {\\sqrt{2\\pi\\sigma^2}} \\exp\\CB{-\\ffrac{(x-\\mu)^2} {2\\sigma^2}}$$\n",
    "\n",
    "with $x \\in \\mathbb{R}$. It's density function is a bell-shaped curve that is symmetric around $\\mu$.\n",
    "\n",
    "$Remark$\n",
    "\n",
    "If $X$ is normally distributed with parameters $\\mu$ and $\\sigma^2$, then for $Y = \\alpha X + \\beta$, it's also normally distributed with parameters $\\alpha \\mu + \\beta$ and $\\alpha^2 \\sigma^2$. and from the linearity, $Y \\in \\mathbb{R}$.\n",
    "\n",
    ">$$\\begin{align}\n",
    "F_Y(a) &= P(Y \\leq a) = P(\\alpha X + \\beta \\leq a)\\\\[0.6em]\n",
    "&= F_X \\left( \\ffrac{a-\\beta} {\\alpha} \\right) \\\\\n",
    "&= \\int_{-\\infty}^{(a - \\beta)/\\alpha} \\frac{1} {\\sqrt{2\\pi\\sigma^2}} \\exp\\CB{-\\ffrac{(x-\\mu)^2} {2\\sigma^2}} \\;\\dd{x} \\\\\n",
    "&\\stackrel{ y = \\alpha x + \\beta} {=} \\int_{-\\infty}^{a}\\frac{1} {\\sqrt{2\\pi}\\sigma\\alpha} \\exp\\CB{-\\ffrac{(y-(\\alpha x + \\beta))^2} {2\\sigma^2\\alpha^2}} \\;\\dd{y}\n",
    "\\end{align}$$\n",
    "\n",
    "$Remark$\n",
    "\n",
    "The previous result can be applied inversely so that any normally distributed $r.v.$ $X$ can be transformed into a specific one with parameters $0$ and $1$, by conducting $Z = (X - \\mu)/\\sigma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expectation of a Random Variable\n",
    "## The Discrete Case\n",
    "\n",
    "If $X$ is a discrete $r.v.$ having a pmf $p(x)$, the the ***expected value*** of $X$ is defined by:\n",
    "\n",
    "$\\d{\\EE{X} = \\sum_{x:p(x)>0}xp(x)}$\n",
    "\n",
    "**e.g.**  \n",
    "Expectation of a **Bernoulli** $r.v.$\n",
    "\n",
    "> $\\EE{X} = 0 \\cdot (1-p) + 1 \\cdot p = p $\n",
    "\n",
    "**e.g.**  \n",
    "Expectation of a **Binomial** $r.v.$\n",
    "\n",
    "> $\\begin{align}\n",
    "\\EE{X} &= \\sum_{i=0}^{n} i \\cdot p(i) = \\sum_{i=0}^{n} i \\cdot \\binom{n} {i} p^i (1-p)^{n-i} \\\\\n",
    "&= \\sum_{i=\\mathbf{1}}^{n} \\ffrac{n!} {(n-i)!(i-1)!} p^i (1-p)^{n-i} \\\\\n",
    "&= np \\sum_{i=\\mathbf{1}}^{n} \\ffrac{(n-1)!} {(n-i)!(i-1)!} p^{i-1} (1-p)^{n-i} \\\\\n",
    "&\\stackrel{k=i-1}{=} np \\sum_{k=\\mathbf{0}}^{n-1} \\cdot \\binom{n-1} {k} p^k (1-p)^{n-1-k} \\\\\n",
    "&= np\\left[p+(1-p)\\right]^{n-1} = np\n",
    "\\end{align}$\n",
    "\n",
    "**e.g.**  \n",
    "Expectation of a **Geometric** $r.v.$\n",
    "\n",
    "> $\\begin{align}\n",
    "\\EE{X} &= \\sum_{n=1}^{\\infty} n \\cdot p(1-p)^{n-1} \\\\\n",
    "&\\stackrel{q=1-p}{=} p \\sum_{n=1}^{\\infty}nq^{n-1} \\\\\n",
    "&= p \\sum_{n=1}^{\\infty} \\ffrac{\\dd{}} {\\dd{q}}q^{n} \\\\\n",
    "&= p \\ffrac{\\dd{}} {\\dd{q}} \\left( \\ffrac{q} {1-q} \\right)\\\\\n",
    "&= \\ffrac{p} {(1-q)^2} = \\ffrac{1} {p}\n",
    "\\end{align}$\n",
    ">\n",
    "> 错位相减法 also works\n",
    "\n",
    "**e.g.**  \n",
    "Expectation of a **Poisson** $r.v.$\n",
    "\n",
    "> $\\begin{align}\n",
    "\\EE{X} &= \\sum_{i=0}^{\\infty} i\\cdot\\ffrac{e^{-\\lambda}\\lambda^i} {i!} \\\\\n",
    "&= \\lambda e^{-\\lambda} \\sum_{i=\\mathbf{1}}^{\\infty} \\ffrac{\\lambda^{i-1}} {(i-1)!} \\\\\n",
    "&= \\lambda e^{-\\lambda} e^{\\lambda} = \\lambda\n",
    "\\end{align}$\n",
    "\n",
    "***\n",
    "\n",
    "## The Continuous Case\n",
    "\n",
    "For $r.v.$ $X$ with pdf $f(x)$, its expected value is defined by $\\EE{X} = \\d{\\int_{-\\infty}^{\\infty} xf(x) \\;\\dd{x}}$.\n",
    "\n",
    "**e.g.**  \n",
    "Expectation of a **Uniform** $r.v.$\n",
    "\n",
    "> $\\begin{align}\n",
    "\\EE{X} &= \\int_{\\alpha}^{\\beta} x \\cdot \\frac{1} {\\beta - \\alpha} \\; \\dd{x} \\\\\n",
    "&= \\frac{\\beta^2 - \\alpha^2} {2(\\beta - \\alpha)} = \\frac{\\beta + \\alpha} {2}\n",
    "\\end{align}$\n",
    "\n",
    "**e.g.**  \n",
    "Expectation of a **Exponential** $r.v.$\n",
    "\n",
    "> $\\begin{align}\n",
    "\\EE{X} &= \\int_{0}^{\\infty} x \\cdot \\lambda e^{-\\lambda x} \\;\\dd{x}  = \\int_{0}^{\\infty} -x\\;\\dd{e^{-\\lambda x}}\\\\\n",
    "&= \\left. -xe^{-\\lambda x}\\right|_{0}^{\\infty} + \\int_{0}^{\\infty} e^{-\\lambda x} \\; \\dd{x} \\\\\n",
    "&= 0 - \\left. \\frac{e^{-\\lambda x}} {\\lambda} \\right|_{0}^{\\infty} \\\\\n",
    "&= \\frac{1} {\\lambda}\n",
    "\\end{align}$\n",
    "\n",
    "**e.g.**  \n",
    "Expectation of a **Normal** $r.v.$\n",
    "\n",
    "> $\\begin{align}\n",
    "\\EE{X} &= \\frac{1} {\\sqrt{2\\pi}\\sigma} \\int_{-\\infty}^{\\infty} x \\cdot \\exp\\CB{-\\frac{(x-\\mu)^2} {2\\sigma^2}} \\;\\dd{x} \\\\\n",
    "&\\stackrel{y=x-\\mu}{=}\\frac{1}{\\sqrt{2\\pi}\\sigma}\\left( \\int_{-\\infty}^{\\infty} y \\exp\\CB{-\\frac{y^2} {2\\sigma^2}} \\; \\dd{y} + \\mu \\int_{-\\infty}^{\\infty} \\exp\\CB{-\\frac{(x-\\mu)^2} {2\\sigma^2}} \\; \\dd{x} \\right) \\\\\n",
    "&= \\frac{1}{\\sqrt{2\\pi}\\sigma} \\int_{-\\infty}^{\\infty} y \\exp\\CB{-\\frac{y^2} {2\\sigma^2}} \\; \\dd{y} + \\mu \\int_{-\\infty}^{\\infty}  f(x) \\; \\dd{x}\n",
    "\\end{align}$\n",
    "\n",
    ">By symmetricity of the first term we can conclude that $\\EE{X} = \\mu \\cdot 1 = \\mu$.\n",
    "\n",
    "***\n",
    "\n",
    "## Expectation of a Function of a $r.v.$\n",
    "\n",
    "$Proposition$\n",
    "\n",
    "For $X$ with pmf $p(x)$, and any real-valued function $g$, $\\EE{g(X)} = \\d{\\sum_{x:p(x)>0}} g(x)p(x)$. And for those with pdf $f(x)$, we have $\\EE{g(X)} = \\d{\\int_{-\\infty}^{\\infty}} g(x)f(x) \\;\\dd{x}$\n",
    "\n",
    "$Corollary$\n",
    "\n",
    "For constant $a$ and $b$, then $\\EE{aX+b} = a\\EE{X} + b$. \n",
    "\n",
    "***\n",
    "\n",
    "We also call the quantity $\\EE{X^n}$ for $n \\geq 1$ the $n\\text{-th}$ ***moment*** of $X$. And the variance, defined by $\\Var{X} = \\EE{(X - \\EE{X})^2}$.\n",
    "\n",
    "**e.g.**  \n",
    "Variance of the ***Normal*** $r.v.$\n",
    "\n",
    "> $\\begin{align}\n",
    "\\Var{X} &= \\EE{(X - \\mu)^2} \\\\\n",
    "&= \\frac{1} {\\sqrt{2\\pi}\\sigma}\\int_{-\\infty}^{\\infty} (x-\\mu)^2 \\exp\\CB{-\\frac{(x-\\mu)^2} {2\\sigma^2}}\\;\\dd{x} \\\\\n",
    "&\\stackrel{y=(x-\\mu)/\\sigma}{=} \\frac{\\sigma^2} {\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} y^2 \\exp\\CB{-\\frac{y^2} {2}} \\;\\dd{y} \\\\\n",
    "&= \\frac{\\sigma^2} {\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} -y \\;\\dd{e^{-y^2/2}}\\\\\n",
    "&= \\frac{\\sigma^2} {\\sqrt{2\\pi}} \\left( \\left.-ye^{-y^2/2}\\right|_{-\\infty}^{\\infty} + \\int_{-\\infty}^{\\infty} e^{-y^2/2} \\;\\dd{y} \\right) \\\\\n",
    "&= \\frac{\\sigma^2} {\\sqrt{2\\pi}} \\cdot \\int_{-\\infty}^{\\infty} e^{-y^2/2} \\;\\dd{y} \\\\\n",
    "&= \\sigma^2\n",
    "\\end{align}$\n",
    "\n",
    "***\n",
    "\n",
    "$Remark$\n",
    "\n",
    "About the provement of $\\d{\\int_{-\\infty}^{\\infty} e^{-y^2/2} \\;\\dd{y}} = \\sqrt{2\\pi}$, u can use the method of double integral. Well, I am gonna think out of another way. (But failed finnaly... sad)\n",
    "\n",
    "$Remark$\n",
    "\n",
    "Another formular will connect the expectation and the variance: $\\Var{X} = \\EE{X^2} - (\\EE{X})^2$, for both continuous case and discrete case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jointly Distributed Random Variables\n",
    "##  Joint Distribution Functions\n",
    "\n",
    "For any two $r.v.$s $X$ and $Y$, we can define the ***joint cumulative probability distritbution function*** of $X$ and $Y$ by\n",
    "\n",
    "$$F(a,b) = P\\CB{X \\leq a, Y \\leq b}$$\n",
    "\n",
    "for $a,b \\in \\mathbb{R}$. And with this we can find the ***marginal cumulative probability distribution*** like:\n",
    "\n",
    "$$\\begin{align}\n",
    "F_X(a) &= P\\CB{X \\leq a} = P \\CB{X \\leq a, Y < \\infty} = F(a, \\infty)\\\\\n",
    "F_Y(b) &= F(\\infty, b)\n",
    "\\end{align}$$\n",
    "\n",
    "In the case where $X$ and $Y$ are both discrete $r.v.$, it's also convenient to define the ***joint probability mass function*** of $X$ and $Y$ by: $p(x,y) = P\\CB{X = x, Y=y}$, then following the ***marginal probability mass function*** like:\n",
    "\n",
    "$$\n",
    "p_X(x) = \\sum_{y:p(x,y)>0} p(x,y) \\;\\lvert\\; p_Y(y) = \\sum_{x:p(x,y)>0} p(x,y)\n",
    "$$\n",
    "\n",
    "We say that $X$ and $Y$ are ***jointly continuous*** if there exists a function $f(x,y)$, namely, the ***joint probability density funciton*** of $X$ and $Y$, defined for all real $x$ and $y$, having the property that for all sets $A$ and $B$ of real numbers this holds:\n",
    "\n",
    "$$\\d{P\\CB{X \\in A, Y \\in B} = \\int_B \\int_A f(x,y) \\; \\dd{x} \\; \\dd{y}}$$ \n",
    "\n",
    "And the ***marginal*** part:\n",
    "\n",
    "$$\\begin{align}\n",
    "P\\CB{X\\in A} &= P\\CB{X \\in A, Y \\in (-\\infty,\\infty)} \\\\\n",
    "&= \\int_{-\\infty}^{\\infty} \\int_A f(x,y) \\;\\dd{x} \\; \\dd{y} \\\\\n",
    "&= \\int_A f_X(x)\\;\\dd{x}\n",
    "\\end{align}$$\n",
    "\n",
    "where $f_X(x) = \\d{\\int_{-\\infty}^{\\infty} f(x,y) \\; \\dd{y}}$, which is how we obtain the marginal pdf of $X$.\n",
    "\n",
    "And because $F(a,b) = P\\CB{X \\leq a,Y \\leq b}=\\d{\\int_{-\\infty}^{a}\\int_{-\\infty}^{b} f(x,y) \\;\\dd{y} \\;\\dd{x}}$, differentiation yields:\n",
    "\n",
    "$$\\ffrac{\\dd{}^2} {\\dd{a}\\;\\dd{b}}F(a,b) = f(a,b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expectation can be calculated by\n",
    "\n",
    "$$\n",
    "\\EE{g(X,Y)} = \\begin{cases}\n",
    "\\d{\\sum_y \\sum_x g(x,y) p(x,y)} & \\text{discrete case}\\\\\n",
    "\\d{\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty} g(x,y) f(x,y) \\;\\dd{x}\\;\\dd{y}} &\\text{continuous case}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "A direct application of this is\n",
    "\n",
    "$$\\EE{\\sum_{i=1}^{n}a_iX_i} = \\sum_{i=1}^{n}a_i\\EE{X_i}$$\n",
    "\n",
    "for $n$ $r.v.$s and $n$ constants $a_1, \\dots, a_n$.\n",
    "\n",
    "$Remark$\n",
    "\n",
    "Only applicable for linear combination!\n",
    "\n",
    "**(V)e.g.**  \n",
    "Choose $10$ letters from $A$ to $Z$. Compute the expected number of different types that are contained in a set of $10$ letters.\n",
    "\n",
    "> It's hard to calculate that directly, so we break it apart, $10$ parts. We first define $X_i$ as\n",
    "\n",
    ">$$X_i = \\begin{cases}\n",
    "1, & \\text{if at least one type of letter } i \\text{ is in the set of } 10\\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    ">Then $X$, as the number of different types in the set of $10$ letters, we have $X = \\sum X_i$. And we have\n",
    "\n",
    ">$$\\begin{align}\n",
    "\\EE{X_i} &= P\\CB{X_i = 1} \\\\\n",
    "&= 1 - P\\CB{\\text{no type of letter }i\\text{ are in the set of }10} \\\\\n",
    "&= 1 - \\left(\\ffrac{25} {26}\\right)^{10}\n",
    "\\end{align}$$\n",
    ">\n",
    ">So that $\\EE{X} = \\sum\\EE{X_i} = 26\\left[1 - \\left(\\ffrac{25} {26}\\right)^{10}\\right]$\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independent $r.v.$\n",
    "\n",
    "$X$ and $Y$ are said to be ***independent*** if for all $a$, $b$, we have $P\\CB{X \\leq a, Y \\leq b} = P\\CB{X \\leq a}\\cdot P\\CB{Y \\leq b}$. *In other words*, the events $E_a = \\CB{X \\leq a}$ and $F_b = \\CB{Y \\leq b}$ are independent.\n",
    "\n",
    "In terms of the joint distribution function $F$, we have that $X$ and $Y$ are independent if for $\\forall a,b$,\n",
    "\n",
    "$$F(a,b) = F_X (a) \\cdot F_Y(b)$$\n",
    "\n",
    "which can also be reduced to \n",
    "\n",
    "$$\\begin{cases}\n",
    "p(x,y) &\\!\\!\\!\\!= p_X(x) \\cdot p_Y(y) & \\text{discrete case}\\\\[0.6em]\n",
    "f(x,y) &\\!\\!\\!\\!= f_X(x) \\cdot f_Y(y) & \\text{continuous case}\n",
    "\\end{cases}$$\n",
    "\n",
    "$Proposition$\n",
    "\n",
    "If $X$ and $Y$ are independent, the for any functions $h$ and $g$: $\\EE{g(X)\\cdot h(Y)} = \\EE{g(X)} \\cdot \\EE{h(Y)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariance and Variance of Sums of $r.v.$\n",
    "\n",
    "The covariance of *any* two random variables $X$ and $Y$, denoted by $\\Cov{X,Y}$, is defined by\n",
    "\n",
    "$$\\begin{align}\n",
    "\\Cov{X, Y} &= \\EE{(X - \\EE{X}) \\cdot (Y - \\EE{Y})} \\\\\n",
    "&= \\EE{XY - Y\\EE{X} - X\\EE{Y} + \\EE{X}\\EE{Y}} \\\\\n",
    "&= \\EE{XY} - \\EE{X} \\EE{Y}\n",
    "\\end{align}$$\n",
    "\n",
    "Easy to see that if $X$ and $Y$ are independent, then $\\Cov{X,Y} = 0$\n",
    "\n",
    "$Remark$\n",
    "\n",
    "In general it can be shown that a **positive** value of $\\Cov{X,Y}$ is an **indication** that $Y$ tends to increase as $X$ does, whereas a negative value indicates that $Y$ tends to decrease as $X$ increase.\n",
    "\n",
    "**e.g.**  \n",
    "\n",
    "Given the joint density function of $X$, $Y$, $f(x) = \\ffrac{1} {y} \\exp\\CB{-y-\\ffrac{x} {y}}$, for $0 < X,Y < \\infty$. Verify that and find the covariance.\n",
    "\n",
    "> For the verification:\n",
    "\n",
    ">$$\\begin{align}\n",
    "\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty} f(x,y) \\;\\dd{y} \\;\\dd{x} &= \\int_{0}^{\\infty}\\int_{0}^{\\infty} \\ffrac{1} {y} \\exp\\CB{-y-\\frac{x} {y}} \\;\\dd{y} \\;\\dd{x} \\\\\n",
    "&= \\int_{0}^{\\infty} e^{-y} \\int_{0}^{\\infty} \\frac{1} {y} \\exp\\CB{-\\frac{x} {y}} \\;\\dd{x} \\;\\dd{y}\\\\\n",
    "&= \\int_{0}^{\\infty} e^{-y} \\;\\dd{y} \\\\\n",
    "&= 1\n",
    "\\end{align}$$\n",
    "\n",
    ">And for the covariance we first need the expectation of separate $r.v.$s. Two ways available. For $\\EE{X}$,\n",
    "\n",
    ">$$\n",
    "\\begin{align}\n",
    "\\EE{X} &= \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty} x\\cdot f(x,y) \\;\\dd{y} \\;\\dd{x} \\\\\n",
    "&= \\int_{0}^{\\infty} e^{-y} \\int_{0}^{\\infty} \\frac{x} {y} \\exp\\CB{-\\frac{x} {y}} \\;\\dd{x} \\;\\dd{y}\n",
    "\\end{align}$$\n",
    "\n",
    ">Note that $\\d{\\int_{0}^{\\infty} \\frac{x} {y} \\exp\\CB{-\\frac{x} {y}} \\;\\dd{x}}$ is the exponential $r.v.$ with parameter $\\ffrac{1}{y}$ and thus is equal to $y$. Consequently, $\\EE{X} = \\d{\\int_{0}^{\\infty} y e^{-y} \\;\\dd{y} = 1}$.\n",
    "\n",
    ">***\n",
    ">Then for $\\EE{Y}$, we need another method. We first calculate the marginal probablity $f_Y(y)$.\n",
    ">\n",
    ">$f_Y(y) = e^{-y} \\d{\\int_{0}^{\\infty} \\ffrac{1} {y} \\exp\\CB{-\\ffrac{x} {y}}\\;\\dd{x}} = e^{-y}$, then $\\EE{Y} = 1$.\n",
    ">\n",
    ">***\n",
    ">$$\n",
    "\\begin{align}\n",
    "\\EE{XY} &= \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty} xy \\cdot f(x,y) \\;\\dd{y} \\;\\dd{x} \\\\\n",
    "&= \\int_{0}^{\\infty} y e^{-y} \\int_{0}^{\\infty} \\frac{x} {y} \\exp\\CB{-\\frac{x} {y}} \\;\\dd{x} \\;\\dd{y} \\\\\n",
    "&= \\int_{0}^{\\infty} y^2 e^{-y} \\;\\dd{y} \\\\\n",
    "&= \\int_{0}^{\\infty} -y^2 \\;\\dd{e^{-y}} \\\\\n",
    "&= \\left.-y^2 e^{-y}\\right|_{0}^{\\infty} + \\int_{0}^{\\infty} 2ye^{-y} \\;\\dd{y} = 2\\EE{Y} = 2\n",
    "\\end{align}$$\n",
    "\n",
    ">Consequently, $\\Cov{X,Y} = \\EE{XY} - \\EE{X}\\EE{Y} = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Remark$\n",
    "\n",
    ">Covariance equaling to $0$ can't imply that the two are independent, the inverse statement is true though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Other \\space properties$\n",
    "\n",
    "For any $r.v.$s $X$, $Y$, $Z$ and constant $c$, we have\n",
    "\n",
    "- $\\Cov{X,X} = \\Var{X}\\\\[0.5em]$\n",
    "- $\\Cov{X,Y} = \\Cov{Y,X}\\\\[0.5em]$\n",
    "- $\\Cov{cX,Y} = c \\cdot\\Cov{X, Y}\\\\[0.5em]$\n",
    "- $\\Cov{X,Y+Z} = \\Cov{X,Y} + \\Cov{X,Z}$\n",
    "\n",
    "And the generalized forth property: $\\d{\\Cov{\\sum_{i=1}^{n}X_i,\\sum_{j=1}^{m}Y_j}=\\sum_{i=1}^{n}\\sum_{j=1}^{m} \\Cov{X_i,Y_j}}$.\n",
    "\n",
    "And one more application for variance\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Var{\\sum_{i=1}^{n} X_i} &= \\Cov{\\sum_{i=1}^{n}X_i,\\sum_{j=1}^{n}X_j} \\\\\n",
    "&= \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\Cov{X_i, X_j} \\\\\n",
    "&= \\sum_{i=1}^{n}\\Cov{X_i, X_i} + \\sum_{i=1}^{n} \\sum_{j \\neq i} \\Cov{X_i, X_j} \\\\\n",
    "&= \\sum_{i=1}^{n}\\Var{X_i} + 2 \\sum_{i=1}^{n} \\sum_{j < i} \\Cov{X_i, X_j}\n",
    "\\end{align}$$\n",
    "\n",
    "Even, when $X_i$ are independent, this will reduce to $\\d{\\Var{\\sum_{i=1}^{n}X_i} = \\sum_{i=1}^{n} \\Var{X_i}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Def$\n",
    "\n",
    "If $X_1, X_2, \\dots, X_n$ are **independent** and **identically distributed**, we define the ***sample mean*** as\n",
    "\n",
    "$$\\bar{X} = \\frac{1} {n}\\sum_{i=1}^{n} {X_i}$$\n",
    "\n",
    "$Proposition$\n",
    "\n",
    "Suppose that $X_1, \\dots, X_n$ araea independent distributed with expected value $\\mu$ and variance $\\sigma^2$. Then:\n",
    "\n",
    "- $\\EE{\\bar{X}} = \\mu$\n",
    "- $\\Var{\\bar{X}} = \\ffrac{\\sigma^2}{n}\\\\[0.5em]$\n",
    "- $\\Cov{\\bar{X}, X_i - \\bar{X}} = 0$, $i = 1,2,\\dots,n\\\\[0.5em]$\n",
    "\n",
    "**e.g.**  \n",
    "Variance of a **Binomial** $r.v.$\n",
    "\n",
    ">We first break it up. $X = X_1 +\\cdots+X_n$, with the $n$ components from $n$ independent *Bernoulli* $r.v.$. Then we have\n",
    "\n",
    ">$$\\Var{X} = \\sum \\Var{X_i}$$\n",
    "\n",
    ">Since $\\Var{X_i} = \\EE{X_i^2} - \\left(\\EE{X_i}\\right)^2 = p - p^2$, $\\Var{X} = np(1-p)$\n",
    "\n",
    "***\n",
    "\n",
    "**e.g.** ***The Hypergeometric***  \n",
    "\n",
    "Consider $N$ individuals with $p$ percent of whom are in favor of a certain proposition and the rest are opposed, where $p$ is assumed to be *unknown* and required to *estimate*. We will randomly choosing and then determining the positions of $n$ members of the population.\n",
    "\n",
    ">We use the portion of the favored in the sample as an estimator of $p$. First we let\n",
    ">\n",
    ">$$X_i = \\begin{cases}\n",
    "1, &\\text{if the }i\\texttt{th}\\text{ person chosen is in favor} \\\\[0.5em]\n",
    "0, &\\text{otherwise}\n",
    "\\end{cases}$$\n",
    ">\n",
    ">Then the estimator of $p$ is $\\ffrac{1} {n}\\sum_{i=1}^{n} X_i$. We now compute its mean and variance for a little comparison\n",
    ">\n",
    ">$\\d{\\EE{\\ffrac{1} {n}\\sum_{i=1}^{n} X_i} = \\ffrac{1} {n}\\sum_{i=1}^{n} \\EE{X_i} } = p$\n",
    ">\n",
    ">$\\d{\\Var{\\ffrac{1} {n}\\sum_{i=1}^{n} X_i} = \\ffrac{1} {n^2} \\left(\\sum_{i=1}^{n} \\Var{X_i} + 2 \\underset{i<j}{\\sum\\sum} \\Cov{X_i, X_j}\\right)}$\n",
    ">\n",
    ">Easy to see that $X_i$ is a **Bernoulli** $r.v.$ so that $\\Var{X_i} = p(1-p)$, so now we get down to handling the covariance.\n",
    ">\n",
    ">$\\begin{align}\n",
    "\\Cov{X_i,X_j} &= \\EE{X_i \\cdot X_j} - \\EE{X_i} \\cdot \\EE{X_j} \\\\[0.5em]\n",
    "&= P\\CB{X_i = 1, X_j = 1} - p^2 \\\\[0.5em]\n",
    "&= \\ffrac{Np} {N} \\cdot \\ffrac{Np-1} {N-1} - p^2\\\\[0.5em]\n",
    "\\end{align}$\n",
    ">\n",
    ">$\\begin{align}\n",
    "\\Var{\\ffrac{1} {n}\\sum_{i=1}^{n} X_i} &= \\ffrac{1} {n^2} \\left[ np(1-p) + 2\\binom{n}{2} \\left( \\ffrac{Np} {N} \\cdot \\ffrac{Np-1} {N-1} - p^2 \\right) \\right] \\\\\n",
    "&= \\ffrac{p(1-p)} {n} - \\ffrac{(n-1)p(1-p)} {n(N-1)} = \\ffrac{p(1-p)(N-n)} {n(N-1)}\n",
    "\\end{align}$\n",
    "\n",
    "$Remark$\n",
    "\n",
    "When $N$ increases, the variance goes larger and the limiting value as $N \\to \\infty$ is $p(1-p)/n$, which is not surprising since for $N$ large enough each $X_i$ can be considered as *independent* **bernoulli** $r.v.$ and thus $\\sum X_i$ can be considered as **binomial** distribution with parameter $n$ and $p$.\n",
    "\n",
    "$Remark$\n",
    "\n",
    "The real ***Hypergeometric*** $r.v.$ is brought out like this: Totally $N$ identities with $p$ percent with a feature and the rest not. Then we select $n$ identities from all $N$ and denote $X$ as the number of identities with that feature:\n",
    "\n",
    "$$\\d{P\\CB{X=k}} = \\ffrac{\\d{\\binom{Np} {k}\\binom{N-Np} {n-k}}} {\\d{\\binom{N} {n}}}$$\n",
    "\n",
    "And an easy example for this is to consider an urn with $Np$ red balls and $N-Np$ blu balls in. We take $n$ balls out and this is the *distribution* of the number of blue balls.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing is the ***convolution*** of the distributions $F_X$ and $F_Y$, the distribution of $X+Y$, from the distributions of $X$ and $Y$, given they are **independent**.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "F_{X+Y}(a) &= P \\CB{X+Y \\leq a} \\\\\n",
    "&= \\iint_{x+y \\leq a} f(x) g(y) \\;\\dd{x} \\;\\dd{y} \\\\\n",
    "&= \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{a-y} f(x) g(y) \\;\\dd{x} \\;\\dd{y}\\\\\n",
    "&= \\int_{-\\infty}^{\\infty} \\left( \\int_{-\\infty}^{a-y} f(x) \\;\\dd{x} \\right) g(y) \\;\\dd{y} \\\\\n",
    "&= \\int_{-\\infty}^{\\infty} F_X (a-y) g(y) \\;\\dd{y}\n",
    "\\end{align}$$ \n",
    "\n",
    "Then we differentiating both sides of the equation above, the pdf comes:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\ffrac{\\dd{}} {\\dd{a}} F_{X+Y}(a) &= \\ffrac{\\dd{}} {\\dd{a}} \\int_{-\\infty}^{\\infty} F_X (a-y) g(y) \\;\\dd{y} \\\\[0.7em]\n",
    "f_{X+Y}(a) &= \\int_{-\\infty}^{\\infty} \\ffrac{\\dd{}} {\\dd{a}} \\big(F_X (a-y)\\big)g(y) \\;\\dd{y} \\\\\n",
    "&= \\int_{-\\infty}^{\\infty} f(a-y) g(y) \\;\\dd{y}\n",
    "\\end{align}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(V)e.g.** **Sum** of Two Independent **Uniform** $r.v.$  \n",
    "\n",
    "Given $X$ and $Y$ are independent $r.v.$ both uniformly distributed on $(0,1)$, find the pdf of $X+Y$.\n",
    "\n",
    ">First we have $f(z) = g(z) = \\begin{cases}\n",
    "1, & 0 < z < 1 \\\\[0.5em]\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}$, and with the previous formula we have: \n",
    ">\n",
    ">$$f_{X+Y} (z) = \\int_{-\\infty}^{\\infty} f(z-y)g(y) \\;\\dd{y} = \\int_0^1f(z-y)\\;\\dd{y}$$\n",
    ">\n",
    ">Then for $0 \\leq z  \\leq 1$, this yields $\\d{f_{X+Y}(z) = \\int_{0}^{z} \\;\\dd{y} = z}$. For $1 < z < 2$, we get $\\d{f_{X+Y} (z) = \\int_{z-1}^{1}\\;\\dd{y} = 2-z}$. Hence we draw the conclusion as\n",
    ">\n",
    ">$$\n",
    "f_{X+Y}(z) = \\begin{cases}\n",
    "z, & 0 \\leq z \\leq 1 \\\\[0.5em]\n",
    "2-z, & 1 < z < 2 \\\\[0.5em]\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    ">\n",
    ">Just for fun, I also calculate the \"triple\" one:\n",
    "\n",
    ">$$\n",
    "f_{Z+Y}(w) = \\begin{cases}\n",
    "\\frac{1} {2}w^2, & 0 \\leq z \\leq 1 \\\\[0.5em]\n",
    "-w^2 + 3w - \\frac{3} {2}, & 1 < z \\leq 2 \\\\[0.5em]\n",
    "\\frac{1} {2}w^2 - 3w + \\frac{9} {2} = \\frac{1} {2}(w-3)^2, & 2 < z < 3 \\\\[0.5em]\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.** **Sum** of Independent **Poisson** $r.v.$  \n",
    "\n",
    "Let $Χ$ and $Y$ be independent **Poisson** $r.v.$ with respective means $\\lambda_1$ and $\\lambda_2$. \n",
    "\n",
    ">$$\\begin{align}\n",
    "P\\CB{X + Y = n} &= \\sum_{k=0}^{n} P \\CB{X=k, Y = n-k} \\\\\n",
    "&= \\sum_{k=0}^{n} P\\CB{X = k} \\cdot P\\CB{Y = n-k} \\\\\n",
    "&= \\sum_{k=0}^{n} e^{-\\lambda_1} \\ffrac{\\lambda_1^k} {k!} \\cdot e^{-\\lambda_2} \\ffrac{\\lambda_2^{n-k}} {(n-k)!}\\\\\n",
    "&= \\ffrac{e^{-\\lambda_1 - \\lambda_2}} {n!} \\sum_{k=0}^{n} \\ffrac{n!} {k!(n-k)!} \\lambda_1^k \\lambda_2^{n-k} \\\\\n",
    "&= \\ffrac{e^{-\\lambda_1 - \\lambda_2}} {n!} (\\lambda_1 + \\lambda_2)^n\n",
    "\\end{align}$$\n",
    "\n",
    ">In words, $X_1+X_2$ has a **Poisson** distribution with mean $\\lambda_1 + \\lambda_2$.\n",
    "\n",
    "$Remark$\n",
    "\n",
    "The general idea of independency is for all values $a_1, a_2, \\dots, a_n$, we have\n",
    "\n",
    "$$P\\CB{X_1 \\leq a_1, X_2 \\leq a_2, \\dots, X_n \\leq a_n} = P\\CB{X_1 \\leq a_1} \\cdot P\\CB{X_2 \\leq a_2} \\cdot  \\cdots \\cdot P\\CB{X_n \\leq a_n} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ***Order Statistics*** \n",
    "\n",
    "Let $X_1, \\dots, X_n$ be $i.i.d.$ continuous $r.v.$ with cdf $F$ and pdf $f = F'$. Define $X_{(i)}$ as the $i\\texttt{th}$ smallest of these $r.v.$, then $X_{(1)}, \\dots, X_{(n)}$ are called the ***Order Statistics*** . Find their distributions.\n",
    "\n",
    " $$P\\CB{X_{(i)} \\leq x} = \\sum_{k=i}^{n} \\binom{n} {k} \\big(F(x)\\big)^k \\big( 1-F(x) \\big)^{n-k}$$\n",
    "\n",
    "Differentiation yields that the density function of $X_{(i)}$ is as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f_{X_{(i)}} (x) &= f(x) \\left( \\sum_{k=i}^{n}\\binom{n} {k} \\cdot k \\big(F(x)\\big)^{k-1} \\big( 1-F(x) \\big)^{n-k} - \\sum_{k=i}^{n}\\binom{n} {k} \\big(F(x)\\big)^{k} \\cdot (n-k) \\big( 1-F(x) \\big)^{n-k-1} \\right) \\\\\n",
    "&= f(x) \\left( \\sum_{k=i}^{n} \\ffrac{n!} {(n-k)!(k-1)!} \\big(F(x)\\big)^{k-1} \\big( 1-F(x) \\big)^{n-k} - \\sum_{k=i}^{n}\\ffrac{n!} {(n-k-1)!k!} \\big(F(x)\\big)^{k} \\big( 1-F(x) \\big)^{n-k-1} \\right) \\\\\n",
    "&= f(x) \\left( \\sum_{k=i}^{n} \\ffrac{n!} {(n-k)!(k-1)!} \\big(F(x)\\big)^{k-1} \\big( 1-F(x) \\big)^{n-k} - \\sum_{j=i+1}^{n}\\ffrac{n!} {(n-j)!(j-1)!} \\big(F(x)\\big)^{j-1} \\big( 1-F(x) \\big)^{n-j} \\right) \\\\\n",
    "&= \\ffrac{n!} {(n-i)!(i-1)!} f(x) \\big(F(x)\\big)^{i-1}\\big(1-F(x)\\big)^{n-i}\n",
    "\\end{align}$$\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint Probability Distribution of Functions of $r.v.$\n",
    "\n",
    "Let $X_1$ and $X_2$ be jointly continuous $r.v.$ with jointly pdf $f(x_1,x_2)$. We need to obtain the joint distribution of two new $r.v.$s $Y_1$ and $Y_2$ that arise as functions of $X_1$ and $X_2$, with $Y_1 = g_1(X_1, X_2)$ and $Y_2 = g_2(X_1, X_2)$.\n",
    "\n",
    "$Assumptions$\n",
    "\n",
    "1. The equations $y_1 = g_1(x_1, x_2)$ and $y_2 = g_2(x_1, x_2)$ can be *uniquely* solved for $x_1$ and $x_2$ in terms of $y_1$ and $y_2$ with solutions given by $x_1 = h_1(y_1,y_2)$ and $x_2 = h_2(y_1,y_2)$.$\\\\[0.7em]$\n",
    "2. The functions $g_1$ and $g_2$ have *continuous partial derivatives* at *all points* $(x_1, x_2)$ and are such that the following determinant$\\\\[0.6em]$\n",
    "$$J(x_1, x_2) = \\begin{vmatrix}\n",
    "\\ffrac{\\partial g_1}{\\partial x_1} & \\ffrac{\\partial g_1}{\\partial x_2} \\\\ \n",
    "\\ffrac{\\partial g_2}{\\partial x_1} & \\ffrac{\\partial g_2}{\\partial x_2}\n",
    "\\end{vmatrix} \\equiv \\ffrac{\\partial g_1}{\\partial x_1} \\cdot \\ffrac{\\partial g_2}{\\partial x_2} \\; \\: - \\; \\: \\ffrac{\\partial g_1}{\\partial x_2} \\cdot \\ffrac{\\partial g_2}{\\partial x_1} \\neq 0\\\\[0.5em]$$\n",
    "at all points $(x_1, x_2)$.\n",
    "\n",
    "Under these, $Y_1$ and $Y_2$ are jointly continuous with their joint density function given by\n",
    "\n",
    "$$f_{Y_1,Y_2}(y_1,y_2) = g(y_1,y_2) = f_{X_1,X_2}(x_1, x_2) \\big| J(x_1, x_2) \\big|^{-1}$$\n",
    "\n",
    "where $x_1 = h_1(y_1,y_2)$ and $x_2 = h_2(y_1,y_2)$. This formula can be obtained by differiate the following equationon both sides with respect to $y_1$ and $y_2$.\n",
    "\n",
    "$$P\\CB{Y_1 \\leq y_1,Y_2 \\leq y_2} = \\iint\\limits_{\\d{\\begin{array}{c}\n",
    "(x_1,x_2): \\\\\n",
    "g_1(x_1,x_2) \\leq y_1\\\\\n",
    "g_2(x_1,x_2) \\leq y_2\n",
    "\\end{array}}} f_{X_1,X_2}(x_1, x_2) \\;\\dd{x_1}\\;\\dd{x_2}$$\n",
    "\n",
    "**e.g.**  \n",
    "\n",
    "If $X$ and $Y$ are independent **gamma** $r.v.$s with parameters $(\\alpha, \\lambda)$ and $(\\beta, \\lambda)$, respectively. Find the joint density of $U = X + Y$ and $V = X/(X+Y)$.\n",
    "\n",
    "> From their independency, we have their joint density function first\n",
    ">\n",
    "> $$\\begin{align}\n",
    "f_{X,Y}(x,y) &= f_X(x) \\cdot f_Y(y) \\\\\n",
    "&= \\ffrac{\\lambda e^{-\\lambda x} (\\lambda x)^{\\alpha - 1}} {\\Gamma(\\alpha)} \\cdot \\ffrac{\\lambda e^{-\\lambda x} (\\lambda x)^{\\beta - 1}} {\\Gamma(\\beta)} \\\\\n",
    "&= \\ffrac{\\lambda^{\\alpha + \\beta} } {\\Gamma(\\alpha)\\Gamma(\\beta)} e^{-\\lambda(x+y)} x^{\\alpha-1} y^{\\beta -1}\n",
    "\\end{align}$$\n",
    ">\n",
    ">Given $g_1(x,y) = x+y, g_2(x,y) = x/(x+y)$, we have $\\ffrac{\\partial g_1} {\\partial x} = \\ffrac{\\partial g_1} {y} = 1$, $\\ffrac{\\partial g_2}{\\partial x} = \\ffrac{y} {(x+y)^2}$, and $\\ffrac{\\partial g_2} {\\partial y}=- \\ffrac{x} {(x+y)^2}$, also the solutions: $x = u\\upsilon$ and $y=u(1-\\upsilon)$ so that:\n",
    ">\n",
    ">$$J(x,y) = \\begin{vmatrix}\n",
    "1 & 1\\\\[0.6em]\n",
    "\\ffrac{y}{\\left( x+y \\right )^2} & \\ffrac{-x}{\\left( x+y \\right )^2}\n",
    "\\end{vmatrix} = - \\ffrac{1} {x+y}$$\n",
    ">\n",
    ">$$\n",
    "\\begin{align}\n",
    "f_{U,V}(u,\\upsilon) &= f_{X,Y}(x,y) \\cdot (x+y) \\\\[0.6em]\n",
    "&= f_{X,Y}(u\\upsilon, u(1-\\upsilon)) \\cdot u \\\\[0.6em]\n",
    "&= \\ffrac{\\lambda^{\\alpha + \\beta} } {\\Gamma(\\alpha)\\Gamma(\\beta)} e^{-\\lambda(u\\upsilon+u(1-\\upsilon))} (u\\upsilon)^{\\alpha-1} (u(1-\\upsilon))^{\\beta -1} \\cdot u \\\\[0.6em]\n",
    "&= \\ffrac{\\lambda^{\\alpha + \\beta-1}\\cdot \\lambda} {\\Gamma(\\alpha)\\Gamma(\\beta)} \\cdot e^{-\\lambda u} \\cdot u^{1 + \\alpha-1 + \\beta -1} \\cdot \\ffrac{\\Gamma(\\alpha + \\beta)} {\\Gamma(\\alpha + \\beta)} \\cdot \\upsilon^{\\alpha-1} \\cdot (1-\\upsilon)^{\\beta -1} \\\\[0.6em]\n",
    "&= \\ffrac{\\lambda e^{-\\lambda u} (\\lambda u)^{\\alpha + \\beta -1}} {\\Gamma(\\alpha + \\beta)} \\cdot \\ffrac{ \\upsilon^{\\alpha-1} (1-\\upsilon)^{\\beta -1} \\Gamma(\\alpha + \\beta)} {\\Gamma(\\alpha)\\Gamma(\\beta)}\n",
    "\\end{align}$$\n",
    "\n",
    "$Remark$\n",
    "\n",
    "Later we will know that $X+Y$ is also a **gamma** $r.v.$ with parameter $(\\alpha + \\beta , \\lambda)$, thus with a pdf: $\\d{f_{U}(u)  = \\ffrac{\\lambda e^{-\\lambda u} (\\lambda u)^{\\alpha + \\beta -1}} {\\Gamma(\\alpha + \\beta)}}$. \n",
    "\n",
    "Also, since $X+Y$ and $X/(X+Y)$ are independent, we can also see that: $\\d{f_V{(\\upsilon)} = \\ffrac{\\upsilon^ {\\alpha-1} (1-\\upsilon)^{\\beta -1} \\Gamma(\\alpha + \\beta)} {\\Gamma(\\alpha)\\Gamma(\\beta)}}$, which is called the ***beta density*** with parameters $(\\alpha, \\beta)$, with $0<\\upsilon<1$.\n",
    "\n",
    "$\\QQQ$ The last paragraph.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the same method can be applied to more than $2$ $r.v.$s. When the joint density function of the $n$ variables $X_1, X_2, \\dots, X_n$ is given and we wnat to compute the joint density function of $Y_1, Y_2, \\dots, Y_n$, where\n",
    "\n",
    "$$Y_1 = g_1(X_1, X_2, \\dots, X_n), Y_2 = g_2(X_1, X_2, \\dots, X_n), \\dots, Y_n = g_n(X_1, X_2, \\dots, X_n)$$\n",
    "\n",
    "Same assumptions required, like the continuous partial derivable and that the Jacobian determinant $J(x_1,x_2,\\dots, x_n) \\neq 0$ for all points $(x_1,x_2,\\dots, x_n)$.\n",
    "\n",
    "$$J(x_1,x_2,\\dots, x_n) = \\begin{vmatrix}\n",
    "\\ffrac{\\partial g_1} {\\partial x_1} & \\ffrac{\\partial g_1} {\\partial x_2} & \\cdots & \\ffrac{\\partial g_1} {\\partial x_n} \\\\ \n",
    "\\ffrac{\\partial g_2} {\\partial x_1} & \\ffrac{\\partial g_2} {\\partial x_2} & \\cdots & \\ffrac{\\partial g_2} {\\partial x_n} \\\\ \n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\ \n",
    "\\ffrac{\\partial g_n} {\\partial x_1} & \\ffrac{\\partial g_n} {\\partial x_2} & \\cdots & \\ffrac{\\partial g_n} {\\partial x_n} \n",
    "\\end{vmatrix}$$\n",
    "\n",
    "and the equation set has a unique solution, $x_i = h_i(y_1,y_2,\\dots,y_n)$, for $y_1 = g_1 (x_1,x_2,\\dots,x_n), y_2 = g_2 (x_1,x_2,\\dots,x_n), \\dots, y_n = g_n (x_1,x_2,\\dots,x_n)$. Under these, the joint dnsity function of the $r.v.$s $Y_i$  is given by\n",
    "\n",
    "$$f_{Y_1,Y_2,\\dots, Y_n}(y_1,y_2,\\dots,y_n) = f_{X_1,X_2,\\dots,X_n}(x_1,x_2,\\dots, x_n)\\big|J(x_1,x_2,\\dots, x_n)\\big|^{-1}$$\n",
    "\n",
    "where $x_i = h_i(y_1,y_2,\\dots,y_n)$.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moment Generating Functions\n",
    "\n",
    "The ***moment generating function*** $\\phi(t)$ of the $r.v.$ $X$ is defined for all values $t$ by\n",
    "\n",
    "$$\\phi(t) = \\EE{e^{tX}} = \\begin{cases}\n",
    "\\d{\\sum_x e^{tx} \\cdot p(x)}, & \\text{if } X \\text{is discrete} \\\\[0.5em]\n",
    "\\d{\\int_{-\\infty}^{\\infty} e^{tx} \\cdot f(x) \\;\\dd{x}}, & \\text{if } X \\text{is continuous}\n",
    "\\end{cases}$$\n",
    "\n",
    "We can use this function to obtain all the moments of $X$ by successively differentiating $\\phi(t)$.\n",
    "\n",
    "$$\\begin{align}\n",
    "\\phi'(t) &= \\ffrac{\\dd{}} {\\dd{t}} \\EE{e^{tX}} \\\\\n",
    "&= \\EE{\\ffrac{\\dd{}} {\\dd{t}} e^{tX}} \\\\\n",
    "&= \\EE{Xe^{tX}} \\\\[0.8em]\n",
    "\\Longrightarrow \\phi'(0) &= \\EE{X}\n",
    "\\end{align}$$\n",
    "\n",
    "Similarly, $\\phi''(t) = \\EE{X^2 e^{tX}} \\; \\Longrightarrow \\; \\phi''(0) = \\EE{X^2}$. So in general, the $n\\texttt{th}$ derivative of $\\phi(t)$ evaluated at $t=0$ equals $\\EE{X^n}$, for $n \\geq 1$.\n",
    "\n",
    "**e.g.** The **Binomial** Distribution\n",
    "\n",
    ">$$\\begin{align}\n",
    "\\phi(t) &= \\EE{e^{tX}} \\\\\n",
    "&= \\sum_{k=0}^{n} e^{tk} \\cdot \\left( \\binom{n} {k} p^k (1-p)^{n-k} \\right)\\\\\n",
    "&= \\sum_{k=0}^{n} \\binom{n} {k} (pe^t)^k (1-p)^{n-k} \\\\[0.5em]\n",
    "&= (pe^t + 1 - p)^n\n",
    "\\end{align}$$\n",
    "\n",
    ">Hence, $\\EE{X} = \\phi'(0) = n(pe^t + 1 - p)^{n-1} \\cdot pe^t \\big.\\big|_{t=0} = np$ and $\\EE{X^2} = \\cdots = n(n-1) p^2 + np$. Thus we can also obtain the variance: $\\Var{X} = \\EE{X^2} - (\\EE{X})^2 = \\cdots = np(1-p)$.\n",
    "\n",
    "**e.g.** The **Poisson** Distribution\n",
    "\n",
    ">$$\\begin{align}\n",
    "\\phi(t) &= \\EE{e^{tX}} \\\\\n",
    "&= \\sum_{n=0}^{\\infty} e^{tn} \\cdot \\ffrac{e^{-\\lambda} \\lambda^n} {n!}\\\\\n",
    "&= e^{-\\lambda} \\sum_{n=0}^{\\infty} \\ffrac{\\left( \\lambda e^t \\right)^n} {n!} \\\\\n",
    "&= \\QQQ e^{-\\lambda} \\cdot e^{\\lambda e^t} = \\exp\\CB{\\lambda \\left(e^t - 1 \\right)}\n",
    "\\end{align}$$\n",
    "\n",
    ">Differentiation yields: $\\phi'(t) = \\lambda e^t \\exp\\CB{\\lambda \\left(e^t - 1 \\right)}$ and $\\phi''(t) = \\left( \\lambda e^t \\right)^2 \\exp\\CB{\\lambda \\left(e^t - 1 \\right)} + \\lambda e^t \\exp\\CB{\\lambda \\left(e^t - 1 \\right)}$\n",
    "\n",
    "> and so $\\EE{X} = \\lambda$, $\\EE{X^2} = \\lambda^2 + \\lambda$. And $\\Var{X} = \\lambda$\n",
    "\n",
    "**e.g.** The **Exponential** Distribution\n",
    "\n",
    ">$$\\begin{align}\n",
    "\\phi(t) &= \\EE{e^{tX}} \\\\\n",
    "&= \\int_{0}^{\\infty} e^{tx} \\cdot \\lambda e^{-\\lambda x} \\;\\dd{x} \\\\\n",
    "&= \\lambda \\int_{0}^{\\infty} e^{-\\left(\\lambda - t\\right)x} \\;\\dd{x} \\\\\n",
    "&\\stackrel{t < \\lambda} {=} \\ffrac{\\lambda} {\\lambda - t}\n",
    "\\end{align}$$\n",
    "\n",
    ">Differentiation of $\\phi(t)$ yields $\\phi'(t) = \\ffrac{\\lambda} {\\left(\\lambda - t\\right)^2}, \\phi''(t) = \\ffrac{2\\lambda} {\\left(\\lambda - t\\right)^3}$. Thus, $\\EE{X} = \\phi'(0) = \\ffrac{1} {\\lambda}$, $\\EE{X^2} = \\phi''(0) = \\ffrac{2} {\\lambda^2}$ and the variance of $X$ is given by $\\Var{X} = \\EE{X^2} - \\left(\\EE{X}\\right) ^2 = \\ffrac{1} {\\lambda^2}$\n",
    "\n",
    "$Remark$\n",
    "\n",
    "Only when $t < \\lambda$ can we calculate the integral.\n",
    "\n",
    "**e.g.** The **Normal** Distribution\n",
    "\n",
    ">$$\\begin{align}\n",
    "\\EE{e^{tZ}} &= \\int_{-\\infty}^{\\infty} e^{tz} \\cdot \\ffrac{1} {\\sqrt{2\\pi}} \\exp\\CB{-\\ffrac{z^2} {2}} \\;\\dd{z} \\\\\n",
    "&= \\ffrac{1} {\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} \\exp\\CB{-\\ffrac{z^2 - 2tz} {2}} \\;\\dd{z} \\\\\n",
    "&= \\ffrac{e^{t^2/2}} {\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} \\exp\\CB{-\\ffrac{(x-t)^2} {2}} \\;\\dd{z} = \\exp\\CB{ \\ffrac{t^2}{2}}\n",
    "\\end{align}$$\n",
    "\n",
    "$\\space$\n",
    "\n",
    ">Here $Z$ is a ***standard normal*** $r.v.$, so for any normal $r.v.$ $X = \\sigma Z + \\mu$ with parameters $\\mu$ and $\\sigma^2$, we have\n",
    ">\n",
    ">$$\\phi(t) = \\EE{e^{tX}} = \\EE{e^{t\\left(\\sigma Z + \\mu\\right)}} = e^{t\\mu} \\EE{e^{t\\sigma Z}} = \\exp\\CB{ \\ffrac{\\sigma^2 t^2} {2} + \\mu t}$$\n",
    ">\n",
    ">And by differentiating we obtian $\\phi'(t) = \\left(\\mu + t \\sigma^2\\right) \\exp\\CB{\\ffrac{\\sigma^2 t^2} {2} + \\mu t}$, so $\\EE{X} = \\phi'(0) = \\mu$, and $\\phi''(t) = \\left(\\mu + t \\sigma^2\\right)^2 \\exp\\CB{\\ffrac{\\sigma^2 t^2} {2}} + \\sigma^2 \\exp\\CB{\\ffrac{\\sigma^2 t^2} {2}}$, so $\\EE{X^2} = \\phi''(0) = \\mu^2 + \\sigma^2$, implying that $\\Var{X} = \\sigma^2$.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important property of the **moment generating function** is that: For the sum of *independent* $r.v.$s, it's mgf is just the product of the individual mgfs. Suppose $X$ and $Y$ are independent and have mgf $\\phi_X(t)$ and $\\phi_Y(t)$, respectively.\n",
    "\n",
    "$$\\begin{align}\n",
    "\\phi_{X+Y}(t) &= \\EE{e^{t\\left(X+Y\\right)}} \\\\\n",
    "&= \\EE{e^{tX} \\cdot e^{tY}} \\\\\n",
    "&\\stackrel{\\texttt{independency}} {=} \\EE{e^{tX}}\\cdot \\EE{e^{tY}} = \\phi_X(t)\\phi_Y(t)\n",
    "\\end{align}$$\n",
    "\n",
    "Another important property is that the mgf *uniquely* determines the distribution. It's a one-to-one correspondence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Remark$\n",
    "\n",
    "More about the **Poisson** Distribution, the ***Poisson paradigm***, that the number of success in $n$ trials that are either independent or at most weakly dependent is, when the trial success probabilities are all small, approximately a **Poisson** $r.v.$.\n",
    "\n",
    "$Remark$\n",
    "\n",
    "***Laplace transform***, for nonnegative $r.v.$ $X$, is defined as for $t \\geq 0$, $g(t) = \\phi(-t) = \\EE{e^{-tX}}$. This would limit the value between $0$ and $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also define the ***joint moment generating function*** of more than just two $r.v.$s. For any $n$ $r.v.$s $X_1, X_2, \\dots, X_n$, and for all real values of $t_1, t_2, \\dots, t_n$ we define:\n",
    "\n",
    "$$\\phi(t_1, t_2, \\dots, t_n) = \\EE{\\exp\\CB{t_1X_1 + t_2X_2 + \\cdots + t_nX_n}}$$\n",
    "\n",
    "and it can be shown that $\\phi(t_1, t_2, \\dots, t_n)$ uniquely determines the joint distribution of $X_1, X_2, \\dots, X_n$.\n",
    "\n",
    "**e.g.** The ***Multivariate Normal Distribution***\n",
    "\n",
    "Let $Z_1,\\dots,Z_n$ be a set of $n$ independent standart normal random variables. If, for some constants $a_{ij}$ and $\\mu_i$, $1 \\leq i \\leq m$, $1 \\leq j \\leq n$,\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "X_1\\!\\!\\!\\! &=&\\!\\!\\!\\!a_{11}Z_1 + \\cdots + a_{1n}Z_n + \\mu_1 \\\\\n",
    "X_2 \\!\\!\\!\\!&=&\\!\\!\\!\\!a_{21}Z_1 + \\cdots + a_{2n}Z_n + \\mu_2 \\\\\n",
    "& \\vdots & \\\\\n",
    "X_i \\!\\!\\!\\!&=&\\!\\!\\!\\!a_{i1}Z_1 + \\cdots + a_{in}Z_n + \\mu_i \\\\\n",
    "& \\vdots & \\\\\n",
    "X_m \\!\\!\\!\\!&=&\\!\\!\\!\\!a_{m1}Z_1 + \\cdots + a_{mn}Z_n + \\mu_m\n",
    "\\end{array}$$\n",
    "\n",
    "Then the $r.v.$s $X_1, X_2, \\dots, X_n$. are said to have a **Multivariate Normal Distribution**.\n",
    "\n",
    "> Easy to see that $\\EE{X_i} = \\mu_i$ and $\\Var{X_i} = \\sum\\limits_{j=1}^{n}a_{ij}^2$. Then $\\EE{\\sum\\limits_{i=1} ^{m} t_iX_i} = \\sum\\limits_{i=1}^{m} t_i\\mu_i$ and\n",
    "\n",
    ">$$\\Var{\\sum\\limits_{i=1} ^{m} t_iX_i} = \\Cov{\\sum\\limits_{i=1} ^{m} t_iX_i,\\sum\\limits_{j=1} ^{m} t_jX_j} = \\sum\\limits_{i=1}^{m}\\sum\\limits_{j=1}^{m} t_it_j\\Cov{X_i,X_j}$$\n",
    "\n",
    ">$$\\phi\\left(t_1,\\dots,t_m\\right) = \\exp\\CB{\\sum\\limits_{i=1}^{m} t_i\\mu_i + \\ffrac{1} {2} \\sum\\limits_{i=1}^{m} \\sum\\limits_{j=1}^{m} t_it_j\\Cov{X_i,X_j}}$$\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  The Joint Distribution of the Sample Mean and Sample Variance from a Normal Population\n",
    "\n",
    "$X_1,\\dots,X_n$ are independent and identical distributed $r.v.$s, each with mean $\\mu$ and variance $\\sigma^2$. We now define the ***sample mean*** $\\bar{X} =\\ffrac{1} {n}\\sum\\limits_{i=1}^{n} X_i$ and ***sample variance***:\n",
    "\n",
    "$$S^2 = \\sum_{i=1}^{n}\\ffrac{\\left(X_i - \\bar{X}\\right)^2} {n-1}$$\n",
    "\n",
    "With the fact that\n",
    "\n",
    "$$\\begin{align}\n",
    "\\sum_{i=1}^{n} \\left(X_i - \\bar{X}\\right) &= \\sum_{i=1}^{n} \\left( X_i - \\mu + \\mu - \\bar{X} \\right)^2 \\\\\n",
    "&= \\left[\\sum_{i=1}^{n} \\left(X_i - \\mu \\right)^2\\right] + n\\left(\\mu - \\bar{X}\\right)^2 + 2\\left(\\mu - \\bar{X} \\right)\\sum_{i=1}^{n} \\left(X_i - \\mu \\right) \\\\\n",
    "&= \\left[\\sum_{i=1}^{n} \\left(X_i - \\mu \\right)^2\\right] + n\\left(\\mu - \\bar{X}\\right)^2 - 2n\\left(\\mu - \\bar{X}\\right)^2 = \\left[\\sum_{i=1}^{n} \\left(X_i - \\mu \\right)^2\\right] - n\\left(\\bar{X} - \\mu\\right)^2\n",
    "\\end{align}$$\n",
    "\n",
    "we can calculate the expectation as\n",
    "\n",
    "$$\\begin{align}\n",
    "\\EE{S^2} &= \\ffrac{1} {n-1} \\left[\\left(\\sum_{i=1}^{n} \\EE{(X_i - \\mu)^2}\\right)-n\\EE{\\left(\\bar{X} - \\mu\\right)^2 }\\right] \\\\\n",
    "&= \\ffrac{1} {n-1}\\left(n\\sigma^2 - n\\Var{\\bar{X}}\\right) = \\sigma^2\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Def$ ***Chi-Squared*** $r.v.$\n",
    "\n",
    "If $Z_1,\\dots,Z_n$ are *independent* **standard normal** $r.v.$s then the $r.v.$ $\\sum Z_i^2$ is said to be a **chi-squared** $r.v.$ with $n$ ***degrees of freedom***.\n",
    "\n",
    "We first compute its mgf, note that\n",
    "\n",
    "$$\\begin{align}\n",
    "\\EE{\\exp\\CB{tZ_i^2}} &= \\int_{-\\infty}^{\\infty}\\exp\\CB{tx^2}\\cdot\\ffrac{1} {\\sqrt{2\\pi}} \\exp\\CB{-\\ffrac{x^2} {2}} \\;\\dd{x} \\\\\n",
    "&= \\ffrac{1} {\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} \\exp\\underset{\\;\\;\\begin{array}{c}\n",
    "\\uparrow \\\\\n",
    "\\sigma^2 = \\left(1-2t\\right)^{-1}\n",
    "\\end{array}}{\\CB{-\\ffrac{x^2} {2\\sigma^2}}} \\;\\dd{x} \\\\[0.8em]\n",
    "&= \\sigma = \\left(1-2t\\right)^{-1/2}\n",
    "\\end{align}$$\n",
    "\n",
    "Hence,\n",
    "\n",
    "$$\\EE{\\exp\\CB{t\\sum_{i=1}^{n} Z_i^2}} = \\prod_{i=1}^{n} \\EE{\\exp\\CB{tZ_i^2}} = \\left(1-2t\\right)^{-n/2}$$\n",
    "\n",
    "$Remark$\n",
    "\n",
    "Consider $Y$ be a **normal** $r.v.$ with mean $\\mu$ and variance $\\sigma^2/n$ that is independent of $X_1, \\dots, X_n$, then the $r.v.$s $Y, X_1-\\bar{X}, X_2-\\bar{X},\\dots,X_n-\\bar{X}$ have a **multivariate normal** distribution. Since they are independent, $\\Cov{Y,X_i - \\bar{X}} = 0$ for $i = 1,\\dots, n$. Also, $\\EE{Y + X_1-\\bar{X} + \\cdots + X_n-\\bar{X}} = \\EE{Y} = \\EE{\\bar{X}}$.\n",
    "\n",
    "Our conclusion is that for a **multivariate normal** distribution is *completely*, *uniquely* determined by its expected values and covariances, $\\bar{X}$ is independent of the sequence of deviations $X_i - \\bar{X}$, $i = 1,\\dots, n$.\n",
    "\n",
    "So that it's also independent of the **sample variance** $S^2 \\equiv \\ffrac{1} {n-1} \\sum_{i=1}^{n}\\left(X_i - \\bar{X} \\right)^2$ and now we're gonna determine the distribution of $S^2$.\n",
    "\n",
    "$$\\ffrac{n-1} {\\sigma^2} S^2 = \\left[ \\sum_{i=1}^{n} \\ffrac{\\left(X_i - \\mu \\right)^2} {\\sigma^2}\\right] - \\ffrac{n\\left(\\bar{X} - \\mu\\right)^2} {\\sigma^2} \\Rightarrow \\ffrac{(n-1)S^2} {\\sigma^2} + \\left( \\ffrac{\\bar{X} - \\mu} {\\sigma / \\sqrt{n}} \\right)^2 = \\sum_{i=1}^{n} \\ffrac{\\left(X_i - \\mu \\right)^2} {\\sigma^2}$$\n",
    "\n",
    "The key is to use the mgf. We've already seen that the mgf for the right side term, the **chi-squared** $r.v.$ with $n$ degree of freedom and the second term on the left, the square of a **standard normal** $r.v.$, the **chi-squared** $r.v.$ with $1$ degree of freedom. So that\n",
    "\n",
    "$$\\EE{\\exp\\CB{t\\cdot \\ffrac{(n-1)S^2} {\\sigma^2}}}(1-2t)^{-1/2} = (1-2t)^{-n/2}$$\n",
    "\n",
    "Thus, the mgf of $\\ffrac{n-1} {\\sigma^2} S^2$ is the same with that of a **chi-squared** $r.v.$ with $n-1$ degrees of freedom, where we can claim the proposition\n",
    "\n",
    "$Proposition$\n",
    "\n",
    "If $X_1,\\dots,X_n$ are $i.i.d.$ **normal** $r.v.$s with mean $\\mu$ and varianc $\\sigma^2$, then the **sample mean** $\\bar{X}$ and **sample variance** $S^2$ are independent. $\\bar{X}$ is a **normal** $r.v.$ with mean $\\mu$ and variance $\\sigma^2/n$; $(n-1)S^2/\\sigma^2$ is a **chi-squared** $r.v.$ with $n-1$ degrees of freedom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Distribution of the Number of Events that Occur\n",
    "\n",
    "Consider arbitrary events $A_1,\\dots,A_n$, and let $X$ denote the number of these events that occur. What's the pmf of $X$? We first define:\n",
    "\n",
    "$$S_k = \\sum_{\\d{i_1 < \\cdots < i_k}} P\\left( A_{\\d{i_1}},\\dots,A_{\\d{i_k}} \\right)$$\n",
    "\n",
    "as the sum of the probabilities of all the $\\d{\\binom{n} {k}}$ intersections ($\\cap$) of $k$ distinct events, and note that the inclusion-exclusion identity states that\n",
    "\n",
    "$$P\\CB{X>0} = P\\left(\\bigcup_{i=1}^{n}A_i\\right) = S_1 - S_2 + S_3 - \\cdots + (-1)^{n+1} S_n$$\n",
    "\n",
    "Now, to help understand we fix $h$ of the $n$ events, say $A_{1},\\dots,A_{h}$ and let $A=\\bigcap\\limits_{j=1}^{h} A_{j}$ be the event that all $h$ of these events occur. Also, let $B=\\bigcap\\limits_{j\\notin\\CB{1, 2, \\dots, h}} A_{j}^{c}$ be the none of the other $n-h$ events occur. Consequently, $A\\cap B = AB$ is the event that $A_{1}, \\dots,A_{h}$ are the *only* events to occur. Then, since $A = AB \\cup AB^c$, we have $P(AB) = P(A) - P(AB^c)$.\n",
    "\n",
    "While $B^c = \\bigcup\\limits_{j \\notin \\CB{1, 2, \\dots, h}} A_j$, so that $P(AB^c) = P\\left( A\\bigcup\\limits _{j\\notin\\CB{1,\\dots,h}} A_j\\right) = P\\left( \\; \\bigcup\\limits _{j\\notin\\CB{1,\\dots,h}} AA_j \\right)$.\n",
    "\n",
    "Then we apply the inclusion-exclusion identity again:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(AB^c) &= \\sum_{\\d{j\\notin \\CB{1, 2, \\dots, h}}} P(AA_j) - \\sum_{\\d{j_1 <j_2 \\notin \\CB{1,\\dots,h}}} P( AA_{\\d{j_1}}A_{\\d{j_2}} ) \\\\[1em]\n",
    "&\\;\\;\\;\\;+ \\sum_{\\d{j_1 < j_2 < j_3 \\notin \\CB{1, 2, \\dots, h}}} P( AA_{\\d{j_1}}A_{\\d{j_2}} A_{\\d{j_3}} ) - \\cdots\n",
    "\\end{align}$$\n",
    "\n",
    "Then followed by $P(A\\cap B) = P(A) - P(AB^c) = P(A_1 \\cap A_2 \\cap \\cdots \\cap A_h) - P(AB^c)$, we can approach our final generalized answer,\n",
    "\n",
    "$\\;\\;\\;\\;\n",
    "\\begin{align}\n",
    "P\\CB{X=k} &= \\sum_{\\d{i_1 <\\dots<i_k}} \\left[ P\\left(A_{\\d{i_1}} \\cap A_{\\d{i_2}} \\cap \\cdots \\cap A_{\\d{i_k}}\\right) - \\sum _{\\d{j \\notin \\CB{i_1,\\dots, i_k}}} P\\left(A_{\\d{i_1}} \\cap A_{\\d{i_2}} \\cap \\cdots \\cap A_{\\d{i_k}} \\cap A_{\\d{j}}\\right)\\right. \\\\[0.8em]\n",
    "&\\;\\;\\;\\;+ \\sum_{\\d{j_1 < j_2 \\notin \\CB{i_1,\\dots,i_k}}} P\\left( A_{\\d{i_1}} \\cap A_{\\d{i_2}} \\cap \\cdots \\cap A_{\\d{i_k}} \\cap A_{\\d{j_1}} \\cap A_{\\d{j_2}} \\right) \\\\\n",
    "&\\;\\;\\;\\;- \\left.\\sum_{\\d{j_1 < j_2 < j_3 \\notin \\CB{i_1,\\dots,i_k}}} P\\left( A_{\\d{i_1}} \\cap A_{\\d{i_2}} \\cap \\cdots \\cap A_{\\d{i_k}} \\cap A_{\\d{j_1}} \\cap A_{\\d{j_2}} \\cap A_{\\d{j_3}} \\right)+\\cdots \\right]\n",
    "\\end{align}$\n",
    "\n",
    "Kinda complex, how to simplify this expression?\n",
    "\n",
    "First note that $S_k = \\sum\\limits_{\\d{i_1 <\\dots<i_k}} P\\left( A_{\\d{i_1}} \\cap A_{\\d{i_2}} \\cap \\cdots \\cap A_{\\d{i_k}} \\right)$. Now consider \n",
    "\n",
    "$$\\sum_{\\d{i_1 < \\cdots <i_k}} \\; \\sum_{\\d{j \\notin \\CB{i_1,\\dots,i_k}}}P\\left( A_{\\d{i_1}} \\cap A_{\\d{i_2}} \\cap \\cdots \\cap A_{\\d{i_k}} \\cap A_j \\right)$$\n",
    "\n",
    "Surely, there're repetition. These $k+1$ distinct events are choosed by two steps. We let them be $A_{\\d{m_1}} \\cap A_{\\d{m_2}} \\cap \\cdots \\cap A_{\\d{m_{k+1}}}$, thus it's easier to find out the probability of every intersection actually appear $\\d{\\binom{k+1}{k}}$ times in this multiple summation. Hence:\n",
    "\n",
    "$\\;\\;\\;\\;\n",
    "\\begin{align}\n",
    "&\\sum_{\\d{i_1 < \\cdots <i_k}} \\; \\sum_{\\d{j \\notin \\CB{i_1,\\dots,i_k}}}P\\left( A_{\\d{i_1}} \\cap A_{\\d{i_2}} \\cap \\cdots \\cap A_{\\d{i_k}} \\cap A_j \\right) \\\\\n",
    "=& \\binom{k+1}{k} \\sum_{\\d{m_1 < \\cdots < m_{k+1}}} P\\left(A_{\\d{m_1}} \\cap A_{\\d{m_2}} \\cap \\cdots \\cap A_{\\d{m_{k+1}}}\\right)\\\\\n",
    "=& \\binom{k+1}{k} S_{k+1} \\\\[1em]\n",
    "&\\texttt{So mf easier!}\n",
    "\\end{align}$\n",
    "\n",
    "Similarly, we can say\n",
    "\n",
    "$\\;\\;\\;\\;P\\CB{X=k} = S_k - \\d{\\binom{k+1} {k}}S_{k+1} + \\cdots + (-1)^{n-k}\\d{\\binom{n} {k}}S_n = \\d{\\sum_{j=k}^{n}} (-1)^{k+j} \\d{\\binom{j} {k}} S_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this we will now prove that $P\\CB{X \\geq k} = \\d{\\sum_{j=k}^{n} (-1)^{k+j} \\binom{j-1}{k-1}S_j}$. We will use a backwards mathematical induction that starts with $k=n$. Now, when $k=n$ the preceding identity states that\n",
    "\n",
    "$$P\\CB{X=n} = Sn$$\n",
    "\n",
    "First step finished! So we assume that $P\\CB{X \\geq k+1} = \\d{\\sum_{j=k+1}^{n} (-1)^{k+1+j} \\binom{j-1} {k}S_j}$. And then\n",
    "\n",
    "$$\\begin{align}\n",
    "P\\CB{X \\geq k} &= P\\CB{X = k} + p\\CB{X \\geq k+1} \\\\[0.8em]\n",
    "&= \\left[\\sum_{j=k}^{n} (-1)^{k+j} \\binom{j} {k} S_j\\right] + \\left[ \\sum_{j=k+1}^{n} (-1)^{k+1+j} \\binom{j-1} {k}S_j \\right]\\\\\n",
    "&= S_k + \\left[ \\sum_{j=k+1}^{n} (-1)^{k+j} \\left[\\binom{j} {k} - \\binom{j-1} {k} \\right] S_j \\right] \\\\\n",
    "&= S_k + \\sum_{j=k+1}^{n} (-1)^{k+j} \\binom{j-1} {k-1}S_j = \\sum_{j=k}^{n} (-1)^{k+j} \\binom{j-1} {k-1}S_j \n",
    "\\end{align}$$\n",
    "\n",
    "All done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limit Theorems\n",
    "\n",
    "First we prove the **Markov's inequality**.\n",
    "\n",
    "$Proposition$ ***Markov's inequality***\n",
    "\n",
    "If $X$ is a $r.v.$ that takes only *nonnegative* values, then for any value $\\alpha > 0$,\n",
    "\n",
    "$$P\\CB{X \\geq a} \\leq \\ffrac{\\EE{X}} {a}$$\n",
    "\n",
    "$Proof$\n",
    "\n",
    "This proof is for the case where $X$ is continuous with density $f$.\n",
    "\n",
    "$$\\begin{align}\n",
    "\\EE{X} &= \\int_{0}^{\\infty} x\\cdot f(x) \\;\\dd{x}\\\\\n",
    "&= \\int_{0}^{a} x\\cdot f(x) \\;\\dd{x} + \\int_{a}^{\\infty} x\\cdot f(x) \\;\\dd{x}\\\\\n",
    "&\\geq \\int_{a}^{\\infty} x\\cdot f(x) \\;\\dd{x} \\geq \\int_{a}^{\\infty} a\\cdot f(x)\\;\\dd{x} \\\\\n",
    "&= a \\cdot P\\CB{X \\geq a}\n",
    "\\end{align}$$\n",
    "\n",
    "$Remark$\n",
    "\n",
    "From the process of proving it, we can easily find that this holds for discrete $r.v.$, and a slightly different result can be made if the $r.v.$ only takes nonpositive values.\n",
    "\n",
    "$Proposition$ ***Chebyshev's Inequality***\n",
    "\n",
    "If $X$ is a $r.v.$ with mean $\\mu$ and variance $\\sigma^2$, then, for any value $k > 0$,\n",
    "\n",
    "$$P \\CB{\\left|X - \\mu \\right| \\geq k} \\leq \\ffrac{\\sigma^2} {k^2}$$\n",
    "\n",
    "$Proof$\n",
    "\n",
    "Since $\\left(X - \\mu\\right)^2$ is a nonnegative $r.v.$, we can apply the previous proposition, the **Markov's inequality** (with $a = k^2$) to obtain:\n",
    "\n",
    "$$P\\CB{\\left(X - \\mu\\right)^2 \\geq k^2} = P\\CB{\\left|X - \\mu\\right| \\geq k} \\leq \\ffrac{\\EE{\\left(X-\\mu\\right)^2}} {k^2}$$\n",
    "\n",
    "$Remark$\n",
    "\n",
    "These two propositions are important for that they produce the methods to find a bound for a certain probability given limited infomations like the mean or the mean and the variane.\n",
    "\n",
    "**e.g.**\n",
    "\n",
    "The number of items produced in a factory during a week is a $r.v.$ with *mean* $500$.\n",
    "\n",
    "What's the probability that this week's production will be at least $1000$?\n",
    "\n",
    "> Let $X$ be the number of items that will be produced in a week.\n",
    ">\n",
    ">$P\\CB{X \\geq 1000} \\leq\\ffrac{\\EE{X}} {1000} = \\ffrac{500} {1000} = 0.5$\n",
    "\n",
    "If the variance is also given with value $100$, then what's the probability that this week's production will be between $400$ and $600$?\n",
    "\n",
    ">$P\\CB{|X-500| \\geq 100} \\leq \\ffrac{\\sigma^2} {100^2} = \\ffrac{1} {100}$, hence, $P\\CB{400 < X < 600} = 1 - \\ffrac{1} {100} = \\ffrac{99} {100}$.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Theorem$ ***Strong Law of Large Numbers***\n",
    "\n",
    "Let $X_1,X_2,\\dots$ be a sequence of *independent* $r.v.$ having a common distribution, and let $\\EE{X_i} = \\mu$. Then **with probability** $1$ (later will be shortened as $wp1$).\n",
    "\n",
    "$$\\lim_{n \\to \\infty}\\ffrac{X_1 + X_2 + \\cdots + X_n} {n} = \\mu$$\n",
    "\n",
    "$Theorem$ ***Central Limit Theorem***\n",
    "\n",
    "Let $X_1,X_2,\\dots$ be a sequence of $i.i.d.$ $r.v.$, each with mean $\\mu$ and variance $\\sigma^2$. Then \n",
    "\n",
    "$$\\lim_{n \\to \\infty} P\\CB{\\ffrac{X_1 + X_2 + \\cdots + X_n - n \\mu} {\\sigma \\sqrt{n}} \\leq a} = \\ffrac{1} {\\sqrt{2 \\pi}} \\int_{-\\infty}^{a} e^{-x^2/2} \\;\\dd{x}$$\n",
    "\n",
    "$Remark$\n",
    "\n",
    "This holds for *any* distribution of the $X_i$s! Herein lies its power! Say an **binomially** distributed $r.v.$ with parameters $n$ and $p$, $X$. Then $X$ can be seen as the sum of $n$ independent **Bernoulli** $r.v.$s, each with parameter $p$. Hence the distribution\n",
    "\n",
    "$$\\ffrac{X- \\EE{X}} {\\sqrt{\\Var{X}}} = \\ffrac{X - np} {\\sqrt{np(1-p)}}$$\n",
    "\n",
    "approaches the **standard normal** distribution as $n$ approaches $\\infty$. And this normal approximation will be generally great for values of $n$ satisfying $np(1-p) \\geq 10$. See the next example~\n",
    "\n",
    "**e.g.** From **Binomial** to **Normal**\n",
    "\n",
    "$X$ is the number of times that a fair coin, flipped $40$ times, lands *heads*. What's the probability that $X = 20$? How's the normal approximation comparing to the exact solution?\n",
    "\n",
    "> How to approximate a discrete $r.v.$ using a continuous $r.v.$? Here's the *trick*.\n",
    ">\n",
    ">$$\n",
    "\\begin{align}\n",
    "P\\CB{X=20} &= P\\CB{19.5 < X < 20.5} \\\\\n",
    "&= P\\CB{\\ffrac{19.5-20} {\\sqrt{10}} < \\ffrac{X - 20} {\\sqrt{10}} < \\ffrac{20.5 - 20} {\\sqrt{10}}} \\\\[0.6em]\n",
    "&= P\\CB{-0.16 < Z < 0.16} \\\\[0.7em]\n",
    "& \\mathbf{\\approx} \\Phi(0.16) - \\Phi(-0.16)\n",
    "\\end{align}$$\n",
    "\n",
    "$Remark$\n",
    "\n",
    "Here, $\\Phi(z)$ is the probability that the **standard normal** is less than $z$ and is given by\n",
    "\n",
    "$$\\Phi(z) = \\ffrac{1} {\\sqrt{2\\pi}} \\int_{-\\infty}^{z} e^{-x^2/2}\\;\\dd{x} $$\n",
    "\n",
    ">Thus by the symmetry of the **standard normal** distribution: $\\Phi(-0.16) = P\\CB{\\N{0,1} > 0.16} = 1 - \\Phi(0.16)$, where $\\N{0,1}$ is a **standard normal** $r.v.$. Hence the answer is\n",
    "\n",
    ">$$P\\CB{X = 20} \\approx 2\\Phi(0.16) - 1 = 0.1272$$\n",
    "\n",
    ">Then, the exact result is $P\\CB{X=20} = \\d{\\binom{40} {20}\\left(\\frac{1} {2}\\right)^{40}} = 0.1268$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following will be a heuristic proof of the ***CLT***, central limit theorem. We first suppose that $X_i$ have mean $0$ and variance $1$, and their common mgf is $\\EE{e^{tX}}$. Then the mgf of $\\ffrac{\\sum X_i} {\\sqrt{n}}$ is:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\EE{\\exp\\CB{t\\cdot\\left( \\ffrac{X_1+\\cdots+X_n} {\\sqrt{n}} \\right)}} &= \\EE{e^{tX_1/\\sqrt{n}} \\cdots e^{tX_n/ \\sqrt{n}}} \\\\\n",
    "&= \\left( \\EE{e^{tX/\\sqrt{n}}} \\right)^n\n",
    "\\end{align}$$\n",
    "\n",
    "From the **Taylor series expnsion** of $e^y$, for large $n$, we have\n",
    "\n",
    "$$e^{tX/\\sqrt{x}} = 1 + \\ffrac{tX} {\\sqrt{n}} + \\ffrac{t^2X^2} {2n} $$\n",
    "\n",
    "and since $\\EE{X} = 0$ and $\\EE{X^2} = 1$, we have\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\EE{\\exp\\CB{t\\cdot\\left( \\ffrac{X_1+\\cdots+X_n} {\\sqrt{n}} \\right)}} &= \\left( \\EE{e^{tX/\\sqrt{n}}} \\right)^n \\\\\n",
    "&= \\left(1 + \\ffrac{t^2} {2n}\\right)^n \\\\\n",
    "&\\to e^{t^2/2} \\;\\;\\;\\;\\text{as } n \\to \\infty\n",
    "\\end{align}$$\n",
    "\n",
    "This is the mgf of a **standard normal** $r.v.$ with mean $0$ and variance $1$. So we can already say that the $r.v.$ $\\ffrac{X_1 + \\cdots + X_n} {\\sqrt{n}}$ converges to the **standard normal** distribution function $\\Phi$.\n",
    "\n",
    "And then when $X_i$ have mean $\\mu$ and variance $\\sigma^2$, we convert them to $\\ffrac{X_i - \\mu} {\\sigma}$ with mean $0$ and $1$. Thus the preceding shows that:\n",
    "\n",
    "$$P\\CB{\\ffrac{X_1 - \\mu + \\cdots +X_n - \\mu} {\\sigma\\sqrt{n}} \\leq a} \\to \\Phi(a)$$\n",
    "\n",
    "which proves the **CLT**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Processes\n",
    "\n",
    "A ***stochastic process*** $\\CB{X(t), t \\in T}$ is a *collection* of $r.v.$. The index $t$ is often interpreted as ***time*** and as a result, we refer to $X(t)$ as the ***state*** of the process at time $t$. And it must be *infinite* elements in it.\n",
    "\n",
    "The set $T$ is the ***index set*** of the process. When $T$ is countable set, the stochastic process is said to be a ***discrete-time process***. And if $T$ is an interval of the real line, the stochastic process is said to be a ***continuous-time process***.\n",
    "\n",
    "The ***state space*** of a stochastic process is the set of *ALL possible values* that the $r.v.$ $X(t)$ can assume.\n",
    "\n",
    "- State Space and Time Parameter are both Discrete (Random Walk)\n",
    "- State Space is continuous and Time Parameter is Discrete (Common)\n",
    "- State Space is Discrete and Time Parameter is continuous (Poisson Process)\n",
    "- State Space and Time Parameter are both Continuous (Brownian Motion Process)\n",
    "\n",
    "**e.g.**\n",
    "\n",
    "Consider a partical that moves along a set of $m+1$ nodes, labeled from $0$ to $m$, that are arranged arround a circle. At each step the particle is equally likely to move one position in either the clockwise or counterclockwise direction. That is, if $X_n$ is the position of the particle after its $n\\texttt{th}$ step, then\n",
    "\n",
    "$$P\\CB{X_{n+1} = i+1 \\mid X_n = i} = P \\CB{X_{n+1} = i-1 \\mid X_n = i} = \\ffrac{1} {2}$$\n",
    "\n",
    "where we let $i+1=0$ when $i=m$ and $i-1=m$ when $i=0$. Now the particle starts at $0$ and continues to move around according to the preceding rules until all the nodes have been visited. What is the probability that node $i$ is the last one visited?\n",
    "\n",
    "![](./figs/e.g.2.53.png)\n",
    "\n",
    "> Consider the first time that the particle is at one of the two neighbors of node $i$ not $0$ as assumed, say, node $i − 1$. Since either node $i$ or $i+1$ has yet been visited, it follows that $i$ will be the last node visited $iff$ $i+1$ is visited before $i$. So that the particle will progress $m − 1$ steps in a specified direction before progressing one step in the other direction. \n",
    "\n",
    ">That is, it is equal to the probability that a gambler who starts with one unit, and wins one when a fair coin turns up heads and loses one when it turns up tails, will have his fortune go up by $m − 1$ before he goes broke. Hence, because the preceding $\\QQQ$ implies that the probability that node $i$ is the last node visited is the same for all $i$, and because these probabilities must sum to $1$, we obtain\n",
    "\n",
    ">$$P \\CB{ i \\text{ is the last node visited}} = 1/ m$$\n",
    "\n",
    "$Remark$\n",
    "\n",
    "Consider that gambler again. He going down $n$ before being up $1$ is with probability $\\QQQ$ $1/(n+1)$; or equivalently,\n",
    "\n",
    "$$P\\CB{\\text{gambler is up }1\\text{ before being down }n} = \\ffrac{n} {n+1}$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& P\\CB{\\text{gambler is up }2\\text{ before being down }n} \\\\\n",
    "=& P\\CB{\\text{up }2\\text{ before down }n \\mid \\text{up }1\\text{ before down }n} \\cdot \\ffrac{n} {n+1} \\\\\n",
    "=& P\\CB{\\text{up }2\\text{ before down }n+1}\\cdot \\ffrac{n} {n+1} \\\\\n",
    "=& \\ffrac{n+1} {n+2}\\ffrac{n} {n+1} = \\ffrac{n} {n+2}\n",
    "\\end{align}$$\n",
    "\n",
    "Repeating this argument yields that\n",
    "\n",
    "$$P\\CB{\\text{gambler is up }k\\text{ before being down }n} = \\ffrac{n} {n+k}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "360px",
    "width": "379px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "494px",
    "left": "0px",
    "right": "1135px",
    "top": "115px",
    "width": "231px"
   },
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
