{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projector\n",
    "$Def$\n",
    "\n",
    "If a square matrix $P \\in \\mathbb{R}^{m\\times m}$ satisfies $P^2 = P$, then we call $P$ a **projector**. **Range** of $A$, the set of images, $i.e.$, *the space spanned by the columnss of matrix $A$*. **Null** of $A$, all vectors whose images are zero, $i.e.$, all $\\vec{x}$ satisfying $A\\vec{x} = \\vec{0}$.\n",
    "\n",
    "$Theorem$ **1**\n",
    "\n",
    "For ***any*** matrix $A\\in \\mathbb{R}^{m \\times m}$, $\\DeclareMathOperator*{\\null}{null} \\DeclareMathOperator*{\\range}{range} \\boxed{ \\null\\left(A\\right) \\subseteq \\range\\left(I-A\\right) }$.\n",
    "\n",
    "$Proof$\n",
    "\n",
    "If $\\vec{x} \\in \\null \\left(A\\right)$, then $A\\vec{x} = \\vec{0}$. Then $\\vec{x} = \\vec{x} - A\\vec{x} = \\left(I - A \\right) \\vec{x} \\in \\range\\left(I-A\\right)$\n",
    "\n",
    "$Theorem$ **2**\n",
    "\n",
    "Let $P \\in \\mathbb{R}^{m \\times m}$. Then $\\boxed{ P^2 = P}$ is *eqivalent* to $\\boxed{ \\null(P) = \\range(I - P)}$\n",
    "\n",
    "$Proof$\n",
    "\n",
    "$\\Rightarrow)$ Since we already have $\\null (P) \\subseteq \\range(I-P)$ from *Theorem 1*, now we prove that $\\null(P) \\supseteq \\range(I-P)$. So for $\\vec{x} \\in \\range(I-P)$, there exists a $\\vec{y}$ such that $\\vec{x} = (I-P)\\vec{y}$, so that $P\\vec{x} = P(I-P)\\vec{y} = (P - P^2)\\vec{y} = \\vec{0}$, so $\\vec{x} \\in \\null(P)$. Half Done!\n",
    "\n",
    "$\\Leftarrow)$ Since now $\\forall \\vec{x} \\in \\mathbb{R}^{m}$, $(I-P)\\vec{x} \\in \\range(I - P) = \\null(P)$, so that $P\\big( (I-P)\\vec{x} \\big) = (P - P^2)\\vec{x} = \\vec{0}$, that is to say that in terms of map, $P$ and $P^2$ are equivalent, $i.e.$, $P = P^2$. ALL DONE! $\\square$\n",
    "\n",
    "In general, any projector $P \\in \\mathbb{R}^{ m \\times m}$ maps $\\mathbb{R}^{ m}$ onto its **range**. For any vector $\\vec{v}$ in the range of $P$, $P\\vec{v} = \\vec{v}$. For any vector $\\vec{v}$ *not* in the **range** of $P$, the difference $\\vec{v} − P\\vec{v}$ is in the **null** of $P$.\n",
    "\n",
    "$Def$  \n",
    "\n",
    "**orthogonal projector** and **oblique projector**: Let $P \\in \\mathbb{R}^{ m \\times m}$ be a projector. If $\\range(I-P) \\perp \\range(P)$, $i.e.$, $\\null(P) \\perp \\range(P)$, then it is an orthogonal projector, otherwise oblique projector.\n",
    "\n",
    ">**e.g.** $B = \\left[ \\begin{array}{ccc}\n",
    "1 & 1\\\\\n",
    "0 & 0\n",
    "\\end{array}\\right]$\n",
    ">\n",
    ">Now $B^2 = \\left[ \\begin{array}{ccc}\n",
    "1 & 1\\\\\n",
    "0 & 0\n",
    "\\end{array}\\right] = B$, so it is a projector. And its **range** is the space spanned by $\\left[ \\begin{array}{c}\n",
    "1\\\\\n",
    "0\n",
    "\\end{array}\\right]$, and its **null** is spanned by $\\left[ \\begin{array}{c}\n",
    "1\\\\\n",
    "-1\n",
    "\\end{array}\\right]$. \n",
    ">\n",
    ">And obviously $\\range(B) \\not \\perp \\null(B)$\n",
    "\n",
    "How to determine whether it is oblique or orthogonal projector?\n",
    "\n",
    "$Theorem$ **3**\n",
    "\n",
    "Let $P \\in \\mathbb{R}^{ m \\times m}$ be a projector. $P$ is an *orthogonal projector* $iff$ $\\boxed{ P = P^{\\mathrm{T}} }$.\n",
    "\n",
    "$Proof$\n",
    "\n",
    "$\\Rightarrow)$ Denote the dimension of $\\range(P)$ by $r$, so that the dimension of $\\null(P)$ is $m-r$. Denote the basis of $\\range(P)$ by $\\vec{q}_{1}, \\vec{q}_{2},\\dots,\\vec{q}_{r}$, and the basis of $\\null(P)$ by $\\vec{q}_{r+1}, \\vec{q}_{r+2},\\dots,\\vec{q}_{m}$. Since $P$ is orthogonal projector, $\\{ \\vec{q}_{1}, \\vec{q}_{2},\\dots,\\vec{q}_{r} \\}$ are orthogonal to $\\{ \\vec{q}_{r+1}, \\vec{q}_{r+2},\\dots,\\vec{q}_{m} \\}$. \n",
    "\n",
    "Let $Q = \\left[ \\begin{array}{cccc}\n",
    "\\vec{q}_1 & \\vec{q}_2 & \\cdots & \\vec{q}_m\n",
    "\\end{array}\\right]$, so that $Q$ is an orthogonal matrix, $Q^{\\mathrm{T}}Q = I$. Then we have\n",
    "\n",
    "$$\\begin{align}\n",
    "Q^{\\mathrm{T}}PQ =& \\left[ \\begin{array}{cccc}\n",
    "\\vec{q}_1 & \\vec{q}_2 & \\cdots & \\vec{q}_m\n",
    "\\end{array}\\right]^{\\mathrm{T}} P \\left[ \\begin{array}{cccc}\n",
    "\\vec{q}_1 & \\vec{q}_2 & \\cdots & \\vec{q}_m\n",
    "\\end{array}\\right] \\\\\n",
    "=& \\left[ \\begin{array}{cccc}\n",
    "\\vec{q}_1 & \\vec{q}_2 & \\cdots & \\vec{q}_m\n",
    "\\end{array}\\right]^{\\mathrm{T}} \\left[ \\begin{array}{cccc}\n",
    "P\\vec{q}_1 & P\\vec{q}_2 & \\cdots & P\\vec{q}_m\n",
    "\\end{array}\\right] \\\\\n",
    "=& \\left[ \\begin{array}{ccccccc}\n",
    "\\vec{q}_1 & \\vec{q}_2 & \\cdots & \\vec{q}_r & \\vec{q}_{r+1} & \\cdots & \\vec{q}_m\n",
    "\\end{array}\\right]^{\\mathrm{T}} \\left[ \\begin{array}{ccccccc}\n",
    "\\vec{q}_1 & \\vec{q}_2 & \\cdots & \\vec{q}_r & 0 & \\cdots & 0\n",
    "\\end{array}\\right] \\\\\n",
    "=& I_r\n",
    "\\end{align}$$\n",
    "\n",
    "Therefore\n",
    "\n",
    "$$\\begin{align}\n",
    "P =& QI_rQ^{\\mathrm{T}} \\\\\n",
    "=& Q(I_rI_r)Q^{\\mathrm{T}} = (QI_r)(I_rQ^{\\mathrm{T}}) \\\\\n",
    "=& \\left[ \\begin{array}{cccc}\n",
    "\\vec{q}_1 & \\vec{q}_2 & \\cdots & \\vec{q}_r\n",
    "\\end{array}\\right] \\left[ \\begin{array}{cccc}\n",
    "\\vec{q}_1^{\\mathrm{T}} & \\vec{q}_2^{\\mathrm{T}} & \\cdots & \\vec{q}_r^{\\mathrm{T}}\n",
    "\\end{array}\\right]^{\\mathrm{T}} \\\\\n",
    "=& Q_r Q_r^{\\mathrm{T}}\n",
    "\\end{align}$$\n",
    "\n",
    "OK, now $P$ is symmetrix.\n",
    "\n",
    "$Conclusion$\n",
    "\n",
    "If $P$ is orthogonal projector, then $P$ can be written as $\\sum \\limits_{i = 1}^{r} \\vec{q}_i \\vec{q}_{i} ^{ \\mathrm{T}}$, where $\\vec{q}_1,\\vec{q}_2,\\dots,\\vec{q}_r$ are set of orthonormal basis vectors of $\\range(P)$. And on the other hand, if $Q_r = \\left[ \\begin{array}{cccc}\n",
    "\\vec{q}_1 & \\vec{q}_2 & \\cdots & \\vec{q}_r\n",
    "\\end{array}\\right]$ contains a set of orthonormal vectors in $\\mathbb{R}^{m}$, then the orthogonal projector from $\\mathbb{R}^{m}$ to the range of $Q_r$ is $P = Q_rQ_r^{\\mathrm{T}} = \\sum \\limits_{i = 1}^{r} \\vec{q}_i \\vec{q}_{i} ^{ \\mathrm{T}}$.\n",
    "\n",
    "In general for any given vector $\\vec{v} \\in \\mathbb{R}^{m}$, not necessarily a normalized vector, the orthogonal projector to the direction $\\vec{v}$ is\n",
    "\n",
    "$$P_{\\vec{v}} = \\bigg(\\frac{\\vec{v}} {\\|\\vec{v}\\|}\\bigg)\\bigg( \\frac{\\vec{v}} {\\|\\vec{v}\\|}\\bigg)^{\\mathrm{T}} = \\boxed{ \\frac{\\vec{v}\\vec{v}^{\\mathrm{T}}} {\\|\\vec{v}\\|^2}}$$\n",
    "\n",
    "After that, given any vector $\\vec{x} \\in \\mathbb{R}^{m}$, we have $P_{\\vec{v}}\\vec{x}$ is the orthogonal projection of $\\vec{x}$ onto the direction $\\vec{v}$ and the difference $\\vec{x} − P_{\\vec{v}}\\vec{x}$ is perpendicular to $\\vec{v}$.\n",
    "\n",
    "And more generally, given a matrix $W \\in \\mathbb{R}^{m \\times n}$, assuming that $m \\geq n$ and the columns of $W$ are linearly independent. The orthogonal projector from $\\mathbb{R}^{m}$ to the column space (the range) of $W$ can be determined as follows, denoted as $P$.\n",
    "1. $\\vec{v}$ be any vector in $\\mathbb{R}^{m}$ and $\\vec{y} = P\\vec{v} \\in \\range(W)$ is the image of $\\vec{v}$ under this orthogonal projector.\n",
    "2. Since the projector is orthogonal, the difference $\\vec{v} − \\vec{y}$ is orthogonal to $\\range(W)$, $i.e.$, we have $W^{\\mathrm{T}}(\\vec{v} − \\vec{y}) = \\vec{0}$.\n",
    "3. We also have $\\vec{y} = W\\vec{x}$, for some $\\vec{x} \\in \\mathbb{R}^{n}$, so that $W^{\\mathrm{T}}(\\vec{v} − W\\vec{x}) = \\vec{0}$, $i.e.$, $\\vec{x} = \\big( W^{\\mathrm{T}}W \\big)^{-1}W^{\\mathrm{T}}\\vec{v}$. Since $W$ is of full column rank as assumed ahead, $\\big( W^{\\mathrm{T}}W \\big)^{-1}$ exists.\n",
    "4. $\\vec{y} = P\\vec{v} = W\\vec{x} = W\\big( W^{\\mathrm{T}}W \\big)^{-1}W^{\\mathrm{T}}\\vec{v} \\Rightarrow \\boxed{ P = W\\big( W^{\\mathrm{T}}W \\big)^{-1}W^{\\mathrm{T}}}$.\n",
    "\n",
    "$\\dagger$ ***NO*** orthogonal matrix, except the identity matrix, is an orthogonal projector.$\\ddagger$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Householder reflector\n",
    "To do Gaussian elimination. For any given vector $\\vec{x}$, we want to find an operator (matrix) $F$ such that, \n",
    "\n",
    "$$\\vec{x} = \\left[ \\begin{array}{c}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "\\vdots \\\\\n",
    "x_n\n",
    "\\end{array}\\right] \\overset{F}{\\longrightarrow } F\\vec{x} = \\left[ \\begin{array}{c}\n",
    "\\pm \\left\\| \\vec{x}\\right\\|_2 \\\\\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "0\n",
    "\\end{array}\\right] = \\pm \\left\\| \\vec{x}\\right\\|_2 \\vec{e}_1$$\n",
    "\n",
    "All are zeros below the first entry. Then how to determine matrix $F$? Assume that $\\vec{x}$ is reflected to $F\\vec{x} = \\|\\vec{x}\\|_2 \\vec{e}_1$, the positive direction of $x\\text{-axis}$.\n",
    "1. Define $\\vec{v} = \\vec{x} - F\\vec{x} = \\vec{x} - \\|\\vec{x}\\|_2 \\vec{e}_1$\n",
    "2. Following above, the orthogonal projector onto the direction of $\\vec{v}$ is $P_{\\vec{v}} = \\newcommand{\\ffrac}{\\displaystyle \\frac} \\ffrac{\\vec{v}\\vec{v}^{\\mathrm{T}}} {\\|\\vec{v}\\|_2^2}$, so that we can see that $\\vec{v} = 2P_{\\vec{v}}\\vec{x}$\n",
    "3. $F\\vec{x} = \\vec{x} - \\vec{v} = (I - 2P_{\\vec{v}})\\vec{x}$, $i.e.$, $\\boxed{ F = I - 2P_{\\vec{v}} = I - 2 \\displaystyle \\frac{\\vec{v}\\vec{v}^{\\mathrm{T}}} {\\|\\vec{v}\\|_2^2}}$\n",
    "\n",
    "$Proof$\n",
    "\n",
    "Brief proof of the second point.\n",
    "\n",
    "To prove $2P_{\\vec{v}}\\vec{x} = 2\\cdot\\ffrac{\\vec{v}\\vec{v}^{\\mathrm{T}}\\vec{x}} {\\|\\vec{v}\\|_2^2}= \\vec{v}$, it equals to prove that $2\\vec{v}^{\\mathrm{T}}\\vec{x} = \\|\\vec{v}\\|_2^2$.\n",
    "\n",
    "$$2\\vec{v}^{\\mathrm{T}}\\vec{x} = 2(\\vec{x}^{\\mathrm{T}} - \\|\\vec{x}\\|_2 \\vec{e}_1^{\\mathrm{T}})\\vec{x} = 2\\|\\vec{x}\\|_2^2 - 2\\|\\vec{x}\\|_2 \\cdot x_1 \\\\\n",
    "\\|\\vec{v}\\|_2^2 = x_1^2 -2\\|\\vec{x}\\|_{2}x_{1} + \\|\\vec{x}\\|_2^2 + x_2^2 + \\cdots + x_n^2 = 2\\|\\vec{x}\\|_2^2 - 2\\|\\vec{x}\\|_2 \\cdot x_1$$\n",
    "\n",
    "ALL DONE! $\\square$\n",
    "\n",
    "***\n",
    "Now we have found what we want, which reflects the vector $\\vec{x}$ to the direction of $\\vec{e}_1$ by multiplying matrix (operator) $F$. Besides we have\n",
    "\n",
    "$$F^{\\mathrm{T}}F = (I - 2P_{\\vec{v}})^{\\mathrm{T}}(I - 2P_{\\vec{v}}) = I^2 - 4P_{\\vec{v}} + 4P_{\\vec{v}}^2 = I - 4P_{\\vec{v}} + 4P_{\\vec{v}} = I$$\n",
    "\n",
    "So actually this *Householder reflector* is an *orthogonal matrix*.\n",
    "\n",
    "## Which reflector\n",
    "For a given $\\vec{x}$ we have\n",
    "\n",
    "| $$F\\vec{x} = +\\sideset{}{^{2}}{\\|\\vec{x}\\|} \\sideset{}{_{1}}{\\vec{e}}$$ | $$F\\vec{x} = -\\sideset{}{^{2}}{\\|\\vec{x}\\|} \\sideset{}{_{1}}{\\vec{e}}$$ |\n",
    "|:---------------------------------------------------------------------:|:---------------------------------------------------------------------:|\n",
    "|                     ![](./Raw/+.png)                    |                     ![](./Raw/-.png)                    |\n",
    "\n",
    "As mentioned before, subtracting two numbers which are *close* is an **ill-conditioned** problem. So that we prefer that $\\vec{x}$ and $F\\vec{x}$ are in opposite direction, $i.e.$\n",
    "\n",
    "$$\\DeclareMathOperator*{\\sign}{sign}\n",
    "F\\vec{x} = \\left[\\begin{array}{c}\n",
    "-\\sign(x_1)\\|\\vec{x}\\|_2 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "0\n",
    "\\end{array} \\right]\n",
    "= -\\sign(x_1) \\|\\vec{x}\\|_2\\vec{e}_1 $$\n",
    "\n",
    "And then $\\vec{v} = \\vec{x} - F\\vec{x} = \\vec{x} + \\sign(x_1)\\|\\vec{x}\\|_2\\vec{e}_1$, while $F$ keeps the same expression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QR factorization by Householder reflectors\n",
    "Now we can use the Householder reflectors to reduce a matrix $A$ to its upper trigangular form. Let $A$ be an $m\\times n$ size matrix, assuming that $m \\geq n$ and $\\DeclareMathOperator*{\\rank}{rank} \\rank(A) = n$, a column full rank matrix.\n",
    "\n",
    "We got our first operator $F_1$ that can reflect the first column of $A$ to its $\\vec{e}_1$ direction.\n",
    "1. Take $\\vec{x} = \n",
    "\\left[\\begin{array}{c}\n",
    "a_{11} \\\\\n",
    "a_{21} \\\\\n",
    "\\vdots \\\\\n",
    "a_{m1} \n",
    "\\end{array} \\right]\n",
    "$, and the projection $F_1\\vec{x} = -\\sign(x_1)\\|\\vec{x}\\|_2 \\vec{e}_1$,then $\\vec{v}_1 = \\vec{x} - F_1 \\vec{x} = \\vec{x} + \\sign(x_1) \\|\\vec{x}\\|_2 \\vec{e}_1$.\n",
    "2. We can get the Householder reflector:\n",
    "$$Q_1 = F_1 = I - 2 \\frac{\\vec{v}_1\\vec{v}_1^{\\mathrm{T}}} {\\|\\vec{v}_1\\|_2^2}$$\n",
    "3. Now we have\n",
    "$$Q_1A = \\left[\\begin{array}{cccc}\n",
    "a_{11}^{(1)} & a_{12} ^{(1)} & \\cdots & a_{1n} ^{(1)} \\\\\n",
    "0 & a_{22} ^{(1)} & \\cdots & a_{2n} ^{(1)} \\\\\n",
    "0 & a_{32} ^{(1)} & \\cdots & a_{3n} ^{(1)} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "0 & a_{m2} ^{(1)} & \\cdots & a_{mn} ^{(1)} \\\\ \n",
    "\\end{array} \\right]$$\n",
    "So now we take $\\vec{x} = \n",
    "\\left[\\begin{array}{c}\n",
    "a_{22} ^{(1)} \\\\\n",
    "a_{32} ^{(1)} \\\\\n",
    "\\vdots \\\\\n",
    "a_{m2} ^{(1)} \n",
    "\\end{array} \\right]\n",
    "$, and the projection $F_2\\vec{x} = -\\sign(x_1)\\|\\vec{x}\\|_2 \\vec{e}_1$,then $\\vec{v}_2 = \\vec{x} - F_2 \\vec{x} = \\vec{x} + \\sign(x_1) \\|\\vec{x}\\|_2 \\vec{e}_1$.\n",
    "4. We can get the second Householder reflector:\n",
    "$$F_2 = I - 2 \\frac{\\vec{v}_2\\vec{v}_2^{\\mathrm{T}}} {\\|\\vec{v}_2\\|_2^2}$$\n",
    "Notice that at this time $F_2$ is an $(m-1)\\times (m-1)$ orthogonal matrix, so we define\n",
    "$$Q_2 = \\left[\\begin{array}{cc}\n",
    "1 & \\vec{0}^{\\mathrm{T}} \\\\\n",
    "\\vec{0} & F_2\n",
    "\\end{array} \\right]_{m \\times m}$$\n",
    "\n",
    "So on and so forth, after $n$ times loop, we get the QR factorization as following:\n",
    "\n",
    "$$Q_{n}Q_{n-1}\\cdots Q_{2}Q_{1}A = \\left[\\begin{array}{cccc}\n",
    "a_{11}^{(1)} & a_{12} ^{(1)} & \\cdots & a_{1n} ^{(1)} \\\\\n",
    "0 & a_{22} ^{(2)} & \\cdots & a_{2n} ^{(2)} \\\\\n",
    "0 & 0 & \\ddots & \\vdots \\\\\n",
    "\\vdots & \\vdots & \\ddots & a_{nn} ^{(n)} \\\\\n",
    "0 & 0 & \\cdots & 0 \\\\ \n",
    "\\vdots & \\vdots & & \\vdots \\\\\n",
    "0 & 0 & \\cdots & 0 \n",
    "\\end{array} \\right]_{m \\times n} = R$$\n",
    "\n",
    "$R$ is upper triangular, and each $Q_i$ is an orthogonal matrix. So $Q_i^{\\mathrm{T}} = Q_i^{-1}$. And seeing from how we create $Q_i$, we also have $Q_i = Q_i^{\\mathrm{T}}$\n",
    "\n",
    "$$A = Q_1Q_2Q_3\\cdots Q_nR := Q_{m \\times m}R_{m \\times n}$$\n",
    "\n",
    "$Algorithm$ **Householder Triangularization**\n",
    "\n",
    "```\n",
    "for k = 1 to n\n",
    "    x = A_{k:m,k}\n",
    "    v_k = x + sign(x_1) \\|x\\|_2 e_1\n",
    "    v_k = v_k / \\|v_k\\|_2\n",
    "    A_{k:m,k:n} = A_{k:m,k:n} − 2v_k ( v_k^T A_{k:m,k:n} )\n",
    "end\n",
    "```\n",
    "\n",
    "The amount on floating point operations of the Householder Triangularization is approximately (): \n",
    "\n",
    "$$\\begin{align}\n",
    "\\sum_{k=1}^n \\sum_{j=k}^{n} 4(m-k+1) =& \\sum_{k=1}^n 4(m-k+1)(n-k+1) \\\\\n",
    "=& \\sum_{k=1}^n 4\\big(mn+m+n+1+k^2+2k(m+1)(n+1)\\big) \\\\\n",
    "\\approx & 2mn^2 - \\frac{2} {3} n^3\n",
    "\\end{align}$$\n",
    "***\n",
    "After the above algorithm, the result $R$ is stored in the upper triangular part of $A$. As for $Q$, it's gone. But we can rebuild it using the saved vector $\\vec{v}_k$.\n",
    "\n",
    "Why $Q$ is not needed? Because we use QR factorization to solve the equation $A\\vec{x} = \\vec{b}$. Then we will have $Q^{\\mathrm{T}}QR\\vec{x} = Q^{\\mathrm{T}}\\vec{b} = R\\vec{x}$. So actually what we need is the product of matrix $Q$ (or its transpose) and a given vector $\\vec{b}$, which can be achieved with the stored $\\vec{v}_k$.\n",
    "\n",
    "$$\\begin{align}\n",
    "Q =& Q_1Q_2Q_3\\cdots Q_n \\\\\n",
    "=& \\big( I_{m} - 2\\vec{v}_1 \\vec{v}_1^{\\mathrm{T}} \\big) \\left( \\begin{array}{cc}\n",
    "1 & \\vec{0}^{\\mathrm{T}} \\\\\n",
    "\\vec{0} & I_{m-1} - 2\\vec{v}_2 \\vec{v}_2^{\\mathrm{T}}\n",
    "\\end{array} \\right) \\cdots \\left( \\begin{array}{cc}\n",
    "I_{n-1} & \\mathbf{0} \\\\\n",
    "\\mathbf{0} & I_{m-n+1} - 2\\vec{v}_n \\vec{v}_n^{\\mathrm{T}}\n",
    "\\end{array} \\right) \\\\[1em]\n",
    "Q^{\\mathrm{T}} =& Q_n^{\\mathrm{T}}Q_{n-1}^{\\mathrm{T}}Q_{n-2}^{\\mathrm{T}}\\cdots Q_1^{\\mathrm{T}} = Q_nQ_{n-1}Q_{n-2}\\cdots Q_1 \\\\\n",
    "=& \\left( \\begin{array}{cc}\n",
    "I_{n-1} & \\mathbf{0} \\\\\n",
    "\\mathbf{0} & I_{m-n+1} - 2\\vec{v}_n \\vec{v}_n^{\\mathrm{T}}\n",
    "\\end{array} \\right) \\left( \\begin{array}{cc}\n",
    "I_{n-2} & \\mathbf{0} \\\\\n",
    "\\mathbf{0} & I_{m-n+2} - 2\\vec{v}_{n-1} \\vec{v}_{n-1}^{\\mathrm{T}}\n",
    "\\end{array} \\right) \\cdots \\left( \\begin{array}{cc}\n",
    "1 & \\vec{0}^{\\mathrm{T}} \\\\\n",
    "\\vec{0} & I_{m-1} - 2\\vec{v}_2 \\vec{v}_2^{\\mathrm{T}}\n",
    "\\end{array} \\right) \n",
    "\\big( I_m - 2\\vec{v}_1 \\vec{v}_1^{\\mathrm{T}} \\big)\n",
    "\\end{align}$$\n",
    "\n",
    "$Algorithm$ **Given $\\vec{b}$, find $Q\\vec{b}$**\n",
    "\n",
    "```\n",
    "x = b\n",
    "for k = n : −1 : 1\n",
    "    x_{k:m} = x_{k:m} − 2 v_k ( v_k^T x_{k:m} )\n",
    "end\n",
    "```\n",
    "\n",
    "$Algorithm$ **Given $\\vec{b}$, find $Q^{\\mathrm{T}}\\vec{b}$**\n",
    "\n",
    "```\n",
    "x = b\n",
    "for k = 1 : n\n",
    "    x_{k:m} = x_{k:m} − 2 v_k ( v_k^T x_{k:m} )\n",
    "end\n",
    "```\n",
    "\n",
    "And if you still want the explicit $Q$, just using $Q\\vec{e}_1, Q\\vec{e}_2, \\dots , Q\\vec{e}_n$ to get that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QR factorization by Gram-Schmidt orthogonalization\n",
    "Still we assume that $A$ be an $m\\times n$ size matrix, and $m \\geq n$ with $\\rank(A) = n$, a column full rank matrix. Let $Q \\in \\mathbb{R}^{m \\times m}$ and $R \\in \\mathbb{R}^{m \\times n}$ be the result of QR factorization.\n",
    "\n",
    "$$A = \\left[\\begin{array}{cccc}\n",
    "\\vec{a}_1^c & \\vec{a}_2^c & \\cdots \\vec{a}_n^c \n",
    "\\end{array} \\right] = \\left[\\begin{array}{cccc}\n",
    "\\vec{q}_1^c & \\vec{q}_2^c & \\cdots \\vec{q}_m^c \n",
    "\\end{array} \\right]\\left[\\begin{array}{c}\n",
    "\\begin{array}{cccc}\n",
    "r_{11} & r_{12} & \\cdots & r_{1n} \\\\\n",
    "0 & r_{22} & \\cdots & r_{2n} \\\\\n",
    "\\vdots & 0 & \\ddots & \\vdots \\\\\n",
    "0 & 0 & \\cdots & r_{nn}\n",
    "\\end{array} \\\\\n",
    "\\mathbf{0}\n",
    "\\end{array} \\right]=QR$$\n",
    "\n",
    "But actually we don't need a **full QR factorization**, here's the **reduced** one.\n",
    "\n",
    "$$A = \\left[\\begin{array}{cccc}\n",
    "\\vec{a}_1^c & \\vec{a}_2^c & \\cdots \\vec{a}_n^c \n",
    "\\end{array} \\right] = \\left[\\begin{array}{cccc}\n",
    "\\vec{q}_1^c & \\vec{q}_2^c & \\cdots \\vec{q}_n^c \n",
    "\\end{array} \\right]\\left[\\begin{array}{cccc}\n",
    "r_{11} & r_{12} & \\cdots & r_{1n} \\\\\n",
    "0 & r_{22} & \\cdots & r_{2n} \\\\\n",
    "\\vdots & \\ddots & \\ddots & \\vdots \\\\\n",
    "0 & \\cdots & 0 & r_{nn}\n",
    "\\end{array}\\right]=\\hat{Q}\\hat{R}$$\n",
    "\n",
    "And more easily we can find that\n",
    "\n",
    "$\\begin{align}\n",
    "\\vec{a}_1^c =& r_{11} \\vec{q}_1^c \\\\\n",
    "\\vec{a}_2^c =& r_{12} \\vec{q}_1^c  + r_{22} \\vec{q}_2^c \\\\\n",
    "\\vdots & \\\\\n",
    "\\vec{a}_j^c =& r_{1j} \\vec{q}_1^c  + r_{2j} \\vec{q}_2^c + \\cdots + r_{jj} \\vec{q}_j^c\\\\\n",
    "\\vdots & \\\\\n",
    "\\vec{a}_n^c =& r_{1n} \\vec{q}_1^c  + r_{2n} \\vec{q}_2^c + \\cdots + r_{nn} \\vec{q}_j^c\\\\\n",
    "\\end{align}$\n",
    "\n",
    "$Conclusion$\n",
    "\n",
    "1. $\\vec{a}_j^c \\in \\big< \\vec{q}_1^c, \\vec{q}_2^c, \\dots, \\vec{q}_j^c \\big>$, $j = 1,2,\\dots, n$\n",
    "2. $r_{jj}\\neq 0$. Firstly that $r_{11}$ can't, otherwise $\\vec{a}_1^c = \\vec{0}$, however $A$ is a column full rank matrix. And for $j = 2,3, \\dots, n$, still $r_{jj}\\neq 0$, otherwise $\\vec{a}_1^c, \\vec{a}_2^c, \\dots, \\vec{a}_j^c$ can be expressed by $\\big< \\vec{q}_1^c, \\vec{q}_2^c, \\dots, \\vec{q}_{j-1}^c \\big>$, which implies that these $j$ columns are linear dependent, can't be! (And we can get to this by seeing how we get the $R$.)\n",
    "3. $\\left\\{\\begin{align}\n",
    "\\vec{q}_1^c =& \\ffrac{\\vec{a}_1^c} {r_{11}} \\\\\n",
    "\\vec{q}_2^c =& \\ffrac{\\vec{a}_2^c - r_{12}\\vec{q}_1^{c}} {r_{22}} \\\\\n",
    "\\vdots & \\\\\n",
    "\\vec{q}_j^c =& \\ffrac{\\vec{a}_j^c - r_{1j}\\vec{q}_1^{c} - r_{2j}\\vec{q}_2^{c} -\\cdots -r_{j-1,j}\\vec{q}_{j-1}^{c}} {r_{jj}}\\\\\n",
    "\\vdots & \\\\\n",
    "\\vec{q}_n^c =& \\ffrac{\\vec{a}_n^c - r_{1n}\\vec{q}_1^{c} - r_{2n}\\vec{q}_2^{c} -\\cdots -r_{n-1,n}\\vec{q}_{n-1}^{c}} {r_{nn}}\\\\\n",
    "\\end{align}\\right.$\n",
    "4. $\\vec{q}_j^c \\in \\big< \\vec{a}_1^c, \\vec{a}_2^c, \\dots, \\vec{a}_j^c \\big>$, $j = 1,2,\\dots, n$\n",
    "5. $\\big< \\vec{a}_1^c, \\vec{a}_2^c, \\dots, \\vec{a}_j^c \\big> = \\big< \\vec{q}_1^c, \\vec{q}_2^c, \\dots, \\vec{q}_j^c \\big>$\n",
    "6. (From the process of getting $R$, we can see that each time $r_{jj} = \\|\\vec{x}\\|$.) So that actually $\\|\\vec{q}_j^c\\| = 1$, and $\\vec{q}_1^c, \\vec{q}_2^c, \\dots, \\vec{q}_j^c$ is actually set of orthonormal vectors.\n",
    "7. So $r_{jj} = \\|\\vec{a}_j - r_{1j}\\vec{q}_1 - r_{2j}\\vec{q}_2 - \\cdots - r_{j-1,j}\\vec{q}_{j-1} \\|_2$, and $\\vec{q}_j^c = \\ffrac{\\vec{a}_j - r_{1j}\\vec{q}_1 - r_{2j}\\vec{q}_2 - \\cdots - r_{j-1,j}\\vec{q}_{j-1}} {r_{jj}}$\n",
    "\n",
    "For the algorithms below, calculate $r_{1,1}$ and $\\vec{q}_1^{c}$ first.\n",
    "\n",
    "$Algorithm$ **QR factorization by classical Gram-Schmidt**\n",
    "\n",
    "```\n",
    "for j = 1 to n\n",
    "    \\vec{v} = \\vec{a}_j\n",
    "    for i = 1 to j−1\n",
    "        r_{ij} = \\vec{q}_i^T \\vec{a}_j\n",
    "        \\vec{v} = \\vec{v} − r_{ij} \\vec{q}_i\n",
    "    end\n",
    "    r_{jj} = \\| \\vec{v} \\|_2\n",
    "    \\vec{q}_j = \\vec{v}/r_{jj}\n",
    "end\n",
    "```\n",
    "\n",
    "With flops $\\sum\\limits_{j=1}^{n}\\sum\\limits_{i=1}^{j-1} 4m \\approx 2mn^2$\n",
    "\n",
    "$Algorithm$ **QR factorization by modified Gram-Schmidt**\n",
    "\n",
    "```\n",
    "for j = 1 to n\n",
    "    \\vec{v} = \\vec{a}_j\n",
    "    for i = 1 to j−1\n",
    "        r_{ij} = \\vec{q}_i^T \\vec{v}\n",
    "        \\vec{v} = \\vec{v} − r_{ij} \\vec{q}_i\n",
    "    end\n",
    "    r_{jj} = \\| \\vec{v} \\|_2\n",
    "    \\vec{q}_j = \\vec{v}/r_{jj}\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward stability of QR factorization\n",
    "$Theorem$\n",
    "\n",
    "Let $A = QR$ by Householder triangularization and for the computed factors, we have $\\tilde{Q}\\tilde{R} = A + \\delta A$, with $\\ffrac{\\|\\delta A\\|} {\\| A \\|} = O(\\epsilon_{machine})$. But it is not backward stable if using the classical or modified Gram-Schmidt algorithm. \n",
    "\n",
    "Take an example.\n",
    "\n",
    "$$A = \\left[\\begin{array}{cc}\n",
    "0.7 & 0.7 + 10^{-15} \\\\\n",
    "0.7 + 10^{-15} & 0.7 + 10^{-15}\n",
    "\\end{array} \\right]$$\n",
    "\n",
    "Condition number of $A$ is about $10^{15}$. Denote the result from Gram-Schmidt algorithm in MATLAB as $Q_G, R_G$, respectively; and similar for $Q_H, R_H$ using Householder triangularization. The result are\n",
    "\n",
    "$$Q_G Q_G^{\\mathrm{T}} = \\left[\\begin{array}{cc}\n",
    "0.890 & 0.012 \\\\\n",
    "0.012 & 1.109\n",
    "\\end{array} \\right],Q_H Q_H^{\\mathrm{T}} = \\left[\\begin{array}{cc}\n",
    "1 & 0 \\\\\n",
    "0 & 1\n",
    "\\end{array} \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution of system of linear equations through QR factorization\n",
    "Given $A \\in \\mathbb{R}^{n \\times n}$ and $\\vec{b} \\in \\mathbb{R}^{n}$. As mentioned above, we have $R\\vec{x} = Q^{\\mathrm{T}}\\vec{b}$ after the QR factorization, which can be solved by backward substitution or backslash function.\n",
    "\n",
    "Comparing using the LU factorization, it's more stable but more expensive.\n",
    "\n",
    "# Legendre polynomials 勒让德多项式\n",
    "Consider the space of polynomials of degree less or equal than $n − 1$ on the interval $x \\in [ − 1, 1]$. The monomials $1, x, x^2 , ... , x^{n − 1}$ form a basis of this space; $p(x) = a_1 + a_2 x + \\cdots + a_n x^{n − 1}$.\n",
    "\n",
    "$Def$ **inner product**\n",
    "\n",
    "$$\\big(p(x),q(x)\\big) = \\int _{-1} ^{1} p(x)q(x) \\,\\mathrm{d}x$$\n",
    "\n",
    "Two polynomials are orthogonal if $\\big(p(x),q(x)\\big)=0$.\n",
    "\n",
    "Then after that the fact is $1, x, x^2 , ... , x^{n − 1}$ is not an orthogonal basis. So using QR factorization to find it.\n",
    "\n",
    "$$A = \\left[\\begin{array}{cccc}\n",
    "1 & x & \\cdots & x^{n-1}\n",
    "\\end{array} \\right] = \\left[\\begin{array}{cccc}\n",
    "P_0(x) & P_1(x) & \\cdots & P_{n-1}(x)\n",
    "\\end{array} \\right] \\left[\\begin{array}{cccc}\n",
    "r_{11} & r_{12}&  \\cdots & r_{1n} \\\\\n",
    "0 & r_{22} & \\cdots & r_{2n} \\\\\n",
    "\\vdots & \\ddots & \\ddots& \\vdots \\\\\n",
    "0 & \\cdots & 0 & r_{nn}\n",
    "\\end{array} \\right]$$\n",
    "\n",
    "And the polynomials in $Q$ are the **Legendre polynomials**. \n",
    "\n",
    "$$\\displaystyle P_{n}(x)={1 \\over 2^{n}n!}{\\mathrm{d}^{n} \\over \\mathrm{d}x^{n}}\\left[(x^{2}-1)^{n}\\right]$$\n",
    "\n",
    "Here each polynomial $P_i(x)$ is of degree $i$, and since $P_i(1) = 1$, so they form an orthogonal basis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "167px",
    "width": "288px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
